{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Replication_Lim.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxQ32n5QT3Eo",
        "colab_type": "text"
      },
      "source": [
        "# This is a replication of Lim's method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8AUbb50-UfE",
        "colab_type": "code",
        "outputId": "c9f6f549-20d3-46fd-f269-735c7dd763b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "# Check GPU for Google Colab\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()\n",
        " "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=e9bc97e125e667055101e18c43613404757512a56224dcc622c65dfa036c7e38\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 26.4 GB  | Proc size: 155.5 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lvn4_zUPHP6u",
        "colab_type": "code",
        "outputId": "aa91892a-2d5d-4fa7-e8bd-1ff335278546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qGHk7xnG_0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import random\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D, concatenate\n",
        "from keras.layers import Activation, BatchNormalization, ZeroPadding2D, Concatenate, Dropout, UpSampling2D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD, Adam, Adadelta\n",
        "from keras.losses import binary_crossentropy\n",
        "from sklearn import preprocessing\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "plt.interactive(False)\n",
        "\n",
        "from itertools import permutations\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from scipy.spatial import distance\n",
        "from PIL import Image, ImageOps\n",
        "import gc\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOji9Qq2HhfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
        "    \"\"\"\n",
        "    Implementation of the triplet loss function\n",
        "    Arguments:\n",
        "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
        "    y_pred -- python list containing three objects:\n",
        "            anchor -- the encodings for the anchor data\n",
        "            positive -- the encodings for the positive data (similar to anchor)\n",
        "            negative -- the encodings for the negative data (different from anchor)\n",
        "    Returns:\n",
        "    loss -- real number, value of the loss\n",
        "    \"\"\"\n",
        "    print('y_pred.shape = ',y_pred)\n",
        "    \n",
        "#     total_lenght = y_pred.shape.as_list()[-1]\n",
        "    total_lenght = 512*3\n",
        "    \n",
        "    anchor = y_pred[:,0:int(total_lenght*1/3)]\n",
        "    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "    \n",
        "#     print(K.eval(K.greater_equal(K.sum(K.square(anchor)), 1)))\n",
        "    \n",
        "    # Normalize vectors as specified in the paper\n",
        "#     if K.eval(K.greater_equal(K.sum(K.square(anchor)), 1)):\n",
        "#       anchor =  K.l2_normalize(anchor, axis=0)\n",
        "#     if K.get_value(K.sum(K.square(positive))) > 1:\n",
        "#       positive =  K.l2_normalize(positive, axis=0)\n",
        "#     if K.get_value(K.sum(K.square(negative))) > 1:\n",
        "#       negative =  K.l2_normalize(negative, axis=0)\n",
        "      \n",
        "    anchor =  K.l2_normalize(anchor, axis=0)\n",
        "    positive =  K.l2_normalize(positive, axis=0)\n",
        "    negative =  K.l2_normalize(negative, axis=0)\n",
        "    \n",
        "    print(\"Ancor shape:\", anchor.shape.as_list()[-1])\n",
        "\n",
        "    \n",
        "    # distance between the anchor and the positive\n",
        "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
        "\n",
        "    # distance between the anchor and the negative\n",
        "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
        "\n",
        "    # compute loss\n",
        "    basic_loss = pos_dist-neg_dist+alpha\n",
        "    loss = K.maximum(basic_loss,0.0)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7QN81ZvHhkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CNN module as specified in the paper\n",
        "def create_base_network(in_dims):\n",
        "    \"\"\"\n",
        "    Base network to be shared.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        " \n",
        "    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=(in_dims[0],in_dims[1],in_dims[2])))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    model.add(Dropout(0.1))\n",
        "    \n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    model.add(Dropout(0.3))\n",
        "    \n",
        "    model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(512, (1, 1), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    \n",
        "    model.add(Flatten(name='embeddings'))\n",
        "    \n",
        "    print(model.summary())\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQFGLZjjA4IG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## different CNN just to test\n",
        "# def create_base_network(in_dims):\n",
        "#     \"\"\"\n",
        "#     Base network to be shared.\n",
        "#     \"\"\"\n",
        "#     model = Sequential()\n",
        " \n",
        "#     model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(in_dims[0],in_dims[1],in_dims[2])))\n",
        "#     model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    \n",
        "#     model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "#     model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    \n",
        "#     model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "#     model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    \n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(512,name='embeddings', activation='sigmoid'))\n",
        "    \n",
        "# #     print(model.summary())\n",
        "    \n",
        "#     return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rt78b7oHhpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compile_model(img_channel=1):\n",
        "  print(\"Compiling model...\")\n",
        "  img_size = 112\n",
        "\n",
        "#   optim = Adam()\n",
        "  optim = Adadelta(lr=1.0, rho=0.95, epsilon=0.0000001, decay=0.0) # as specified in the paper\n",
        "\n",
        "  anchor_input = Input((img_size,img_size,img_channel, ), name='anchor_input')\n",
        "  positive_input = Input((img_size,img_size,img_channel, ), name='positive_input')\n",
        "  negative_input = Input((img_size,img_size,img_channel, ), name='negative_input')\n",
        "\n",
        "  # Shared embedding layer for positive and negative items\n",
        "  Shared_DNN = create_base_network([img_size,img_size,img_channel])\n",
        "\n",
        "  encoded_anchor = Shared_DNN(anchor_input)\n",
        "  encoded_positive = Shared_DNN(positive_input)\n",
        "  encoded_negative = Shared_DNN(negative_input)\n",
        "\n",
        "  merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
        "\n",
        "\n",
        "  model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
        "  model.compile(loss=triplet_loss, optimizer=optim)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eISJVsEiHJge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def plot_3d(img_id):\n",
        "#   from sklearn.decomposition import PCA\n",
        "#   import matplotlib.pyplot as plt\n",
        "\n",
        "#   total_length = pred.shape[1]\n",
        "  \n",
        "#   print(\"Plotting...\")\n",
        "#   # Evaluate on test data\n",
        "# #   X = [Anchor_test, Positive_test, Negative_test]\n",
        "#   X = [Anchor, Positive, Negative]\n",
        "#   y_pred = model.predict(x=X,verbose=1)\n",
        "  \n",
        "#   anchor = y_pred[:,0:int(total_length*1/3)]\n",
        "#   positive = y_pred[:,int(total_length*1/3):int(total_length*2/3)]\n",
        "#   negative = y_pred[:,int(total_length*2/3):int(total_length*3/3)]\n",
        "\n",
        "#   data_plot = np.concatenate([anchor,positive,negative])\n",
        "\n",
        "  \n",
        "#   pca = PCA(n_components=2)\n",
        "#   pca.fit(data_plot)\n",
        "\n",
        "#   data_plot = pca.transform(data_plot)\n",
        "\n",
        "# #   # Set colors\n",
        "# #   colors = []\n",
        "# #   num_data = 1\n",
        "# #   for i in range(0,num_data):\n",
        "# #     colors.append(\"red\")\n",
        "# #   for i in range(0,num_data):\n",
        "# #     colors.append(\"blue\")\n",
        "# #   for i in range(0,num_data):\n",
        "# #     colors.append(\"green\")\n",
        "\n",
        "#   fig = plt.figure()\n",
        "  \n",
        "#   print(data_plot[:,0].shape)\n",
        "#   plt.scatter(data_plot[:,0], data_plot[:,1])\n",
        "  \n",
        "#   img_name = \"drive/My Drive/Saved_Images/figure\" + str(img_id) + \".png\"\n",
        "#   plt.savefig(img_name) \n",
        "#   plt.close()\n",
        "# #   plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nmqRSI127gHU",
        "colab": {}
      },
      "source": [
        "def evaluate_triplet(X_test, Y_test):\n",
        "  \n",
        "#   X_test = [Negative_train, Positive_train, Anchor_train]\n",
        "#   Y_test = np.zeros((Anchor_train.shape[0],1))\n",
        "  \n",
        "  pred = model.predict(x=X_test, verbose=1)\n",
        "  \n",
        "  # Split into anchor, a, and b sets\n",
        "  total_lenght = pred.shape[1]\n",
        "  pred_anchor = pred[:,0:int(total_lenght*1/3)]\n",
        "  pred_a = pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "  pred_b = pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "  \n",
        "  \n",
        "  # Normalize vectors as specified in the paper\n",
        "#   if np.sum(pred_anchor**2) > 1:\n",
        "#     pred_anchor = preprocessing.normalize([pred_anchor], norm='l2')[0]\n",
        "#   if np.sum(pred_a**2) > 1:\n",
        "#     pred_a = preprocessing.normalize([pred_a], norm='l2')[0]\n",
        "#   if np.sum(pred_b**2) > 1:\n",
        "#     pred_b = preprocessing.normalize([pred_b], norm='l2')[0]\n",
        "\n",
        "#   print(\"pred_anchor\", np.sum(pred_anchor[0]**2))\n",
        "\n",
        "  pred_anchor = preprocessing.normalize(pred_anchor, norm='l2')\n",
        "  pred_a = preprocessing.normalize(pred_a, norm='l2')\n",
        "  pred_b = preprocessing.normalize(pred_b, norm='l2')\n",
        "  \n",
        "#   print(\"pred_anchor norm\", np.sum(pred_anchor[0]**2))\n",
        "  \n",
        "  result = []\n",
        "  for i in range(pred.shape[0]):\n",
        "    dist1 = distance.euclidean(pred_anchor[i], pred_a[i])\n",
        "    dist2 = distance.euclidean(pred_anchor[i], pred_b[i])\n",
        "\n",
        "    if dist1 < dist2:\n",
        "      result.append(0)\n",
        "    else:\n",
        "      result.append(1)   \n",
        "\n",
        "  print(\"Evaluation accuracy: \", accuracy_score(Y_test, result))\n",
        "\n",
        "  return accuracy_score(Y_test, result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3EOP4ycaIht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pred():\n",
        "  \n",
        "  X_test = [Anchor_test, Positive_test, Negative_test]\n",
        "  Y_test = np.zeros((Anchor_test.shape[0],1))\n",
        "\n",
        "  pred = model.predict(x=X_test, verbose=1)\n",
        "  \n",
        "  # Split into anchor, a, and b sets\n",
        "  total_lenght = pred.shape[1]\n",
        "  pred_anchor = pred[:,0:int(total_lenght*1/3)]\n",
        "  pred_a = pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "  pred_b = pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "  \n",
        "  y_pred = []\n",
        "  for i in range(pred.shape[0]):\n",
        "    dist1 = distance.euclidean(pred_anchor[i], pred_a[i])\n",
        "    dist2 = distance.euclidean(pred_anchor[i], pred_b[i])\n",
        "\n",
        "    if dist1 < dist2:\n",
        "      y_pred.append(0)\n",
        "    else:\n",
        "      y_pred.append(1)   \n",
        "\n",
        "  print(\"Evaluation accuracy: \", accuracy_score(Y_test, y_pred))\n",
        "\n",
        "  return y_pred\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3kZW2GsGyH6",
        "colab_type": "code",
        "outputId": "679d58ee-57bb-4b13-d082-3e2a0e509ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Evaluate on test data for multi-views\n",
        "n_views = 3\n",
        "\n",
        "# Evaluate on test data\n",
        "X_test = [Anchor_test, Positive_test, Negative_test]\n",
        "Y_test = np.zeros((Anchor_test.shape[0],1))\n",
        "\n",
        "pred = model.predict(x=X_test, verbose=1)\n",
        "\n",
        "# Split into anchor, a, and b sets\n",
        "total_lenght = pred.shape[1]\n",
        "pred_anchor = pred[:,0:int(total_lenght*1/3)]\n",
        "pred_a = pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "pred_b = pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "\n",
        "y_pred = []\n",
        "for i in range(pred.shape[0]):\n",
        "  dist_pos = distance.euclidean(pred_anchor[i], pred_a[i])\n",
        "  dist_neg = distance.euclidean(pred_anchor[i], pred_b[i])\n",
        "  \n",
        "  if dist_pos < dist_neg:\n",
        "    y_pred.append(0)\n",
        "  else:\n",
        "    y_pred.append(1)   \n",
        "\n",
        "## Voting from multi-view\n",
        "y_pred_new = []\n",
        "df_vote = pd.DataFrame()\n",
        "length =int(pred.shape[0]/3)\n",
        "for i in range(0,length):\n",
        "  \n",
        "  if (y_pred[i]+y_pred[i+length]+y_pred[i+length*2] < 2):\n",
        "    y_pred_new.append(0)\n",
        "  else:\n",
        "    y_pred_new.append(1)\n",
        "    \n",
        "    df_vote = df_vote.append(pd.DataFrame([y_pred[i], y_pred[i+length], y_pred[i+length*2]]).T)\n",
        "\n",
        "print(accuracy_score(y_true[0:length], y_pred_new))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-56ea9e3b211b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Evaluate on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mAnchor_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPositive_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNegative_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAnchor_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Anchor_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sCxnEs8Mrt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Create two plots; accuracy and loss\n",
        "def save_plot(eval_name, fold_id, view_id):\n",
        "  fig = plt.figure()\n",
        "\n",
        "  # top\n",
        "  ax1 = fig.add_subplot(2, 1, 1)\n",
        "  ax1.plot(df_history[0])\n",
        "  ax1.plot(df_history[1])\n",
        "  ax1.set_xlabel(\"epoch\")\n",
        "  ax1.set_ylabel(\"loss\")\n",
        "  plt.legend(['train_loss', 'val_loss'], loc='upper right', fontsize='x-small')\n",
        "  plt.title(\"Fold-\"+str(fold_id)+\", View-\"+str(view_id))\n",
        "\n",
        "  # bottom\n",
        "  ax2 = fig.add_subplot(2, 1, 2)\n",
        "  ax2.plot(df_history[2])\n",
        "  ax2.set_xlabel(\"epoch\")\n",
        "  ax2.set_ylabel(\"accuracy\")\n",
        "  plt.legend(['val_accuracy'], loc='upper left', fontsize='x-small')\n",
        "\n",
        "#   fig.show()\n",
        "  fig.savefig(\"drive/My Drive/Saved_Images/CV_eval_img/\" + eval_name + \"/Fold-\"+str(fold_id) + \"-View-\"+str(view_id))\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEK0_ssqreZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(fold_id,view_ids=[1,6,8]):  \n",
        "  ## Generate training/test data\n",
        "  df_train = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_building/df_train_\" + str(fold_id) + \".csv\")\n",
        "  df_test = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_building/df_test_\" + str(fold_id) + \".csv\")\n",
        "#   df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
        "#   df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
        "  df_train = df_train.reset_index(drop=\"True\")\n",
        "  df_test = df_test.reset_index(drop=\"True\")\n",
        "\n",
        "  ### Create image dataset for triplet learning \n",
        "  img_size = 128\n",
        "  img_mode = \"L\" #[\"RGB\", \"L\"]\n",
        "  img_path = \"drive/My Drive/Style_data2_views/building/\"\n",
        "\n",
        "  out_list = []\n",
        "  ################ Generate Training Data ################\n",
        "  name_list = [\"query\", \"pos\", \"neg\"]\n",
        "  data_length = len(df_train)\n",
        "\n",
        "  for k in range(0,3):\n",
        "    print(\"Generating training data\", k+1, \"/3...\")\n",
        "    input_data = []\n",
        "    for i in range(data_length):\n",
        "      for j in view_ids:\n",
        "          data_id = df_train.loc[:,name_list[k]][i]\n",
        "          img_file = data_id + \"_\" + str(j) + \".png\"\n",
        "          img = img_path + img_file\n",
        "          img = Image.open(img)\n",
        "          img = ImageOps.expand(img, border=25, fill=0)  # padding with 0s\n",
        "          img = img.convert(img_mode)\n",
        "          img = img.resize((img_size, img_size))\n",
        "          data = np.asarray(img)/255\n",
        "          \n",
        "          # Randomly crop 112x112 images\n",
        "          for i in range(2):             # num of cropped images\n",
        "            rand1 = random.randrange(17)\n",
        "            rand2 = random.randrange(17)\n",
        "            data_crop = np.asarray(data[rand1:rand1+112, rand2:rand2+112])\n",
        "\n",
        "            input_data.append(data_crop)\n",
        "            input_data.append(np.fliplr(data_crop)) ## Flipped image\n",
        "\n",
        "    out_list.append(np.array(input_data))\n",
        "    \n",
        "\n",
        "\n",
        "  ############### Generate Test Data #########################\n",
        "  data_length = len(df_test)\n",
        "\n",
        "  for k in range(0,3):\n",
        "    print(\"Generating test data\", k+1, \"/3...\")\n",
        "    input_data = []\n",
        "    for i in range(data_length):\n",
        "      for j in view_ids:\n",
        "          data_id = df_test.loc[:,name_list[k]][i]\n",
        "          img_file = data_id + \"_\" + str(j) + \".png\"\n",
        "          img = img_path + img_file\n",
        "          img = Image.open(img)\n",
        "          img = ImageOps.expand(img, border=25, fill=0) # padding with 0s\n",
        "          img = img.convert(img_mode)\n",
        "          img = img.resize((img_size, img_size))\n",
        "          data = np.asarray(img)/255\n",
        "          data_crop = np.asarray(data[8:120, 8:120]) # crop a image on the centre\n",
        "          input_data.append(data_crop)\n",
        "          \n",
        "    out_list.append(np.array(input_data))\n",
        "\n",
        "  return out_list\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJFZDCu_MCjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Regenerate training set every 10 epoch as proposed in the paper \n",
        "\n",
        "def regenerate_set(train_set):\n",
        "  print(\"Regenerating training set...\")\n",
        "  \n",
        "  Anchor_train_all = train_set[0].sample(frac=1).reset_index(drop=True)\n",
        "  Positive_train_all = train_set[1].sample(frac=1).reset_index(drop=True)\n",
        "  Negative_train_all = train_set[2].sample(frac=1).reset_index(drop=True)\n",
        "  \n",
        "  pred = model.predict(x=[Anchor_train_all,Positive_train_all,Negative_train_all],verbose=1)\n",
        "  \n",
        "  total_length = pred.shape[1]\n",
        "  \n",
        "#   cur_50 = \n",
        "  \n",
        "#   pred_anchor = pred[:,0:int(total_length*1/3)]\n",
        "#   pred_a = pred[:,int(total_length*1/3):int(total_length*2/3)]\n",
        "#   pred_b = pred[:,int(total_length*2/3):int(total_length*3/3)]\n",
        "\n",
        "#   index_new = []\n",
        "  \n",
        "#   for i in range(pred.shape[0]):\n",
        "#     dist_pos = distance.euclidean(pred_anchor[i], pred_a[i])\n",
        "#     dist_neg = distance.euclidean(pred_anchor[i], pred_b[i])\n",
        "#     print(\"pos, neg\",dist_pos,dist_neg)\n",
        "    \n",
        "#     if dist_pos < dist_neg:\n",
        "#       index_new.append(i)\n",
        "  \n",
        "  Anchor_train_all_new = Anchor_train_all_new.sample(frac=1).reset_index(drop=True)\n",
        "  Positive_train_all_new = Positive_train_all_new.sample(frac=1).reset_index(drop=True)\n",
        "  Negative_train_all_new = Negative_train_all_new.sample(frac=1).reset_index(drop=True)\n",
        "     \n",
        "  return [Anchor_train_all_new, Positive_train_all_new, Negative_train_all_new]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Vv9DnMEOAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_triplet_vote(X_test, Y_test, num_views):\n",
        "  \n",
        "  pred = model.predict(x=X_test, verbose=1)\n",
        "  \n",
        "  # Split into anchor, a, and b sets\n",
        "  total_lenght = pred.shape[1]\n",
        "  pred_anchor = pred[:,0:int(total_lenght*1/3)]\n",
        "  pred_a = pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "  pred_b = pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "  \n",
        "  # Normalize vectors as specified in the paper\n",
        "#   if np.sum(pred_anchor**2) > 1:\n",
        "#     pred_anchor = preprocessing.normalize([pred_anchor], norm='l2')[0]\n",
        "#   if np.sum(pred_a**2) > 1:\n",
        "#     pred_a = preprocessing.normalize([pred_a], norm='l2')[0]\n",
        "#   if np.sum(pred_b**2) > 1:\n",
        "#     pred_b = preprocessing.normalize([pred_b], norm='l2')[0]\n",
        "\n",
        "#   print(\"pred_anchor\", np.sum(pred_anchor[0]**2))\n",
        "\n",
        "  pred_anchor = preprocessing.normalize(pred_anchor, norm='l2')\n",
        "  pred_a = preprocessing.normalize(pred_a, norm='l2')\n",
        "  pred_b = preprocessing.normalize(pred_b, norm='l2')\n",
        "  \n",
        "#   print(\"pred_anchor norm\", np.sum(pred_anchor[0]**2))\n",
        "  \n",
        "  result_tmp = []\n",
        "  result = []\n",
        "  for i in range(pred.shape[0]):\n",
        "    dist1 = distance.euclidean(pred_anchor[i], pred_a[i])\n",
        "    dist2 = distance.euclidean(pred_anchor[i], pred_b[i])\n",
        "\n",
        "    if dist1 < dist2:\n",
        "      result_tmp.append(0)\n",
        "    else:\n",
        "      result_tmp.append(1)\n",
        "      \n",
        "    if len(result_tmp)==num_views:\n",
        "      if (sum(result_tmp) < math.ceil(num_views/2)):\n",
        "        result.append(0)\n",
        "      else:\n",
        "        result.append(1)\n",
        "      \n",
        "      result_tmp=[]\n",
        "\n",
        "  print(\"Voting evaluation accuracy: \", accuracy_score(Y_test[0:int(len(Y_test)/3)], result))\n",
        "\n",
        "  return accuracy_score(Y_test[0:int(len(Y_test)/3)], result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqZSlTqlGvtE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "45262648-efa0-448c-eed3-ddb4b8a28d77"
      },
      "source": [
        "sum([1,2,3])\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I0XEY0eLR43",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78f3cef0-a3a4-4bef-f70f-e61e14e5b9c4"
      },
      "source": [
        "len(np.zeros((Anchor_test_all.shape[0],1)))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFluQG8PDNqN",
        "colab_type": "text"
      },
      "source": [
        "# 10-fold-cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kySp9S6VQaMU",
        "colab_type": "code",
        "outputId": "66fff0a9-6308-41e4-9ab0-a3f61a84ad5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Multi-view version\n",
        "## Training using all views\n",
        "\n",
        "import time\n",
        "# Time\n",
        "start = time.time()\n",
        "\n",
        "df_acc = pd.DataFrame()\n",
        "##### 10-fold cross-validation ###########\n",
        "for k in range(2,11):\n",
        "  \n",
        "  print(\"==================== Fold:\", k, \"/10 ===================\")\n",
        "  ## Generate i th fold\n",
        "  views = [1,3,7]\n",
        "  generated_data = data_generator(k,view_ids=views)\n",
        "\n",
        "  Anchor_train_all = generated_data[0]\n",
        "  Positive_train_all = generated_data[1]\n",
        "  Negative_train_all = generated_data[2]\n",
        "  # Use for RGB mode\n",
        "  Anchor_train_all = Anchor_train_all[:, :, :, np.newaxis]\n",
        "  Positive_train_all = Positive_train_all[:, :, :, np.newaxis]\n",
        "  Negative_train_all = Negative_train_all[:, :, :, np.newaxis]\n",
        "\n",
        "  Anchor_test_all = generated_data[3]\n",
        "  Positive_test_all = generated_data[4]\n",
        "  Negative_test_all = generated_data[5]\n",
        "  # Use for RGB mode\n",
        "  Anchor_test_all = Anchor_test_all[:, :, :, np.newaxis]\n",
        "  Positive_test_all = Positive_test_all[:, :, :, np.newaxis]\n",
        "  Negative_test_all = Negative_test_all[:, :, :, np.newaxis]\n",
        "\n",
        "\n",
        "  Y_dummy1 = np.zeros((Anchor_train_all.shape[0],1))\n",
        "  Y_dummy2 = np.zeros((Anchor_test_all.shape[0],1))\n",
        "\n",
        "  train_length =int(Anchor_train_all.shape[0]/3)\n",
        "  test_length =int(Anchor_test_all.shape[0]/3)\n",
        "  \n",
        "  num_epoch = 500\n",
        "  \n",
        "  ## Complie model\n",
        "  model = compile_model(img_channel=1)\n",
        "  \n",
        "  df_history = pd.DataFrame()\n",
        "  for i in range(1,num_epoch+1):\n",
        "    print(\"Epoch:\",i,\"/\",num_epoch)\n",
        "    \n",
        "#     [Anchor_train_all, Positive_train_all, Negative_train_all] = regenerate_set([Anchor_train_all, Positive_train_all, Negative_train_all])\n",
        "    # Regenerate training set every 10 epoch\n",
        "#     if i%10==0 and i>1:\n",
        "#       [Anchor_train_all, Positive_train_all, Negative_train_all] = regenerate_set([Anchor_train_all, Positive_train_all, Negative_train_all])\n",
        "    \n",
        "  #   plot_3d(i)\n",
        "    val_acc = evaluate_triplet(X_test = [Negative_test_all, Positive_test_all, Anchor_test_all], \n",
        "                                Y_test = np.zeros((Anchor_test_all.shape[0],1)))\n",
        "    evaluate_triplet(X_test = [Negative_train_all, Positive_train_all, Anchor_train_all], \n",
        "                                Y_test = np.zeros((Anchor_train_all.shape[0],1)))\n",
        "    \n",
        "    evaluate_triplet_vote(X_test = [Negative_test_all, Positive_test_all, Anchor_test_all], \n",
        "                                Y_test = np.zeros((Anchor_test_all.shape[0],1)), num_views=len(views))\n",
        "    \n",
        "    history= model.fit(x=[Anchor_train_all, Positive_train_all, Negative_train_all], y=Y_dummy1,\n",
        "                       validation_data=([Anchor_test_all,Positive_test_all,Negative_test_all],Y_dummy2),\n",
        "                batch_size=64, epochs=1, verbose=1, shuffle=True)\n",
        "\n",
        "    df_history = df_history.append(pd.DataFrame([history.history['loss'], history.history['val_loss'], [val_acc]]).T)\n",
        "    df_history = df_history.reset_index(drop=\"Ture\")\n",
        "\n",
        "    # elapsed time\n",
        "    elapsed_time = time.time() - start\n",
        "    print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)), \"\\n\")\n",
        "    \n",
        "  save_plot(\"Lun_building\", fold_id=k, view_id=1)\n",
        "    \n",
        "  \n",
        "  gc.collect()\n",
        "  \n",
        "  # Save a list of accuracy\n",
        "  np.save(\"drive/My Drive/MultiviewAccuracy_10CV/Lun_building/df_acc_\"+ str(k), np.array(df_acc))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================== Fold: 2 /10 ===================\n",
            "Generating training data 1 /3...\n",
            "Generating training data 2 /3...\n",
            "Generating training data 3 /3...\n",
            "Generating test data 1 /3...\n",
            "Generating test data 2 /3...\n",
            "Generating test data 3 /3...\n",
            "Compiling model...\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_71 (Conv2D)           (None, 112, 112, 32)      320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_71 (Activation)   (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_57 (MaxPooling (None, 37, 37, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_57 (Dropout)         (None, 37, 37, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_72 (Conv2D)           (None, 37, 37, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 37, 37, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_72 (Activation)   (None, 37, 37, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_58 (MaxPooling (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_58 (Dropout)         (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_73 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_73 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_73 (Activation)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_59 (MaxPooling (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_59 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_74 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_74 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_74 (Activation)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_60 (MaxPooling (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_60 (Dropout)         (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_75 (Conv2D)           (None, 1, 1, 512)         131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_75 (Batc (None, 1, 1, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_75 (Activation)   (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "embeddings (Flatten)         (None, 512)               0         \n",
            "=================================================================\n",
            "Total params: 523,392\n",
            "Trainable params: 521,408\n",
            "Non-trainable params: 1,984\n",
            "_________________________________________________________________\n",
            "None\n",
            "y_pred.shape =  Tensor(\"merged_layer_14/concat:0\", shape=(?, ?), dtype=float32)\n",
            "Ancor shape: None\n",
            "Epoch: 1 / 500\n",
            "90/90 [==============================] - 7s 80ms/step\n",
            "Evaluation accuracy:  0.5222222222222223\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.3547008547008547\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 32s 4ms/step - loss: 0.4678 - val_loss: 3.7229\n",
            "Elapsed time: 00:01:14 \n",
            "\n",
            "Epoch: 2 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.3547008547008547\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0704 - val_loss: 4.8131\n",
            "Elapsed time: 00:01:44 \n",
            "\n",
            "Epoch: 3 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6444444444444445\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.30822649572649574\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.7\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0375 - val_loss: 3.7801\n",
            "Elapsed time: 00:02:14 \n",
            "\n",
            "Epoch: 4 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.3926282051282051\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0195 - val_loss: 5.7370\n",
            "Elapsed time: 00:02:44 \n",
            "\n",
            "Epoch: 5 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.27323717948717946\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.7\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0149 - val_loss: 5.3075\n",
            "Elapsed time: 00:03:14 \n",
            "\n",
            "Epoch: 6 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.33547008547008544\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0099 - val_loss: 5.0944\n",
            "Elapsed time: 00:03:45 \n",
            "\n",
            "Epoch: 7 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.4202724358974359\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0081 - val_loss: 5.7037\n",
            "Elapsed time: 00:04:15 \n",
            "\n",
            "Epoch: 8 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6111111111111112\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5691773504273504\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0028 - val_loss: 4.3392\n",
            "Elapsed time: 00:04:45 \n",
            "\n",
            "Epoch: 9 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.48691239316239315\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0043 - val_loss: 5.6625\n",
            "Elapsed time: 00:05:15 \n",
            "\n",
            "Epoch: 10 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6111111111111112\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5181623931623932\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6666666666666666\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0032 - val_loss: 5.3763\n",
            "Elapsed time: 00:05:45 \n",
            "\n",
            "Epoch: 11 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6333333333333333\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5906784188034188\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.7\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0049 - val_loss: 5.8677\n",
            "Elapsed time: 00:06:15 \n",
            "\n",
            "Epoch: 12 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6111111111111112\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.6921741452991453\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.7333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0024 - val_loss: 5.1378\n",
            "Elapsed time: 00:06:44 \n",
            "\n",
            "Epoch: 13 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6222222222222222\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.484107905982906\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0030 - val_loss: 5.3769\n",
            "Elapsed time: 00:07:14 \n",
            "\n",
            "Epoch: 14 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5666666666666667\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.6175213675213675\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0014 - val_loss: 6.0322\n",
            "Elapsed time: 00:07:44 \n",
            "\n",
            "Epoch: 15 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5888888888888889\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.45205662393162394\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0011 - val_loss: 5.1434\n",
            "Elapsed time: 00:08:13 \n",
            "\n",
            "Epoch: 16 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.42988782051282054\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0010 - val_loss: 6.0323\n",
            "Elapsed time: 00:08:43 \n",
            "\n",
            "Epoch: 17 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5666666666666667\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.34615384615384615\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0026 - val_loss: 4.4338\n",
            "Elapsed time: 00:09:13 \n",
            "\n",
            "Epoch: 18 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5777777777777777\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.32759081196581197\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0016 - val_loss: 6.9330\n",
            "Elapsed time: 00:09:43 \n",
            "\n",
            "Epoch: 19 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6222222222222222\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.7073985042735043\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0015 - val_loss: 6.6808\n",
            "Elapsed time: 00:10:13 \n",
            "\n",
            "Epoch: 20 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5777777777777777\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.7250267094017094\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0024 - val_loss: 5.3506\n",
            "Elapsed time: 00:10:43 \n",
            "\n",
            "Epoch: 21 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5888888888888889\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5333867521367521\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0024 - val_loss: 5.2658\n",
            "Elapsed time: 00:11:12 \n",
            "\n",
            "Epoch: 22 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6444444444444445\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5986912393162394\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.7333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0015 - val_loss: 4.8945\n",
            "Elapsed time: 00:11:42 \n",
            "\n",
            "Epoch: 23 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5555555555555556\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.4389690170940171\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0019 - val_loss: 4.3153\n",
            "Elapsed time: 00:12:12 \n",
            "\n",
            "Epoch: 24 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5444444444444444\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.3015491452991453\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.4666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 8.4837e-04 - val_loss: 5.7308\n",
            "Elapsed time: 00:12:42 \n",
            "\n",
            "Epoch: 25 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5555555555555556\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.4873130341880342\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 5.4802e-04 - val_loss: 5.2676\n",
            "Elapsed time: 00:13:12 \n",
            "\n",
            "Epoch: 26 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5666666666666667\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5371260683760684\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 6.3771e-04 - val_loss: 4.4536\n",
            "Elapsed time: 00:13:41 \n",
            "\n",
            "Epoch: 27 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5222222222222223\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5575587606837606\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0014 - val_loss: 6.0356\n",
            "Elapsed time: 00:14:10 \n",
            "\n",
            "Epoch: 28 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5222222222222223\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5647702991452992\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.4666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 3.2963e-04 - val_loss: 4.8602\n",
            "Elapsed time: 00:14:40 \n",
            "\n",
            "Epoch: 29 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5666666666666667\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.44297542735042733\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 5.7236e-04 - val_loss: 5.3771\n",
            "Elapsed time: 00:15:10 \n",
            "\n",
            "Epoch: 30 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5444444444444444\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5634348290598291\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0014 - val_loss: 5.7102\n",
            "Elapsed time: 00:15:40 \n",
            "\n",
            "Epoch: 31 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5888888888888889\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.6000267094017094\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0000e+00 - val_loss: 4.9974\n",
            "Elapsed time: 00:16:10 \n",
            "\n",
            "Epoch: 32 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5777777777777777\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5126869658119658\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 3.4887e-04 - val_loss: 5.9914\n",
            "Elapsed time: 00:16:41 \n",
            "\n",
            "Epoch: 33 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6111111111111112\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5888087606837606\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6666666666666666\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0000e+00 - val_loss: 4.8536\n",
            "Elapsed time: 00:17:11 \n",
            "\n",
            "Epoch: 34 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5666666666666667\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5710470085470085\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 1.9400e-04 - val_loss: 5.9202\n",
            "Elapsed time: 00:17:41 \n",
            "\n",
            "Epoch: 35 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5666666666666667\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.6438301282051282\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 6.3544e-04 - val_loss: 5.2306\n",
            "Elapsed time: 00:18:11 \n",
            "\n",
            "Epoch: 36 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6111111111111112\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.6159188034188035\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 3.0383e-04 - val_loss: 4.4350\n",
            "Elapsed time: 00:18:41 \n",
            "\n",
            "Epoch: 37 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5777777777777777\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5572916666666666\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0011 - val_loss: 6.1038\n",
            "Elapsed time: 00:19:11 \n",
            "\n",
            "Epoch: 38 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5555555555555556\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5547542735042735\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 1.9318e-04 - val_loss: 3.3107\n",
            "Elapsed time: 00:19:41 \n",
            "\n",
            "Epoch: 39 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5444444444444444\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.35363247863247865\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 8.7596e-04 - val_loss: 5.6561\n",
            "Elapsed time: 00:20:10 \n",
            "\n",
            "Epoch: 40 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5555555555555556\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.6228632478632479\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 1.3421e-04 - val_loss: 5.6951\n",
            "Elapsed time: 00:20:40 \n",
            "\n",
            "Epoch: 41 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5444444444444444\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.640625\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 5.5018e-04 - val_loss: 5.9182\n",
            "Elapsed time: 00:21:09 \n",
            "\n",
            "Epoch: 42 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5777777777777777\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.6127136752136753\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 4.3692e-04 - val_loss: 6.2752\n",
            "Elapsed time: 00:21:39 \n",
            "\n",
            "Epoch: 43 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5666666666666667\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5681089743589743\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 7.4615e-04 - val_loss: 5.5430\n",
            "Elapsed time: 00:22:08 \n",
            "\n",
            "Epoch: 44 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5666666666666667\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5500801282051282\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6666666666666666\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 4.2179e-04 - val_loss: 4.6734\n",
            "Elapsed time: 00:22:37 \n",
            "\n",
            "Epoch: 45 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5444444444444444\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.49278846153846156\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 2.5273e-04 - val_loss: 5.4507\n",
            "Elapsed time: 00:23:07 \n",
            "\n",
            "Epoch: 46 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5888888888888889\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.6800213675213675\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 3.8812e-04 - val_loss: 5.2942\n",
            "Elapsed time: 00:23:36 \n",
            "\n",
            "Epoch: 47 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5777777777777777\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5825320512820513\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0010 - val_loss: 5.7174\n",
            "Elapsed time: 00:24:05 \n",
            "\n",
            "Epoch: 48 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.6072382478632479\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.7333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 2.5927e-04 - val_loss: 4.7998\n",
            "Elapsed time: 00:24:35 \n",
            "\n",
            "Epoch: 49 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5888888888888889\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.4130608974358974\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0000e+00 - val_loss: 5.1038\n",
            "Elapsed time: 00:25:04 \n",
            "\n",
            "Epoch: 50 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5888888888888889\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5053418803418803\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 22s 3ms/step - loss: 2.2043e-04 - val_loss: 5.2729\n",
            "Elapsed time: 00:25:34 \n",
            "\n",
            "Epoch: 51 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5666666666666667\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.4186698717948718\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 22s 3ms/step - loss: 0.0012 - val_loss: 4.6149\n",
            "Elapsed time: 00:26:04 \n",
            "\n",
            "Epoch: 52 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5444444444444444\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.328525641025641\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 1.3549e-04 - val_loss: 4.6615\n",
            "Elapsed time: 00:26:35 \n",
            "\n",
            "Epoch: 53 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5777777777777777\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.34962606837606836\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 4.2517e-04 - val_loss: 5.3013\n",
            "Elapsed time: 00:27:04 \n",
            "\n",
            "Epoch: 54 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5555555555555556\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5399305555555556\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0000e+00 - val_loss: 5.7435\n",
            "Elapsed time: 00:27:33 \n",
            "\n",
            "Epoch: 55 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5444444444444444\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.5774572649572649\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 5.8020e-04 - val_loss: 4.3181\n",
            "Elapsed time: 00:28:03 \n",
            "\n",
            "Epoch: 56 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5333333333333333\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.4045138888888889\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.4666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 1.8585e-04 - val_loss: 5.2250\n",
            "Elapsed time: 00:28:32 \n",
            "\n",
            "Epoch: 57 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5555555555555556\n",
            "7488/7488 [==============================] - 7s 1ms/step\n",
            "Evaluation accuracy:  0.5444711538461539\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 9.0298e-05 - val_loss: 4.4928\n",
            "Elapsed time: 00:29:01 \n",
            "\n",
            "Epoch: 58 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5555555555555556\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.4248130341880342\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0011 - val_loss: 4.6189\n",
            "Elapsed time: 00:29:31 \n",
            "\n",
            "Epoch: 59 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5888888888888889\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.3422809829059829\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 7.1645e-04 - val_loss: 6.6941\n",
            "Elapsed time: 00:30:00 \n",
            "\n",
            "Epoch: 60 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5666666666666667\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.7130074786324786\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 3.9717e-04 - val_loss: 4.9723\n",
            "Elapsed time: 00:30:29 \n",
            "\n",
            "Epoch: 61 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.6\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.4623397435897436\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6666666666666666\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 0.0000e+00 - val_loss: 5.1106\n",
            "Elapsed time: 00:30:59 \n",
            "\n",
            "Epoch: 62 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5888888888888889\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.47863247863247865\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6666666666666666\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 4.8281e-04 - val_loss: 6.7348\n",
            "Elapsed time: 00:31:28 \n",
            "\n",
            "Epoch: 63 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5777777777777777\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.7059294871794872\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 7.7968e-05 - val_loss: 4.5338\n",
            "Elapsed time: 00:31:57 \n",
            "\n",
            "Epoch: 64 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5888888888888889\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.3170405982905983\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.6333333333333333\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "7488/7488 [==============================] - 21s 3ms/step - loss: 7.4980e-05 - val_loss: 6.4059\n",
            "Elapsed time: 00:32:26 \n",
            "\n",
            "Epoch: 65 / 500\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Evaluation accuracy:  0.5333333333333333\n",
            "7488/7488 [==============================] - 8s 1ms/step\n",
            "Evaluation accuracy:  0.6131143162393162\n",
            "90/90 [==============================] - 0s 1ms/step\n",
            "Voting evaluation accuracy:  0.5666666666666667\n",
            "Train on 7488 samples, validate on 90 samples\n",
            "Epoch 1/1\n",
            "5440/7488 [====================>.........] - ETA: 5s - loss: 0.0000e+00"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-6d640ac037ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     history= model.fit(x=[Anchor_train_all, Positive_train_all, Negative_train_all], y=Y_dummy1,\n\u001b[1;32m     62\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAnchor_test_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPositive_test_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNegative_test_all\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_dummy2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 batch_size=64, epochs=1, verbose=1, shuffle=True)\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mdf_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgf7GICoAECC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_building/df_train_1.csv\")\n",
        "df_test = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_building/df_test_1.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vajPm9TvAc7b",
        "colab_type": "code",
        "outputId": "c0311eff-2d07-47fd-a970-98a8e50eba5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Check reversed duplicates\n",
        "df_tmp = df_train.copy()\n",
        "print(\"df_tmp:\", df_tmp.shape)\n",
        "df_reversed = df_tmp.loc[:,[\"query\",\"neg\",\"pos\"]].copy()\n",
        "df_reversed = df_reversed.rename(columns={\"pos\":\"neg\", \"neg\":\"pos\"})\n",
        "df_concat = pd.concat([df_tmp, df_reversed])\n",
        "df_concat = df_concat.reset_index(drop=True)\n",
        "print(int(df_concat.shape[0]))\n",
        "df_concat = df_concat.drop_duplicates(keep='first')\n",
        "\n",
        "print(int(df_concat.shape[0])-int(df_tmp.shape[0]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_tmp: (246, 3)\n",
            "492\n",
            "246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLSV1KT8EEQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0DVbPHgVNNo",
        "colab_type": "code",
        "outputId": "c223f080-20d5-4215-d119-db558df920e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        }
      },
      "source": [
        "id = 3\n",
        "data = Anchor_test_all[id][:,:,0]\n",
        "plt.imshow(data)\n",
        "plt.show()\n",
        "data = Positive_test_all[id][:,:,0]\n",
        "plt.imshow(data)\n",
        "plt.show()\n",
        "data = Negative_test_all[id][:,:,0]\n",
        "plt.imshow(data)\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntBJQlICjlYCiBBytBBQl4Px/9c8F3une7I8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYBgxx5yAYcQccwKGEXPMCRhGzPn/C1VCKm9aSTAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMD_Gr70wZgV",
        "colab_type": "code",
        "outputId": "849de374-2161-42eb-948e-90309034871f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history.history['loss']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0070456871460002615]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSKZRgc-PBO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.load(\"drive/My Drive/MultiviewAccuracy_10CV/dining/df_acc_1.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_bQQ7wg_X_H",
        "colab_type": "code",
        "outputId": "93556e81-77f5-4b73-a3b9-feedbff3cae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Evaluate on test data\n",
        "# Anchor_test = input_data2_test\n",
        "# Positive_test = input_data1_test\n",
        "# Negative_test = input_data3_test\n",
        "y_true = np.zeros((Anchor_test.shape[0],1))\n",
        "\n",
        "pred = model.predict(x=[Anchor_test,Positive_test,Negative_test],verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "105/105 [==============================] - 1s 7ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2_Qo7a1i7AY",
        "colab_type": "code",
        "outputId": "5bc40091-7e93-416d-b7b7-a70d72cc4a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "pred.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-2f4f35ded540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DftDxBrkC2KN",
        "colab_type": "code",
        "outputId": "d1379b38-1ece-45b6-c638-fcb1241b3970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Evaluate on training data\n",
        "# Anchor = input_data2_train\n",
        "# Positive = input_data1_train\n",
        "# Negative = input_data3_train\n",
        "y_true = np.zeros((Anchor.shape[0],1))\n",
        "\n",
        "pred = model.predict(x=[Anchor_train,Positive_train,Negative_train],verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "702/702 [==============================] - 19s 28ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA8DSVCQBB3O",
        "colab_type": "code",
        "outputId": "fa5b9fc7-ddf5-46e0-d796-be6e59d31ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Split into anchor, a, and b sets\n",
        "total_lenght = pred.shape[1]\n",
        "pred_anchor = pred[:,0:int(total_lenght*1/3)]\n",
        "pred_a = pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "pred_b = pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "pred_a.shape\n",
        "\n",
        "y_pred = []\n",
        "for i in range(pred.shape[0]):\n",
        "  dist_pos = distance.euclidean(pred_anchor[i], pred_a[i])\n",
        "  dist_neg = distance.euclidean(pred_anchor[i], pred_b[i])\n",
        "  print(\"Triplet\", i, \":\")\n",
        "  print(\"dist_pos\", dist_pos)\n",
        "  print(\"dist_neg\", dist_neg)\n",
        "  print(\"---------------------------------\")\n",
        "  if dist_pos < dist_neg:\n",
        "    y_pred.append(0)\n",
        "  else:\n",
        "    y_pred.append(1)   \n",
        "    \n",
        "\n",
        "print(accuracy_score(y_true, y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c418ec5b27ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_lenght\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred_anchor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXpL4m_ucw9G",
        "colab_type": "code",
        "outputId": "0af04ea3-fb02-4215-81fa-fe0be7dbe176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Anchor_test_all.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(381, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "367P5cEgaIwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_building/df_test_1.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNPPpNV_O3vj",
        "colab_type": "text"
      },
      "source": [
        "# Generate empeddings for each object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn5CdIQqPCd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## This model takes one object and outputs a embedding.\n",
        "model=Model(inputs=model.get_layer(\"model_3\").get_input_at(0),outputs=model.get_layer(\"model_3\").get_output_at(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNVxjBt3Tg4f",
        "colab_type": "code",
        "outputId": "7cd09f63-016f-4911-adc1-919d26d974b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pred_cat1 = model.predict(x=Anchor,verbose=1)\n",
        "pred_cat2 = model.predict(x=Positive,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "702/702 [==============================] - 6s 9ms/step\n",
            "702/702 [==============================] - 6s 9ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45OfMZBwTlvP",
        "colab_type": "code",
        "outputId": "ea334171-546a-425f-bd27-b8155999a657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "query = pred_cat1[0]\n",
        "df_dist = pd.DataFrame()\n",
        "for i in range(pred_cat2.shape[0]):\n",
        "  dist = distance.euclidean(query, pred_cat2[i])\n",
        "  df_dist = df_dist.append(pd.DataFrame([dist]))\n",
        "  print(\"Sample\", i, \":\")\n",
        "  print(\"dist\", dist)\n",
        "df_dist = df_dist.reset_index(drop=\"True\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample 0 :\n",
            "dist 11.942307472229004\n",
            "Sample 1 :\n",
            "dist 9.089716911315918\n",
            "Sample 2 :\n",
            "dist 5.0141496658325195\n",
            "Sample 3 :\n",
            "dist 11.207891464233398\n",
            "Sample 4 :\n",
            "dist 11.485407829284668\n",
            "Sample 5 :\n",
            "dist 11.594229698181152\n",
            "Sample 6 :\n",
            "dist 10.215923309326172\n",
            "Sample 7 :\n",
            "dist 8.77670955657959\n",
            "Sample 8 :\n",
            "dist 11.207891464233398\n",
            "Sample 9 :\n",
            "dist 9.089716911315918\n",
            "Sample 10 :\n",
            "dist 9.005356788635254\n",
            "Sample 11 :\n",
            "dist 8.812785148620605\n",
            "Sample 12 :\n",
            "dist 10.139031410217285\n",
            "Sample 13 :\n",
            "dist 11.737712860107422\n",
            "Sample 14 :\n",
            "dist 12.618820190429688\n",
            "Sample 15 :\n",
            "dist 6.905245780944824\n",
            "Sample 16 :\n",
            "dist 10.506524085998535\n",
            "Sample 17 :\n",
            "dist 12.860376358032227\n",
            "Sample 18 :\n",
            "dist 3.7555344104766846\n",
            "Sample 19 :\n",
            "dist 15.70533275604248\n",
            "Sample 20 :\n",
            "dist 9.249247550964355\n",
            "Sample 21 :\n",
            "dist 9.054219245910645\n",
            "Sample 22 :\n",
            "dist 10.1978759765625\n",
            "Sample 23 :\n",
            "dist 11.168105125427246\n",
            "Sample 24 :\n",
            "dist 14.29787540435791\n",
            "Sample 25 :\n",
            "dist 9.054219245910645\n",
            "Sample 26 :\n",
            "dist 5.6047210693359375\n",
            "Sample 27 :\n",
            "dist 8.992192268371582\n",
            "Sample 28 :\n",
            "dist 4.894937038421631\n",
            "Sample 29 :\n",
            "dist 10.372018814086914\n",
            "Sample 30 :\n",
            "dist 9.856998443603516\n",
            "Sample 31 :\n",
            "dist 8.756172180175781\n",
            "Sample 32 :\n",
            "dist 5.826895713806152\n",
            "Sample 33 :\n",
            "dist 12.618820190429688\n",
            "Sample 34 :\n",
            "dist 4.514033794403076\n",
            "Sample 35 :\n",
            "dist 5.0141496658325195\n",
            "Sample 36 :\n",
            "dist 6.086319923400879\n",
            "Sample 37 :\n",
            "dist 10.27345085144043\n",
            "Sample 38 :\n",
            "dist 14.197386741638184\n",
            "Sample 39 :\n",
            "dist 10.816109657287598\n",
            "Sample 40 :\n",
            "dist 11.129412651062012\n",
            "Sample 41 :\n",
            "dist 10.550861358642578\n",
            "Sample 42 :\n",
            "dist 9.572949409484863\n",
            "Sample 43 :\n",
            "dist 9.089716911315918\n",
            "Sample 44 :\n",
            "dist 10.07865047454834\n",
            "Sample 45 :\n",
            "dist 11.207891464233398\n",
            "Sample 46 :\n",
            "dist 9.691022872924805\n",
            "Sample 47 :\n",
            "dist 4.894937038421631\n",
            "Sample 48 :\n",
            "dist 11.485407829284668\n",
            "Sample 49 :\n",
            "dist 9.054219245910645\n",
            "Sample 50 :\n",
            "dist 10.833091735839844\n",
            "Sample 51 :\n",
            "dist 15.017980575561523\n",
            "Sample 52 :\n",
            "dist 8.019840240478516\n",
            "Sample 53 :\n",
            "dist 10.395532608032227\n",
            "Sample 54 :\n",
            "dist 8.920267105102539\n",
            "Sample 55 :\n",
            "dist 10.09423542022705\n",
            "Sample 56 :\n",
            "dist 12.336888313293457\n",
            "Sample 57 :\n",
            "dist 9.864704132080078\n",
            "Sample 58 :\n",
            "dist 9.039032936096191\n",
            "Sample 59 :\n",
            "dist 9.089716911315918\n",
            "Sample 60 :\n",
            "dist 11.129412651062012\n",
            "Sample 61 :\n",
            "dist 9.10306453704834\n",
            "Sample 62 :\n",
            "dist 7.31467342376709\n",
            "Sample 63 :\n",
            "dist 10.186614990234375\n",
            "Sample 64 :\n",
            "dist 6.924028396606445\n",
            "Sample 65 :\n",
            "dist 9.054219245910645\n",
            "Sample 66 :\n",
            "dist 9.864704132080078\n",
            "Sample 67 :\n",
            "dist 8.019840240478516\n",
            "Sample 68 :\n",
            "dist 11.24622917175293\n",
            "Sample 69 :\n",
            "dist 7.15024995803833\n",
            "Sample 70 :\n",
            "dist 11.168105125427246\n",
            "Sample 71 :\n",
            "dist 6.714141368865967\n",
            "Sample 72 :\n",
            "dist 8.992192268371582\n",
            "Sample 73 :\n",
            "dist 6.086319923400879\n",
            "Sample 74 :\n",
            "dist 7.15024995803833\n",
            "Sample 75 :\n",
            "dist 10.27345085144043\n",
            "Sample 76 :\n",
            "dist 9.089716911315918\n",
            "Sample 77 :\n",
            "dist 9.691022872924805\n",
            "Sample 78 :\n",
            "dist 9.004803657531738\n",
            "Sample 79 :\n",
            "dist 8.019840240478516\n",
            "Sample 80 :\n",
            "dist 9.550772666931152\n",
            "Sample 81 :\n",
            "dist 11.94116497039795\n",
            "Sample 82 :\n",
            "dist 8.920267105102539\n",
            "Sample 83 :\n",
            "dist 10.75291919708252\n",
            "Sample 84 :\n",
            "dist 10.310395240783691\n",
            "Sample 85 :\n",
            "dist 11.129412651062012\n",
            "Sample 86 :\n",
            "dist 10.206989288330078\n",
            "Sample 87 :\n",
            "dist 6.924028396606445\n",
            "Sample 88 :\n",
            "dist 14.807355880737305\n",
            "Sample 89 :\n",
            "dist 13.467585563659668\n",
            "Sample 90 :\n",
            "dist 8.460783004760742\n",
            "Sample 91 :\n",
            "dist 11.485407829284668\n",
            "Sample 92 :\n",
            "dist 9.691022872924805\n",
            "Sample 93 :\n",
            "dist 10.00361156463623\n",
            "Sample 94 :\n",
            "dist 6.229179382324219\n",
            "Sample 95 :\n",
            "dist 9.213743209838867\n",
            "Sample 96 :\n",
            "dist 10.287613868713379\n",
            "Sample 97 :\n",
            "dist 6.714141368865967\n",
            "Sample 98 :\n",
            "dist 8.976008415222168\n",
            "Sample 99 :\n",
            "dist 9.089716911315918\n",
            "Sample 100 :\n",
            "dist 10.550861358642578\n",
            "Sample 101 :\n",
            "dist 11.594229698181152\n",
            "Sample 102 :\n",
            "dist 8.16422176361084\n",
            "Sample 103 :\n",
            "dist 3.7555344104766846\n",
            "Sample 104 :\n",
            "dist 6.714141368865967\n",
            "Sample 105 :\n",
            "dist 8.992192268371582\n",
            "Sample 106 :\n",
            "dist 9.691022872924805\n",
            "Sample 107 :\n",
            "dist 8.812785148620605\n",
            "Sample 108 :\n",
            "dist 11.737712860107422\n",
            "Sample 109 :\n",
            "dist 8.812785148620605\n",
            "Sample 110 :\n",
            "dist 9.214948654174805\n",
            "Sample 111 :\n",
            "dist 10.423500061035156\n",
            "Sample 112 :\n",
            "dist 9.100789070129395\n",
            "Sample 113 :\n",
            "dist 9.659614562988281\n",
            "Sample 114 :\n",
            "dist 9.213743209838867\n",
            "Sample 115 :\n",
            "dist 10.206989288330078\n",
            "Sample 116 :\n",
            "dist 8.64344596862793\n",
            "Sample 117 :\n",
            "dist 9.550772666931152\n",
            "Sample 118 :\n",
            "dist 10.473353385925293\n",
            "Sample 119 :\n",
            "dist 10.053415298461914\n",
            "Sample 120 :\n",
            "dist 12.336888313293457\n",
            "Sample 121 :\n",
            "dist 9.550772666931152\n",
            "Sample 122 :\n",
            "dist 10.07865047454834\n",
            "Sample 123 :\n",
            "dist 10.814924240112305\n",
            "Sample 124 :\n",
            "dist 6.08117151260376\n",
            "Sample 125 :\n",
            "dist 15.017980575561523\n",
            "Sample 126 :\n",
            "dist 9.856998443603516\n",
            "Sample 127 :\n",
            "dist 6.714141368865967\n",
            "Sample 128 :\n",
            "dist 9.550772666931152\n",
            "Sample 129 :\n",
            "dist 5.922364711761475\n",
            "Sample 130 :\n",
            "dist 9.089716911315918\n",
            "Sample 131 :\n",
            "dist 7.31467342376709\n",
            "Sample 132 :\n",
            "dist 14.941376686096191\n",
            "Sample 133 :\n",
            "dist 8.64344596862793\n",
            "Sample 134 :\n",
            "dist 11.129412651062012\n",
            "Sample 135 :\n",
            "dist 8.992192268371582\n",
            "Sample 136 :\n",
            "dist 11.67253303527832\n",
            "Sample 137 :\n",
            "dist 10.053415298461914\n",
            "Sample 138 :\n",
            "dist 9.249247550964355\n",
            "Sample 139 :\n",
            "dist 11.594229698181152\n",
            "Sample 140 :\n",
            "dist 10.053415298461914\n",
            "Sample 141 :\n",
            "dist 9.054219245910645\n",
            "Sample 142 :\n",
            "dist 6.924028396606445\n",
            "Sample 143 :\n",
            "dist 9.659614562988281\n",
            "Sample 144 :\n",
            "dist 10.27345085144043\n",
            "Sample 145 :\n",
            "dist 11.91559886932373\n",
            "Sample 146 :\n",
            "dist 9.039032936096191\n",
            "Sample 147 :\n",
            "dist 13.467585563659668\n",
            "Sample 148 :\n",
            "dist 10.00361156463623\n",
            "Sample 149 :\n",
            "dist 10.00361156463623\n",
            "Sample 150 :\n",
            "dist 10.00361156463623\n",
            "Sample 151 :\n",
            "dist 5.0141496658325195\n",
            "Sample 152 :\n",
            "dist 9.10306453704834\n",
            "Sample 153 :\n",
            "dist 12.618820190429688\n",
            "Sample 154 :\n",
            "dist 14.807355880737305\n",
            "Sample 155 :\n",
            "dist 8.976008415222168\n",
            "Sample 156 :\n",
            "dist 9.550772666931152\n",
            "Sample 157 :\n",
            "dist 6.714141368865967\n",
            "Sample 158 :\n",
            "dist 11.91559886932373\n",
            "Sample 159 :\n",
            "dist 12.167671203613281\n",
            "Sample 160 :\n",
            "dist 9.214948654174805\n",
            "Sample 161 :\n",
            "dist 13.467585563659668\n",
            "Sample 162 :\n",
            "dist 14.807355880737305\n",
            "Sample 163 :\n",
            "dist 10.09423542022705\n",
            "Sample 164 :\n",
            "dist 11.162457466125488\n",
            "Sample 165 :\n",
            "dist 12.167671203613281\n",
            "Sample 166 :\n",
            "dist 9.249247550964355\n",
            "Sample 167 :\n",
            "dist 10.1978759765625\n",
            "Sample 168 :\n",
            "dist 9.864704132080078\n",
            "Sample 169 :\n",
            "dist 5.922364711761475\n",
            "Sample 170 :\n",
            "dist 9.089716911315918\n",
            "Sample 171 :\n",
            "dist 6.714141368865967\n",
            "Sample 172 :\n",
            "dist 8.460783004760742\n",
            "Sample 173 :\n",
            "dist 13.467585563659668\n",
            "Sample 174 :\n",
            "dist 11.129412651062012\n",
            "Sample 175 :\n",
            "dist 9.214948654174805\n",
            "Sample 176 :\n",
            "dist 9.10306453704834\n",
            "Sample 177 :\n",
            "dist 11.168105125427246\n",
            "Sample 178 :\n",
            "dist 14.605659484863281\n",
            "Sample 179 :\n",
            "dist 8.812785148620605\n",
            "Sample 180 :\n",
            "dist 5.922364711761475\n",
            "Sample 181 :\n",
            "dist 11.594229698181152\n",
            "Sample 182 :\n",
            "dist 9.739049911499023\n",
            "Sample 183 :\n",
            "dist 9.005356788635254\n",
            "Sample 184 :\n",
            "dist 8.903407096862793\n",
            "Sample 185 :\n",
            "dist 10.287613868713379\n",
            "Sample 186 :\n",
            "dist 11.347253799438477\n",
            "Sample 187 :\n",
            "dist 14.197386741638184\n",
            "Sample 188 :\n",
            "dist 7.5439677238464355\n",
            "Sample 189 :\n",
            "dist 10.585667610168457\n",
            "Sample 190 :\n",
            "dist 6.924028396606445\n",
            "Sample 191 :\n",
            "dist 11.099358558654785\n",
            "Sample 192 :\n",
            "dist 10.423500061035156\n",
            "Sample 193 :\n",
            "dist 8.992192268371582\n",
            "Sample 194 :\n",
            "dist 10.186614990234375\n",
            "Sample 195 :\n",
            "dist 9.10306453704834\n",
            "Sample 196 :\n",
            "dist 12.372003555297852\n",
            "Sample 197 :\n",
            "dist 12.618820190429688\n",
            "Sample 198 :\n",
            "dist 13.546152114868164\n",
            "Sample 199 :\n",
            "dist 13.467585563659668\n",
            "Sample 200 :\n",
            "dist 7.31467342376709\n",
            "Sample 201 :\n",
            "dist 12.649848937988281\n",
            "Sample 202 :\n",
            "dist 6.714141368865967\n",
            "Sample 203 :\n",
            "dist 9.039032936096191\n",
            "Sample 204 :\n",
            "dist 4.514033794403076\n",
            "Sample 205 :\n",
            "dist 10.00361156463623\n",
            "Sample 206 :\n",
            "dist 9.550772666931152\n",
            "Sample 207 :\n",
            "dist 9.039032936096191\n",
            "Sample 208 :\n",
            "dist 9.572949409484863\n",
            "Sample 209 :\n",
            "dist 8.920267105102539\n",
            "Sample 210 :\n",
            "dist 11.207891464233398\n",
            "Sample 211 :\n",
            "dist 10.565771102905273\n",
            "Sample 212 :\n",
            "dist 8.756172180175781\n",
            "Sample 213 :\n",
            "dist 7.356421947479248\n",
            "Sample 214 :\n",
            "dist 10.1978759765625\n",
            "Sample 215 :\n",
            "dist 10.813817024230957\n",
            "Sample 216 :\n",
            "dist 7.31467342376709\n",
            "Sample 217 :\n",
            "dist 9.10306453704834\n",
            "Sample 218 :\n",
            "dist 15.70533275604248\n",
            "Sample 219 :\n",
            "dist 10.27345085144043\n",
            "Sample 220 :\n",
            "dist 10.852871894836426\n",
            "Sample 221 :\n",
            "dist 7.826977729797363\n",
            "Sample 222 :\n",
            "dist 9.822888374328613\n",
            "Sample 223 :\n",
            "dist 10.287613868713379\n",
            "Sample 224 :\n",
            "dist 6.08117151260376\n",
            "Sample 225 :\n",
            "dist 9.249247550964355\n",
            "Sample 226 :\n",
            "dist 14.048661231994629\n",
            "Sample 227 :\n",
            "dist 13.467585563659668\n",
            "Sample 228 :\n",
            "dist 11.099358558654785\n",
            "Sample 229 :\n",
            "dist 5.6047210693359375\n",
            "Sample 230 :\n",
            "dist 7.45473051071167\n",
            "Sample 231 :\n",
            "dist 8.16422176361084\n",
            "Sample 232 :\n",
            "dist 11.594229698181152\n",
            "Sample 233 :\n",
            "dist 8.047513008117676\n",
            "Sample 234 :\n",
            "dist 4.514033794403076\n",
            "Sample 235 :\n",
            "dist 10.395532608032227\n",
            "Sample 236 :\n",
            "dist 9.572949409484863\n",
            "Sample 237 :\n",
            "dist 10.07865047454834\n",
            "Sample 238 :\n",
            "dist 10.758524894714355\n",
            "Sample 239 :\n",
            "dist 11.129412651062012\n",
            "Sample 240 :\n",
            "dist 10.372018814086914\n",
            "Sample 241 :\n",
            "dist 11.942307472229004\n",
            "Sample 242 :\n",
            "dist 10.215923309326172\n",
            "Sample 243 :\n",
            "dist 9.856998443603516\n",
            "Sample 244 :\n",
            "dist 6.714141368865967\n",
            "Sample 245 :\n",
            "dist 10.395532608032227\n",
            "Sample 246 :\n",
            "dist 12.618820190429688\n",
            "Sample 247 :\n",
            "dist 8.728707313537598\n",
            "Sample 248 :\n",
            "dist 9.856998443603516\n",
            "Sample 249 :\n",
            "dist 9.328691482543945\n",
            "Sample 250 :\n",
            "dist 14.197386741638184\n",
            "Sample 251 :\n",
            "dist 4.514033794403076\n",
            "Sample 252 :\n",
            "dist 6.086319923400879\n",
            "Sample 253 :\n",
            "dist 10.564896583557129\n",
            "Sample 254 :\n",
            "dist 11.594229698181152\n",
            "Sample 255 :\n",
            "dist 6.086319923400879\n",
            "Sample 256 :\n",
            "dist 8.976008415222168\n",
            "Sample 257 :\n",
            "dist 7.356421947479248\n",
            "Sample 258 :\n",
            "dist 10.473353385925293\n",
            "Sample 259 :\n",
            "dist 12.167671203613281\n",
            "Sample 260 :\n",
            "dist 12.618820190429688\n",
            "Sample 261 :\n",
            "dist 6.086319923400879\n",
            "Sample 262 :\n",
            "dist 6.924028396606445\n",
            "Sample 263 :\n",
            "dist 8.019840240478516\n",
            "Sample 264 :\n",
            "dist 9.691022872924805\n",
            "Sample 265 :\n",
            "dist 6.905245780944824\n",
            "Sample 266 :\n",
            "dist 12.336888313293457\n",
            "Sample 267 :\n",
            "dist 10.437501907348633\n",
            "Sample 268 :\n",
            "dist 10.58375072479248\n",
            "Sample 269 :\n",
            "dist 3.7555344104766846\n",
            "Sample 270 :\n",
            "dist 8.992192268371582\n",
            "Sample 271 :\n",
            "dist 9.039032936096191\n",
            "Sample 272 :\n",
            "dist 6.722531318664551\n",
            "Sample 273 :\n",
            "dist 11.594229698181152\n",
            "Sample 274 :\n",
            "dist 9.10306453704834\n",
            "Sample 275 :\n",
            "dist 10.186614990234375\n",
            "Sample 276 :\n",
            "dist 9.089716911315918\n",
            "Sample 277 :\n",
            "dist 12.860376358032227\n",
            "Sample 278 :\n",
            "dist 9.005356788635254\n",
            "Sample 279 :\n",
            "dist 9.659614562988281\n",
            "Sample 280 :\n",
            "dist 7.356421947479248\n",
            "Sample 281 :\n",
            "dist 9.739049911499023\n",
            "Sample 282 :\n",
            "dist 9.856998443603516\n",
            "Sample 283 :\n",
            "dist 9.864704132080078\n",
            "Sample 284 :\n",
            "dist 7.356421947479248\n",
            "Sample 285 :\n",
            "dist 8.865540504455566\n",
            "Sample 286 :\n",
            "dist 10.27345085144043\n",
            "Sample 287 :\n",
            "dist 10.09423542022705\n",
            "Sample 288 :\n",
            "dist 9.039032936096191\n",
            "Sample 289 :\n",
            "dist 8.019840240478516\n",
            "Sample 290 :\n",
            "dist 11.594229698181152\n",
            "Sample 291 :\n",
            "dist 10.07865047454834\n",
            "Sample 292 :\n",
            "dist 10.423500061035156\n",
            "Sample 293 :\n",
            "dist 9.864704132080078\n",
            "Sample 294 :\n",
            "dist 8.019840240478516\n",
            "Sample 295 :\n",
            "dist 6.905245780944824\n",
            "Sample 296 :\n",
            "dist 9.089716911315918\n",
            "Sample 297 :\n",
            "dist 5.791200160980225\n",
            "Sample 298 :\n",
            "dist 10.565771102905273\n",
            "Sample 299 :\n",
            "dist 9.214948654174805\n",
            "Sample 300 :\n",
            "dist 8.756172180175781\n",
            "Sample 301 :\n",
            "dist 6.714141368865967\n",
            "Sample 302 :\n",
            "dist 11.162457466125488\n",
            "Sample 303 :\n",
            "dist 9.864704132080078\n",
            "Sample 304 :\n",
            "dist 10.437501907348633\n",
            "Sample 305 :\n",
            "dist 10.506524085998535\n",
            "Sample 306 :\n",
            "dist 12.167671203613281\n",
            "Sample 307 :\n",
            "dist 10.473353385925293\n",
            "Sample 308 :\n",
            "dist 8.812785148620605\n",
            "Sample 309 :\n",
            "dist 9.214948654174805\n",
            "Sample 310 :\n",
            "dist 13.546152114868164\n",
            "Sample 311 :\n",
            "dist 10.565771102905273\n",
            "Sample 312 :\n",
            "dist 9.213743209838867\n",
            "Sample 313 :\n",
            "dist 9.659614562988281\n",
            "Sample 314 :\n",
            "dist 5.826895713806152\n",
            "Sample 315 :\n",
            "dist 7.444174289703369\n",
            "Sample 316 :\n",
            "dist 9.10306453704834\n",
            "Sample 317 :\n",
            "dist 9.214948654174805\n",
            "Sample 318 :\n",
            "dist 11.099358558654785\n",
            "Sample 319 :\n",
            "dist 14.359901428222656\n",
            "Sample 320 :\n",
            "dist 11.207891464233398\n",
            "Sample 321 :\n",
            "dist 9.739049911499023\n",
            "Sample 322 :\n",
            "dist 10.816109657287598\n",
            "Sample 323 :\n",
            "dist 8.812785148620605\n",
            "Sample 324 :\n",
            "dist 9.054219245910645\n",
            "Sample 325 :\n",
            "dist 12.860376358032227\n",
            "Sample 326 :\n",
            "dist 8.16422176361084\n",
            "Sample 327 :\n",
            "dist 10.11634349822998\n",
            "Sample 328 :\n",
            "dist 7.45473051071167\n",
            "Sample 329 :\n",
            "dist 10.053415298461914\n",
            "Sample 330 :\n",
            "dist 10.27345085144043\n",
            "Sample 331 :\n",
            "dist 9.659614562988281\n",
            "Sample 332 :\n",
            "dist 8.16422176361084\n",
            "Sample 333 :\n",
            "dist 10.372018814086914\n",
            "Sample 334 :\n",
            "dist 10.814924240112305\n",
            "Sample 335 :\n",
            "dist 10.11634349822998\n",
            "Sample 336 :\n",
            "dist 12.860376358032227\n",
            "Sample 337 :\n",
            "dist 10.565771102905273\n",
            "Sample 338 :\n",
            "dist 10.585667610168457\n",
            "Sample 339 :\n",
            "dist 9.822664260864258\n",
            "Sample 340 :\n",
            "dist 10.09423542022705\n",
            "Sample 341 :\n",
            "dist 8.812785148620605\n",
            "Sample 342 :\n",
            "dist 15.017980575561523\n",
            "Sample 343 :\n",
            "dist 10.833091735839844\n",
            "Sample 344 :\n",
            "dist 11.207891464233398\n",
            "Sample 345 :\n",
            "dist 9.864704132080078\n",
            "Sample 346 :\n",
            "dist 7.801884174346924\n",
            "Sample 347 :\n",
            "dist 12.372003555297852\n",
            "Sample 348 :\n",
            "dist 9.054219245910645\n",
            "Sample 349 :\n",
            "dist 8.395001411437988\n",
            "Sample 350 :\n",
            "dist 10.206989288330078\n",
            "Sample 351 :\n",
            "dist 12.123115539550781\n",
            "Sample 352 :\n",
            "dist 10.816109657287598\n",
            "Sample 353 :\n",
            "dist 7.444174289703369\n",
            "Sample 354 :\n",
            "dist 11.207891464233398\n",
            "Sample 355 :\n",
            "dist 7.356421947479248\n",
            "Sample 356 :\n",
            "dist 12.860376358032227\n",
            "Sample 357 :\n",
            "dist 9.822888374328613\n",
            "Sample 358 :\n",
            "dist 10.852871894836426\n",
            "Sample 359 :\n",
            "dist 10.310395240783691\n",
            "Sample 360 :\n",
            "dist 11.942307472229004\n",
            "Sample 361 :\n",
            "dist 5.0141496658325195\n",
            "Sample 362 :\n",
            "dist 12.336888313293457\n",
            "Sample 363 :\n",
            "dist 10.437501907348633\n",
            "Sample 364 :\n",
            "dist 10.550861358642578\n",
            "Sample 365 :\n",
            "dist 10.053415298461914\n",
            "Sample 366 :\n",
            "dist 8.16422176361084\n",
            "Sample 367 :\n",
            "dist 12.366839408874512\n",
            "Sample 368 :\n",
            "dist 5.826895713806152\n",
            "Sample 369 :\n",
            "dist 8.920267105102539\n",
            "Sample 370 :\n",
            "dist 11.485407829284668\n",
            "Sample 371 :\n",
            "dist 7.45473051071167\n",
            "Sample 372 :\n",
            "dist 8.812785148620605\n",
            "Sample 373 :\n",
            "dist 12.372003555297852\n",
            "Sample 374 :\n",
            "dist 8.812785148620605\n",
            "Sample 375 :\n",
            "dist 7.15024995803833\n",
            "Sample 376 :\n",
            "dist 9.213743209838867\n",
            "Sample 377 :\n",
            "dist 10.506524085998535\n",
            "Sample 378 :\n",
            "dist 11.942307472229004\n",
            "Sample 379 :\n",
            "dist 9.054219245910645\n",
            "Sample 380 :\n",
            "dist 5.922364711761475\n",
            "Sample 381 :\n",
            "dist 13.467585563659668\n",
            "Sample 382 :\n",
            "dist 13.467585563659668\n",
            "Sample 383 :\n",
            "dist 4.894937038421631\n",
            "Sample 384 :\n",
            "dist 11.129412651062012\n",
            "Sample 385 :\n",
            "dist 10.852871894836426\n",
            "Sample 386 :\n",
            "dist 8.812785148620605\n",
            "Sample 387 :\n",
            "dist 6.647155284881592\n",
            "Sample 388 :\n",
            "dist 11.207891464233398\n",
            "Sample 389 :\n",
            "dist 11.099358558654785\n",
            "Sample 390 :\n",
            "dist 10.506524085998535\n",
            "Sample 391 :\n",
            "dist 8.64344596862793\n",
            "Sample 392 :\n",
            "dist 9.856998443603516\n",
            "Sample 393 :\n",
            "dist 5.791200160980225\n",
            "Sample 394 :\n",
            "dist 8.047513008117676\n",
            "Sample 395 :\n",
            "dist 11.594229698181152\n",
            "Sample 396 :\n",
            "dist 8.992192268371582\n",
            "Sample 397 :\n",
            "dist 9.100789070129395\n",
            "Sample 398 :\n",
            "dist 9.214948654174805\n",
            "Sample 399 :\n",
            "dist 7.801884174346924\n",
            "Sample 400 :\n",
            "dist 9.100789070129395\n",
            "Sample 401 :\n",
            "dist 7.15024995803833\n",
            "Sample 402 :\n",
            "dist 5.0141496658325195\n",
            "Sample 403 :\n",
            "dist 8.756172180175781\n",
            "Sample 404 :\n",
            "dist 10.852871894836426\n",
            "Sample 405 :\n",
            "dist 14.848955154418945\n",
            "Sample 406 :\n",
            "dist 5.060171127319336\n",
            "Sample 407 :\n",
            "dist 5.0141496658325195\n",
            "Sample 408 :\n",
            "dist 9.856998443603516\n",
            "Sample 409 :\n",
            "dist 9.039032936096191\n",
            "Sample 410 :\n",
            "dist 10.09423542022705\n",
            "Sample 411 :\n",
            "dist 10.550861358642578\n",
            "Sample 412 :\n",
            "dist 10.813817024230957\n",
            "Sample 413 :\n",
            "dist 12.366839408874512\n",
            "Sample 414 :\n",
            "dist 15.017980575561523\n",
            "Sample 415 :\n",
            "dist 10.186614990234375\n",
            "Sample 416 :\n",
            "dist 9.039032936096191\n",
            "Sample 417 :\n",
            "dist 10.00361156463623\n",
            "Sample 418 :\n",
            "dist 11.67253303527832\n",
            "Sample 419 :\n",
            "dist 10.547954559326172\n",
            "Sample 420 :\n",
            "dist 13.343537330627441\n",
            "Sample 421 :\n",
            "dist 9.100789070129395\n",
            "Sample 422 :\n",
            "dist 13.467585563659668\n",
            "Sample 423 :\n",
            "dist 9.039032936096191\n",
            "Sample 424 :\n",
            "dist 9.550772666931152\n",
            "Sample 425 :\n",
            "dist 10.585667610168457\n",
            "Sample 426 :\n",
            "dist 10.206989288330078\n",
            "Sample 427 :\n",
            "dist 11.162457466125488\n",
            "Sample 428 :\n",
            "dist 7.356421947479248\n",
            "Sample 429 :\n",
            "dist 9.856998443603516\n",
            "Sample 430 :\n",
            "dist 7.45473051071167\n",
            "Sample 431 :\n",
            "dist 10.75291919708252\n",
            "Sample 432 :\n",
            "dist 6.714141368865967\n",
            "Sample 433 :\n",
            "dist 5.060171127319336\n",
            "Sample 434 :\n",
            "dist 10.186614990234375\n",
            "Sample 435 :\n",
            "dist 9.054219245910645\n",
            "Sample 436 :\n",
            "dist 7.15024995803833\n",
            "Sample 437 :\n",
            "dist 10.215923309326172\n",
            "Sample 438 :\n",
            "dist 12.336888313293457\n",
            "Sample 439 :\n",
            "dist 10.06309700012207\n",
            "Sample 440 :\n",
            "dist 11.084732055664062\n",
            "Sample 441 :\n",
            "dist 7.801884174346924\n",
            "Sample 442 :\n",
            "dist 7.5439677238464355\n",
            "Sample 443 :\n",
            "dist 10.00361156463623\n",
            "Sample 444 :\n",
            "dist 7.801884174346924\n",
            "Sample 445 :\n",
            "dist 10.415663719177246\n",
            "Sample 446 :\n",
            "dist 14.197386741638184\n",
            "Sample 447 :\n",
            "dist 10.565771102905273\n",
            "Sample 448 :\n",
            "dist 11.91559886932373\n",
            "Sample 449 :\n",
            "dist 9.089716911315918\n",
            "Sample 450 :\n",
            "dist 11.942307472229004\n",
            "Sample 451 :\n",
            "dist 11.737712860107422\n",
            "Sample 452 :\n",
            "dist 5.922364711761475\n",
            "Sample 453 :\n",
            "dist 14.197386741638184\n",
            "Sample 454 :\n",
            "dist 11.91559886932373\n",
            "Sample 455 :\n",
            "dist 7.45473051071167\n",
            "Sample 456 :\n",
            "dist 10.1978759765625\n",
            "Sample 457 :\n",
            "dist 8.992192268371582\n",
            "Sample 458 :\n",
            "dist 9.864704132080078\n",
            "Sample 459 :\n",
            "dist 9.004803657531738\n",
            "Sample 460 :\n",
            "dist 11.129412651062012\n",
            "Sample 461 :\n",
            "dist 9.856998443603516\n",
            "Sample 462 :\n",
            "dist 10.287613868713379\n",
            "Sample 463 :\n",
            "dist 9.739049911499023\n",
            "Sample 464 :\n",
            "dist 6.924028396606445\n",
            "Sample 465 :\n",
            "dist 7.356421947479248\n",
            "Sample 466 :\n",
            "dist 11.099358558654785\n",
            "Sample 467 :\n",
            "dist 9.004803657531738\n",
            "Sample 468 :\n",
            "dist 7.356421947479248\n",
            "Sample 469 :\n",
            "dist 9.739049911499023\n",
            "Sample 470 :\n",
            "dist 9.10306453704834\n",
            "Sample 471 :\n",
            "dist 11.594229698181152\n",
            "Sample 472 :\n",
            "dist 5.0141496658325195\n",
            "Sample 473 :\n",
            "dist 11.162457466125488\n",
            "Sample 474 :\n",
            "dist 9.328691482543945\n",
            "Sample 475 :\n",
            "dist 10.186614990234375\n",
            "Sample 476 :\n",
            "dist 10.06309700012207\n",
            "Sample 477 :\n",
            "dist 12.167671203613281\n",
            "Sample 478 :\n",
            "dist 10.09423542022705\n",
            "Sample 479 :\n",
            "dist 10.423500061035156\n",
            "Sample 480 :\n",
            "dist 10.07865047454834\n",
            "Sample 481 :\n",
            "dist 5.826895713806152\n",
            "Sample 482 :\n",
            "dist 10.186614990234375\n",
            "Sample 483 :\n",
            "dist 6.647155284881592\n",
            "Sample 484 :\n",
            "dist 6.086319923400879\n",
            "Sample 485 :\n",
            "dist 8.865540504455566\n",
            "Sample 486 :\n",
            "dist 10.215923309326172\n",
            "Sample 487 :\n",
            "dist 11.24622917175293\n",
            "Sample 488 :\n",
            "dist 10.547954559326172\n",
            "Sample 489 :\n",
            "dist 10.473353385925293\n",
            "Sample 490 :\n",
            "dist 10.139031410217285\n",
            "Sample 491 :\n",
            "dist 9.054219245910645\n",
            "Sample 492 :\n",
            "dist 6.647155284881592\n",
            "Sample 493 :\n",
            "dist 9.215288162231445\n",
            "Sample 494 :\n",
            "dist 7.31467342376709\n",
            "Sample 495 :\n",
            "dist 12.123115539550781\n",
            "Sample 496 :\n",
            "dist 9.864704132080078\n",
            "Sample 497 :\n",
            "dist 11.594229698181152\n",
            "Sample 498 :\n",
            "dist 6.714141368865967\n",
            "Sample 499 :\n",
            "dist 10.415663719177246\n",
            "Sample 500 :\n",
            "dist 10.628581047058105\n",
            "Sample 501 :\n",
            "dist 14.29787540435791\n",
            "Sample 502 :\n",
            "dist 9.659614562988281\n",
            "Sample 503 :\n",
            "dist 10.816109657287598\n",
            "Sample 504 :\n",
            "dist 8.77670955657959\n",
            "Sample 505 :\n",
            "dist 9.039032936096191\n",
            "Sample 506 :\n",
            "dist 11.168105125427246\n",
            "Sample 507 :\n",
            "dist 9.004803657531738\n",
            "Sample 508 :\n",
            "dist 9.691022872924805\n",
            "Sample 509 :\n",
            "dist 12.123115539550781\n",
            "Sample 510 :\n",
            "dist 7.356421947479248\n",
            "Sample 511 :\n",
            "dist 8.16422176361084\n",
            "Sample 512 :\n",
            "dist 10.186614990234375\n",
            "Sample 513 :\n",
            "dist 9.215288162231445\n",
            "Sample 514 :\n",
            "dist 13.467585563659668\n",
            "Sample 515 :\n",
            "dist 9.856998443603516\n",
            "Sample 516 :\n",
            "dist 12.336888313293457\n",
            "Sample 517 :\n",
            "dist 7.489496231079102\n",
            "Sample 518 :\n",
            "dist 10.1978759765625\n",
            "Sample 519 :\n",
            "dist 10.813817024230957\n",
            "Sample 520 :\n",
            "dist 9.039032936096191\n",
            "Sample 521 :\n",
            "dist 14.197386741638184\n",
            "Sample 522 :\n",
            "dist 9.572949409484863\n",
            "Sample 523 :\n",
            "dist 8.976008415222168\n",
            "Sample 524 :\n",
            "dist 8.992192268371582\n",
            "Sample 525 :\n",
            "dist 9.822888374328613\n",
            "Sample 526 :\n",
            "dist 7.826977729797363\n",
            "Sample 527 :\n",
            "dist 10.36896800994873\n",
            "Sample 528 :\n",
            "dist 11.162457466125488\n",
            "Sample 529 :\n",
            "dist 8.019840240478516\n",
            "Sample 530 :\n",
            "dist 8.903407096862793\n",
            "Sample 531 :\n",
            "dist 12.167671203613281\n",
            "Sample 532 :\n",
            "dist 5.922364711761475\n",
            "Sample 533 :\n",
            "dist 12.123115539550781\n",
            "Sample 534 :\n",
            "dist 8.976008415222168\n",
            "Sample 535 :\n",
            "dist 9.039032936096191\n",
            "Sample 536 :\n",
            "dist 8.77670955657959\n",
            "Sample 537 :\n",
            "dist 9.659614562988281\n",
            "Sample 538 :\n",
            "dist 8.77670955657959\n",
            "Sample 539 :\n",
            "dist 3.7555344104766846\n",
            "Sample 540 :\n",
            "dist 10.423500061035156\n",
            "Sample 541 :\n",
            "dist 9.864704132080078\n",
            "Sample 542 :\n",
            "dist 10.186614990234375\n",
            "Sample 543 :\n",
            "dist 15.017980575561523\n",
            "Sample 544 :\n",
            "dist 6.722531318664551\n",
            "Sample 545 :\n",
            "dist 11.737712860107422\n",
            "Sample 546 :\n",
            "dist 9.659614562988281\n",
            "Sample 547 :\n",
            "dist 10.565771102905273\n",
            "Sample 548 :\n",
            "dist 9.214948654174805\n",
            "Sample 549 :\n",
            "dist 9.691022872924805\n",
            "Sample 550 :\n",
            "dist 10.814924240112305\n",
            "Sample 551 :\n",
            "dist 9.572949409484863\n",
            "Sample 552 :\n",
            "dist 11.108943939208984\n",
            "Sample 553 :\n",
            "dist 7.444174289703369\n",
            "Sample 554 :\n",
            "dist 10.565771102905273\n",
            "Sample 555 :\n",
            "dist 5.922364711761475\n",
            "Sample 556 :\n",
            "dist 10.592268943786621\n",
            "Sample 557 :\n",
            "dist 10.206989288330078\n",
            "Sample 558 :\n",
            "dist 11.594229698181152\n",
            "Sample 559 :\n",
            "dist 10.547954559326172\n",
            "Sample 560 :\n",
            "dist 10.852871894836426\n",
            "Sample 561 :\n",
            "dist 10.565771102905273\n",
            "Sample 562 :\n",
            "dist 9.328691482543945\n",
            "Sample 563 :\n",
            "dist 8.019840240478516\n",
            "Sample 564 :\n",
            "dist 8.64344596862793\n",
            "Sample 565 :\n",
            "dist 11.942307472229004\n",
            "Sample 566 :\n",
            "dist 9.249247550964355\n",
            "Sample 567 :\n",
            "dist 9.214948654174805\n",
            "Sample 568 :\n",
            "dist 10.053415298461914\n",
            "Sample 569 :\n",
            "dist 15.772144317626953\n",
            "Sample 570 :\n",
            "dist 10.11634349822998\n",
            "Sample 571 :\n",
            "dist 12.167671203613281\n",
            "Sample 572 :\n",
            "dist 12.167671203613281\n",
            "Sample 573 :\n",
            "dist 7.826977729797363\n",
            "Sample 574 :\n",
            "dist 7.801884174346924\n",
            "Sample 575 :\n",
            "dist 7.45473051071167\n",
            "Sample 576 :\n",
            "dist 9.659614562988281\n",
            "Sample 577 :\n",
            "dist 9.328691482543945\n",
            "Sample 578 :\n",
            "dist 12.618820190429688\n",
            "Sample 579 :\n",
            "dist 10.585667610168457\n",
            "Sample 580 :\n",
            "dist 9.659614562988281\n",
            "Sample 581 :\n",
            "dist 9.005356788635254\n",
            "Sample 582 :\n",
            "dist 6.714141368865967\n",
            "Sample 583 :\n",
            "dist 11.099358558654785\n",
            "Sample 584 :\n",
            "dist 8.019840240478516\n",
            "Sample 585 :\n",
            "dist 10.395532608032227\n",
            "Sample 586 :\n",
            "dist 7.444174289703369\n",
            "Sample 587 :\n",
            "dist 9.039032936096191\n",
            "Sample 588 :\n",
            "dist 8.992192268371582\n",
            "Sample 589 :\n",
            "dist 9.054219245910645\n",
            "Sample 590 :\n",
            "dist 7.444174289703369\n",
            "Sample 591 :\n",
            "dist 6.714141368865967\n",
            "Sample 592 :\n",
            "dist 9.328691482543945\n",
            "Sample 593 :\n",
            "dist 10.423500061035156\n",
            "Sample 594 :\n",
            "dist 10.550861358642578\n",
            "Sample 595 :\n",
            "dist 9.856998443603516\n",
            "Sample 596 :\n",
            "dist 10.852871894836426\n",
            "Sample 597 :\n",
            "dist 10.585667610168457\n",
            "Sample 598 :\n",
            "dist 10.215923309326172\n",
            "Sample 599 :\n",
            "dist 10.05289363861084\n",
            "Sample 600 :\n",
            "dist 8.16422176361084\n",
            "Sample 601 :\n",
            "dist 6.086319923400879\n",
            "Sample 602 :\n",
            "dist 7.295797348022461\n",
            "Sample 603 :\n",
            "dist 8.64344596862793\n",
            "Sample 604 :\n",
            "dist 9.572949409484863\n",
            "Sample 605 :\n",
            "dist 10.813817024230957\n",
            "Sample 606 :\n",
            "dist 8.812785148620605\n",
            "Sample 607 :\n",
            "dist 10.11634349822998\n",
            "Sample 608 :\n",
            "dist 9.249247550964355\n",
            "Sample 609 :\n",
            "dist 11.485407829284668\n",
            "Sample 610 :\n",
            "dist 9.822664260864258\n",
            "Sample 611 :\n",
            "dist 12.372003555297852\n",
            "Sample 612 :\n",
            "dist 7.356421947479248\n",
            "Sample 613 :\n",
            "dist 7.826977729797363\n",
            "Sample 614 :\n",
            "dist 8.756172180175781\n",
            "Sample 615 :\n",
            "dist 8.865540504455566\n",
            "Sample 616 :\n",
            "dist 10.814924240112305\n",
            "Sample 617 :\n",
            "dist 6.714141368865967\n",
            "Sample 618 :\n",
            "dist 12.860376358032227\n",
            "Sample 619 :\n",
            "dist 10.1978759765625\n",
            "Sample 620 :\n",
            "dist 8.16422176361084\n",
            "Sample 621 :\n",
            "dist 8.992192268371582\n",
            "Sample 622 :\n",
            "dist 11.24622917175293\n",
            "Sample 623 :\n",
            "dist 11.129412651062012\n",
            "Sample 624 :\n",
            "dist 12.226571083068848\n",
            "Sample 625 :\n",
            "dist 11.207891464233398\n",
            "Sample 626 :\n",
            "dist 7.45473051071167\n",
            "Sample 627 :\n",
            "dist 11.942307472229004\n",
            "Sample 628 :\n",
            "dist 8.16422176361084\n",
            "Sample 629 :\n",
            "dist 10.565771102905273\n",
            "Sample 630 :\n",
            "dist 15.22869873046875\n",
            "Sample 631 :\n",
            "dist 7.31467342376709\n",
            "Sample 632 :\n",
            "dist 10.1978759765625\n",
            "Sample 633 :\n",
            "dist 4.329094409942627\n",
            "Sample 634 :\n",
            "dist 11.737712860107422\n",
            "Sample 635 :\n",
            "dist 8.865540504455566\n",
            "Sample 636 :\n",
            "dist 8.920267105102539\n",
            "Sample 637 :\n",
            "dist 10.09423542022705\n",
            "Sample 638 :\n",
            "dist 9.039032936096191\n",
            "Sample 639 :\n",
            "dist 14.605659484863281\n",
            "Sample 640 :\n",
            "dist 9.822888374328613\n",
            "Sample 641 :\n",
            "dist 7.31467342376709\n",
            "Sample 642 :\n",
            "dist 8.16422176361084\n",
            "Sample 643 :\n",
            "dist 7.928226947784424\n",
            "Sample 644 :\n",
            "dist 10.1978759765625\n",
            "Sample 645 :\n",
            "dist 8.047513008117676\n",
            "Sample 646 :\n",
            "dist 9.691022872924805\n",
            "Sample 647 :\n",
            "dist 7.45473051071167\n",
            "Sample 648 :\n",
            "dist 9.039032936096191\n",
            "Sample 649 :\n",
            "dist 15.017980575561523\n",
            "Sample 650 :\n",
            "dist 10.628581047058105\n",
            "Sample 651 :\n",
            "dist 5.922364711761475\n",
            "Sample 652 :\n",
            "dist 8.16422176361084\n",
            "Sample 653 :\n",
            "dist 9.100789070129395\n",
            "Sample 654 :\n",
            "dist 10.423500061035156\n",
            "Sample 655 :\n",
            "dist 9.856998443603516\n",
            "Sample 656 :\n",
            "dist 10.585667610168457\n",
            "Sample 657 :\n",
            "dist 9.739049911499023\n",
            "Sample 658 :\n",
            "dist 10.36896800994873\n",
            "Sample 659 :\n",
            "dist 9.039032936096191\n",
            "Sample 660 :\n",
            "dist 5.826895713806152\n",
            "Sample 661 :\n",
            "dist 11.942307472229004\n",
            "Sample 662 :\n",
            "dist 11.24622917175293\n",
            "Sample 663 :\n",
            "dist 9.822664260864258\n",
            "Sample 664 :\n",
            "dist 5.0141496658325195\n",
            "Sample 665 :\n",
            "dist 10.547954559326172\n",
            "Sample 666 :\n",
            "dist 11.737712860107422\n",
            "Sample 667 :\n",
            "dist 7.45473051071167\n",
            "Sample 668 :\n",
            "dist 9.214948654174805\n",
            "Sample 669 :\n",
            "dist 9.739049911499023\n",
            "Sample 670 :\n",
            "dist 9.005356788635254\n",
            "Sample 671 :\n",
            "dist 9.822664260864258\n",
            "Sample 672 :\n",
            "dist 11.594229698181152\n",
            "Sample 673 :\n",
            "dist 10.423500061035156\n",
            "Sample 674 :\n",
            "dist 7.801884174346924\n",
            "Sample 675 :\n",
            "dist 7.356421947479248\n",
            "Sample 676 :\n",
            "dist 8.395001411437988\n",
            "Sample 677 :\n",
            "dist 9.005356788635254\n",
            "Sample 678 :\n",
            "dist 10.00361156463623\n",
            "Sample 679 :\n",
            "dist 10.585667610168457\n",
            "Sample 680 :\n",
            "dist 5.6047210693359375\n",
            "Sample 681 :\n",
            "dist 9.089716911315918\n",
            "Sample 682 :\n",
            "dist 11.942307472229004\n",
            "Sample 683 :\n",
            "dist 9.659614562988281\n",
            "Sample 684 :\n",
            "dist 6.714141368865967\n",
            "Sample 685 :\n",
            "dist 9.089716911315918\n",
            "Sample 686 :\n",
            "dist 4.894937038421631\n",
            "Sample 687 :\n",
            "dist 9.039032936096191\n",
            "Sample 688 :\n",
            "dist 9.100789070129395\n",
            "Sample 689 :\n",
            "dist 8.019840240478516\n",
            "Sample 690 :\n",
            "dist 10.506524085998535\n",
            "Sample 691 :\n",
            "dist 11.737712860107422\n",
            "Sample 692 :\n",
            "dist 9.659614562988281\n",
            "Sample 693 :\n",
            "dist 10.1978759765625\n",
            "Sample 694 :\n",
            "dist 11.67253303527832\n",
            "Sample 695 :\n",
            "dist 11.129412651062012\n",
            "Sample 696 :\n",
            "dist 9.739049911499023\n",
            "Sample 697 :\n",
            "dist 11.207891464233398\n",
            "Sample 698 :\n",
            "dist 9.572949409484863\n",
            "Sample 699 :\n",
            "dist 10.816109657287598\n",
            "Sample 700 :\n",
            "dist 12.123115539550781\n",
            "Sample 701 :\n",
            "dist 9.328691482543945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CULsI8xHWm9w",
        "colab_type": "code",
        "outputId": "daf8e8eb-6208-4154-8eee-15523714d0cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df_dist.sort_values(by=0, ascending=\"True\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3.755534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>3.755534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>3.755534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>3.755534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633</th>\n",
              "      <td>4.329094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>4.514034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>4.514034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>4.514034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>4.514034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>4.894937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>686</th>\n",
              "      <td>4.894937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>4.894937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4.894937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>407</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>5.060171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>5.060171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>680</th>\n",
              "      <td>5.604721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5.604721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>5.604721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>5.791200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>5.791200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>5.826896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>660</th>\n",
              "      <td>5.826896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>13.467586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>13.546152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>13.546152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>14.048661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>14.297875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>14.297875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>14.359901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639</th>\n",
              "      <td>14.605659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>14.605659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>14.807356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>14.807356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>14.807356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>405</th>\n",
              "      <td>14.848955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>14.941377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630</th>\n",
              "      <td>15.228699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>15.705333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>15.705333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>15.772144</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>702 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0\n",
              "18    3.755534\n",
              "269   3.755534\n",
              "103   3.755534\n",
              "539   3.755534\n",
              "633   4.329094\n",
              "34    4.514034\n",
              "251   4.514034\n",
              "204   4.514034\n",
              "234   4.514034\n",
              "47    4.894937\n",
              "686   4.894937\n",
              "383   4.894937\n",
              "28    4.894937\n",
              "151   5.014150\n",
              "664   5.014150\n",
              "35    5.014150\n",
              "472   5.014150\n",
              "361   5.014150\n",
              "402   5.014150\n",
              "407   5.014150\n",
              "2     5.014150\n",
              "406   5.060171\n",
              "433   5.060171\n",
              "680   5.604721\n",
              "26    5.604721\n",
              "229   5.604721\n",
              "393   5.791200\n",
              "297   5.791200\n",
              "481   5.826896\n",
              "660   5.826896\n",
              "..         ...\n",
              "514  13.467586\n",
              "198  13.546152\n",
              "310  13.546152\n",
              "226  14.048661\n",
              "521  14.197387\n",
              "446  14.197387\n",
              "38   14.197387\n",
              "453  14.197387\n",
              "250  14.197387\n",
              "187  14.197387\n",
              "501  14.297875\n",
              "24   14.297875\n",
              "319  14.359901\n",
              "639  14.605659\n",
              "178  14.605659\n",
              "88   14.807356\n",
              "162  14.807356\n",
              "154  14.807356\n",
              "405  14.848955\n",
              "132  14.941377\n",
              "51   15.017981\n",
              "342  15.017981\n",
              "125  15.017981\n",
              "543  15.017981\n",
              "414  15.017981\n",
              "649  15.017981\n",
              "630  15.228699\n",
              "19   15.705333\n",
              "218  15.705333\n",
              "569  15.772144\n",
              "\n",
              "[702 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}