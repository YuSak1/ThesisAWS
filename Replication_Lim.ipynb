{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Replication_Lim.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxQ32n5QT3Eo",
        "colab_type": "text"
      },
      "source": [
        "# This is a replication of Lim's method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8AUbb50-UfE",
        "colab_type": "code",
        "outputId": "81e47010-dd2b-4bd0-e2fb-504a3aa495c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "# Check GPU for Google Colab\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=1c1f9736e153b3284da8969d499abfae7a32fbdc7557fcd13a7085813b3813a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 14.0 GB  | Proc size: 15.9 GB\n",
            "GPU RAM Free: 13667MB | Used: 2613MB | Util  16% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF_L9GoTj4OU",
        "colab_type": "code",
        "outputId": "5f666660-d101-486c-ddea-1d4a52ddf503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Dec  2 16:57:30 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.33.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lvn4_zUPHP6u",
        "colab_type": "code",
        "outputId": "112aa297-2732-4516-fadc-5d046cc0f40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qGHk7xnG_0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import random\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D, concatenate\n",
        "from keras.layers import Activation, BatchNormalization, ZeroPadding2D, Concatenate, Dropout, UpSampling2D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD, Adam, Adadelta\n",
        "from keras.losses import binary_crossentropy\n",
        "from sklearn import preprocessing\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "plt.interactive(False)\n",
        "\n",
        "from itertools import permutations\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from scipy.spatial import distance\n",
        "from PIL import Image, ImageOps\n",
        "import gc\n",
        "import math\n",
        "import time\n",
        "import itertools\n",
        "from operator import itemgetter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BJJZ9VTrhey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "category = \"building\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7QN81ZvHhkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CNN module as specified in the paper\n",
        "def create_base_network(in_dims):\n",
        "    \"\"\"\n",
        "    Base network to be shared.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        " \n",
        "    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=(in_dims[0],in_dims[1],in_dims[2])))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    model.add(Dropout(0.1))\n",
        "    \n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    model.add(Dropout(0.3))\n",
        "    \n",
        "    model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(512, (1, 1), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    \n",
        "    model.add(Flatten(name='embeddings'))\n",
        "    \n",
        "    print(model.summary())\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rt78b7oHhpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compile_model(img_channel=1):\n",
        "  print(\"Compiling model...\")\n",
        "  img_size = 112\n",
        "\n",
        "  optim = Adadelta(lr=1.0, rho=0.95, epsilon=0.000001, decay=0.0) # as specified in the paper\n",
        "\n",
        "  anchor_input = Input((img_size,img_size,img_channel, ), name='anchor_input')\n",
        "  positive_input = Input((img_size,img_size,img_channel, ), name='positive_input')\n",
        "  negative_input = Input((img_size,img_size,img_channel, ), name='negative_input')\n",
        "\n",
        "  # Shared embedding layer for positive and negative items\n",
        "  Shared_DNN = create_base_network([img_size,img_size,img_channel])\n",
        "\n",
        "  encoded_anchor = Shared_DNN(anchor_input)\n",
        "  encoded_positive = Shared_DNN(positive_input)\n",
        "  encoded_negative = Shared_DNN(negative_input)\n",
        "\n",
        "  merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
        "\n",
        "  model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
        "  model.compile(loss=triplet_loss, optimizer=optim)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nmqRSI127gHU",
        "colab": {}
      },
      "source": [
        "def evaluate_triplet(X_test, Y_test):\n",
        "  \n",
        "#   X_test = [Negative_train, Positive_train, Anchor_train]\n",
        "#   Y_test = np.zeros((Anchor_train.shape[0],1))\n",
        "  \n",
        "  pred = model.predict(x=X_test, verbose=1)\n",
        "  \n",
        "  # Split into anchor, a, and b sets\n",
        "  total_lenght = pred.shape[1]\n",
        "  pred_anchor = pred[:,0:int(total_lenght*1/3)]\n",
        "  pred_a = pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "  pred_b = pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "  \n",
        "  \n",
        "  # Normalize vectors as specified in the paper\n",
        "  for i in range(len(pred_anchor)):\n",
        "    if np.sum(pred_anchor[i]**2) >= 1:\n",
        "      pred_anchor[i] = preprocessing.normalize([pred_anchor[i]], norm='l2')[0]\n",
        "    if np.sum(pred_a[i]**2) >= 1:\n",
        "      pred_a[i] = preprocessing.normalize([pred_a[i]], norm='l2')[0]\n",
        "    if np.sum(pred_b[i]**2) >= 1:\n",
        "      pred_b[i] = preprocessing.normalize([pred_b[i]], norm='l2')[0]\n",
        "\n",
        "  \n",
        "  result = []\n",
        "  for i in range(pred.shape[0]):\n",
        "    dist1 = distance.euclidean(pred_anchor[i], pred_a[i])\n",
        "    dist2 = distance.euclidean(pred_anchor[i], pred_b[i])\n",
        "\n",
        "    if dist1 < dist2:\n",
        "      result.append(0)\n",
        "    else:\n",
        "      result.append(1)   \n",
        "  \n",
        "  accuracy = accuracy_score(Y_test, result)\n",
        "  print(\"Evaluation accuracy: \", accuracy)\n",
        "\n",
        "  return accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sCxnEs8Mrt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Create two plots; accuracy and loss\n",
        "def save_plot(eval_name, fold_id, view_id):\n",
        "  fig = plt.figure()\n",
        "\n",
        "  # top\n",
        "  ax1 = fig.add_subplot(2, 1, 1)\n",
        "  ax1.plot(df_history['train_loss'])\n",
        "  ax1.plot(df_history['val_loss'])\n",
        "  ax1.set_xlabel(\"epoch\")\n",
        "  ax1.set_ylabel(\"loss\")\n",
        "  plt.legend(['train_loss', 'val_loss'], loc='upper right', fontsize='x-small')\n",
        "  plt.title(\"Fold-\"+str(fold_id)+\", View-\"+str(view_id))\n",
        "\n",
        "  # bottom\n",
        "  ax2 = fig.add_subplot(2, 1, 2)\n",
        "  ax2.plot(df_history['val_acc'])\n",
        "  ax2.set_xlabel(\"epoch\")\n",
        "  ax2.set_ylabel(\"accuracy\")\n",
        "  plt.legend(['val_accuracy'], loc='upper left', fontsize='x-small')\n",
        "\n",
        "#   fig.show()\n",
        "  fig.savefig(\"drive/My Drive/Saved_Images/CV_eval_img/\" + eval_name + \"/Fold-\"+str(fold_id) + \"-View-\"+str(view_id))\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOji9Qq2HhfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
        "    \"\"\"\n",
        "    Implementation of the triplet loss function\n",
        "    Arguments:\n",
        "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
        "    y_pred -- python list containing three objects:\n",
        "            anchor -- the encodings for the anchor data\n",
        "            positive -- the encodings for the positive data (similar to anchor)\n",
        "            negative -- the encodings for the negative data (different from anchor)\n",
        "    Returns:\n",
        "    loss -- real number, value of the loss\n",
        "    \"\"\"\n",
        "    \n",
        "#     total_lenght = y_pred.shape.as_list()[-1]\n",
        "    total_lenght = 512*3\n",
        "    \n",
        "    anchor = y_pred[:,0:int(total_lenght*1/3)]\n",
        "    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "     \n",
        "    # Normalize vectors as specified in the paper\n",
        "    anchor = K.switch(K.greater_equal(K.sum(K.square(anchor)), 1), lambda: K.l2_normalize(anchor, axis=0), lambda: anchor)\n",
        "    positive = K.switch(K.greater_equal(K.sum(K.square(positive)), 1), lambda: K.l2_normalize(positive, axis=0), lambda: positive)\n",
        "    negative = K.switch(K.greater_equal(K.sum(K.square(negative)), 1), lambda: K.l2_normalize(negative, axis=0), lambda: negative)\n",
        "\n",
        "    # distance between the anchor and the positive\n",
        "    pos_dist = K.sqrt(K.sum(K.square(anchor-positive),axis=1))\n",
        "\n",
        "    # distance between the anchor and the negative\n",
        "    neg_dist = K.sqrt(K.sum(K.square(anchor-negative),axis=1))\n",
        "\n",
        "    # compute loss\n",
        "    basic_loss = pos_dist-neg_dist+alpha\n",
        "    loss = K.maximum(basic_loss,0.0)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Vv9DnMEOAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_triplet_vote(X_test, Y_test, num_views):\n",
        "  \n",
        "  pred = model.predict(x=X_test, verbose=1)\n",
        "  \n",
        "  # Split into anchor, a, and b sets\n",
        "  total_lenght = pred.shape[1]\n",
        "  pred_anchor = pred[:,0:int(total_lenght*1/3)]\n",
        "  pred_a = pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "  pred_b = pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "  \n",
        "  # Normalize vectors as specified in the paper\n",
        "  for i in range(len(pred_anchor)):\n",
        "    if np.sum(pred_anchor[i]**2) >= 1:\n",
        "      pred_anchor[i] = preprocessing.normalize([pred_anchor[i]], norm='l2')[0]\n",
        "    if np.sum(pred_a[i]**2) >= 1:\n",
        "      pred_a[i] = preprocessing.normalize([pred_a[i]], norm='l2')[0]\n",
        "    if np.sum(pred_b[i]**2) >= 1:\n",
        "      pred_b[i] = preprocessing.normalize([pred_b[i]], norm='l2')[0]\n",
        "  \n",
        "  \n",
        "  result_tmp = []\n",
        "  result = []\n",
        "  for i in range(pred.shape[0]):\n",
        "    dist1 = distance.euclidean(pred_anchor[i], pred_a[i])\n",
        "    dist2 = distance.euclidean(pred_anchor[i], pred_b[i])\n",
        "\n",
        "    if dist1 < dist2:\n",
        "      result_tmp.append(0)\n",
        "    else:\n",
        "      result_tmp.append(1)\n",
        "      \n",
        "    if len(result_tmp)==num_views:\n",
        "      if (sum(result_tmp) < math.ceil(num_views/2)):\n",
        "        result.append(0)\n",
        "      else:\n",
        "        result.append(1)\n",
        "      \n",
        "      result_tmp=[]\n",
        "\n",
        "  print(\"Voting evaluation accuracy: \", accuracy_score(Y_test[0:int(len(Y_test)/num_views)], result))\n",
        "\n",
        "  return accuracy_score(Y_test[0:int(len(Y_test)/num_views)], result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxp6A0QgU8P0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator2(fold_id,view_ids=[1,2,3], num_triplets=5):  \n",
        "  ## Generate training/test data\n",
        "  df_train = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_\" + category + \"/df_train_\" + str(fold_id) + \".csv\").reset_index(drop=True)\n",
        "  df_test = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_\" + category + \"/df_test_\" + str(fold_id) + \".csv\").reset_index(drop=True)\n",
        "\n",
        "  df_train = df_train.reset_index(drop=\"True\")\n",
        "  df_test = df_test.reset_index(drop=\"True\")\n",
        "\n",
        "  print(\"Training original size: \", df_train.shape)\n",
        "  print(\"Test original size: \", df_test.shape)\n",
        "  ### Create image dataset for triplet learning \n",
        "  img_size = 128\n",
        "  img_mode = \"L\" #[\"RGB\", \"L\"]\n",
        "  img_path = \"drive/My Drive/Style_data2_views/\" + category +\"_90views_sep/\"\n",
        "  \n",
        "  ### All combinations of views ###\n",
        "  view_comb = []\n",
        "  for i in itertools.product(view_ids, repeat=3):\n",
        "    view_comb.append(i)\n",
        "\n",
        "  out_list = [] # final output\n",
        "  ################ Generate Training Data ################\n",
        "  name_list = [\"query\", \"pos\", \"neg\"]\n",
        "  data_length = len(df_train)\n",
        "  \n",
        "  print(\"Generating training data...\")\n",
        "  \n",
        "  out = [[], [], []]\n",
        "  cur_triplets_all = []\n",
        "  \n",
        "  for i in range(data_length):\n",
        "    cur_triplets_sub = []\n",
        "    rand_list = random.sample(range(0, len(view_comb)), num_triplets)\n",
        "    for j in range(num_triplets):\n",
        "      view_out = view_comb[rand_list[j]]           # randomly selected view combination\n",
        "      cur_triplets_sub.append(view_out)\n",
        "      \n",
        "      for k in range(0,3):\n",
        "          data_id = df_train.loc[:,name_list[k]][i]\n",
        "          img_file = data_id + \"/\" + data_id + \"_\" + str(view_out[k]) + \".png\"\n",
        "          img = img_path + img_file\n",
        "          img = Image.open(img)\n",
        "          img = ImageOps.expand(img, border=30, fill=0)  # padding with 0s\n",
        "          img = img.convert(img_mode)\n",
        "          img = img.resize((img_size, img_size))\n",
        "          data = np.asarray(img)/255\n",
        "          \n",
        "          # Randomly crop 112x112 images\n",
        "          rand1 = random.randrange(17)\n",
        "          rand2 = random.randrange(17)\n",
        "          data_crop = np.asarray(data[rand1:rand1+112, rand2:rand2+112])\n",
        "\n",
        "          # Flip image by 50% probability\n",
        "          rand_flip = random.randrange(2)\n",
        "          if rand_flip==0:\n",
        "            out[k].append(data_crop)\n",
        "          else:\n",
        "            out[k].append(np.fliplr(data_crop)) ## Flipped image\n",
        "            \n",
        "    cur_triplets_all.append(cur_triplets_sub)\n",
        "    \n",
        "    if i%5 == 0:\n",
        "      print(\"\\r{0}\".format(i), \"/\", data_length,  end=\"\")\n",
        "    \n",
        "  out_list.append(np.array(out[0]))\n",
        "  out_list.append(np.array(out[1]))\n",
        "  out_list.append(np.array(out[2]))\n",
        "    \n",
        "\n",
        "\n",
        "  ############### Generate Test Data #########################\n",
        "  data_length = len(df_test)\n",
        "\n",
        "  for k in range(0,3):\n",
        "    print(\"Generating test data\", k+1, \"/3...\")\n",
        "    out345 = []\n",
        "    for i in range(data_length):\n",
        "      for j in view_ids:\n",
        "          data_id = df_test.loc[:,name_list[k]][i]\n",
        "          img_file = data_id + \"_\" + str(j) + \".png\"\n",
        "          img = img_path + img_file\n",
        "          img = Image.open(img)\n",
        "          img = ImageOps.expand(img, border=25, fill=0) # padding with 0s\n",
        "          img = img.convert(img_mode)\n",
        "          img = img.resize((img_size, img_size))\n",
        "          data = np.asarray(img)/255\n",
        "          data_crop = np.asarray(data[8:120, 8:120]) # crop a image on the centre\n",
        "          out345.append(data_crop)\n",
        "          \n",
        "    out_list.append(np.array(out345))\n",
        "\n",
        "  return [out_list, np.array(cur_triplets_all)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJFZDCu_MCjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ## Regenerate training set every 10 epoch as proposed in the paper \n",
        "\n",
        "# def regenerate_set(fold_id, train_set, cur_triplets, num_triplets, view_ids=[1,2,3]):\n",
        "#   margin = 0.2 # margin for gap criteria\n",
        "# #   print(\"Current:\", cur_triplets)\n",
        "  \n",
        "#   df_train = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_\" + category + \"/df_train_\" + str(fold_id) + \".csv\").reset_index(drop=\"True\").reset_index(drop=True)\n",
        "  \n",
        "#   ### Create image dataset for triplet learning \n",
        "#   img_size = 128\n",
        "#   img_mode = \"L\" #[\"RGB\", \"L\"]\n",
        "#   img_path = \"drive/My Drive/Style_data2_views/\" + category + \"/\"\n",
        "  \n",
        "#   print(\"=========== Regenerating training set ===========\")\n",
        "  \n",
        "#   #########################  Select 50% of view combinations from each triplet to retain  ##################################\n",
        "#   Anchor_train_all = train_set[0]\n",
        "#   Positive_train_all = train_set[1]\n",
        "#   Negative_train_all = train_set[2]\n",
        "#   pred = model.predict(x=[Anchor_train_all,Positive_train_all,Negative_train_all],verbose=1)\n",
        "  \n",
        "#   total_width = pred.shape[1]\n",
        "#   data_length = len(pred)\n",
        "\n",
        "#   pred_a = pred[:,int(total_width*1/3):int(total_width*2/3)]\n",
        "#   pred_b = pred[:,int(total_width*2/3):int(total_width*3/3)]\n",
        "  \n",
        "#   dist_pos_neg = np.sqrt(np.sum(np.power(pred_a - pred_b, 2),axis=1))\n",
        "\n",
        "#   df_dist = pd.DataFrame()\n",
        "#   df_dist[\"dist\"] = dist_pos_neg\n",
        "#   index_retain = []\n",
        "#   index_cur = []\n",
        "#   for i in range(0, data_length, num_triplets):\n",
        "#     df_dist_sub = df_dist.iloc[i:i+num_triplets]\n",
        "#     df_dist_sub = df_dist_sub.sort_values(by=\"dist\", ascending=False)\n",
        "#     index_retain.append(df_dist_sub.head(math.floor(num_triplets/4)).index.values.tolist())\n",
        "#     index_retain.append(df_dist_sub.tail(math.floor(num_triplets/4)).index.values.tolist())\n",
        "#     index_cur.append(df_dist_sub.index.values.tolist()) # used later\n",
        "  \n",
        "#   cur_triplets_flat = list(itertools.chain.from_iterable(cur_triplets))\n",
        "#   cur_triplets_flat = [l.tolist() for l in cur_triplets_flat]\n",
        "\n",
        "#   index_retain = list(itertools.chain.from_iterable(index_retain))\n",
        "#   index_retain.sort()\n",
        "# #   print(\"index_retain: \", index_retain)\n",
        "#   print(np.array(index_retain).shape)\n",
        "#   print(Anchor_train_all.shape)\n",
        "#   Anchor_train_retain = Anchor_train_all[index_retain]\n",
        "#   Positive_train_retain = Positive_train_all[index_retain]\n",
        "#   Negative_train_retain = Negative_train_all[index_retain]\n",
        "\n",
        "#   remaining_triplets = np.array(list(itertools.chain.from_iterable(np.array(cur_triplets_flat)[index_retain]))).reshape(-1, math.floor(num_triplets/4)*2, 3).tolist()\n",
        "\n",
        "#   print(\"\\nRemaining 50% length:\", len(Anchor_train_retain))\n",
        "# #   print(\"Remaining:\", remaining_triplets)\n",
        "  \n",
        "  \n",
        "#   ###################################     Select new 50% of view combinations for each triplet    ############################################\n",
        "#   # All combinations of views\n",
        "#   view_comb = []\n",
        "#   new_triplets = []\n",
        "#   Anchor_train_new = []\n",
        "#   Positive_train_new = []\n",
        "#   Negative_train_new = []\n",
        "#   for i in itertools.product(view_ids, repeat=3):\n",
        "#     view_comb.append(i)\n",
        "#   view_comb = np.array(view_comb)\n",
        "#   print(\"view_comb shape: \", view_comb.shape)\n",
        "#   for i in range(int(data_length/num_triplets)):\n",
        "#     new_triplets_sub = [item for item in view_comb.tolist() if item not in cur_triplets[i].tolist()]\n",
        "#     new_triplets.append(new_triplets_sub)\n",
        "\n",
        "#   # Compute the distance of all the view combinations that are not in the current triplet\n",
        "#   name_list = [\"query\", \"pos\", \"neg\"]\n",
        "  \n",
        "#   for i in range(int(data_length/num_triplets)):\n",
        "#     print(\"\\r{0}\".format(i), \"/\", int(data_length/num_triplets),  end=\"\")\n",
        "#     out_new_triplets = [[], [], []]\n",
        "#     for j in range(len(new_triplets[i])):\n",
        "#       view_out = new_triplets[i][j]\n",
        "#       for k in range(0,3):\n",
        "#           data_id = df_train.loc[:,name_list[k]][i]\n",
        "#           img_file = data_id + \"_\" + str(view_out[k]) + \".png\"\n",
        "#           img = img_path + img_file\n",
        "#           img = Image.open(img)\n",
        "#           img = ImageOps.expand(img, border=30, fill=0)  # padding with 0s\n",
        "#           img = img.convert(img_mode)\n",
        "#           img = img.resize((img_size, img_size))\n",
        "#           data = np.asarray(img)/255\n",
        "          \n",
        "#           # Randomly crop 112x112 images\n",
        "#           rand1 = random.randrange(17)\n",
        "#           rand2 = random.randrange(17)\n",
        "#           data_crop = np.asarray(data[rand1:rand1+112, rand2:rand2+112])\n",
        "\n",
        "#           # Flip image by 50% probability\n",
        "#           rand_flip = random.randrange(2)\n",
        "#           if rand_flip==0:\n",
        "#             out_new_triplets[k].append(data_crop)\n",
        "#           else:\n",
        "#             out_new_triplets[k].append(np.fliplr(data_crop)) ## Flipped image\n",
        "     \n",
        "#     # Generate triplets for all view combinations\n",
        "#     Anchor_train_all = out_new_triplets[0]\n",
        "#     Positive_train_all = out_new_triplets[1]\n",
        "#     Negative_train_all = out_new_triplets[2]\n",
        "\n",
        "#     # add a new dimension\n",
        "#     Anchor_train_all = np.array(Anchor_train_all)[:, :, :, np.newaxis]\n",
        "#     Positive_train_all = np.array(Positive_train_all)[:, :, :, np.newaxis]\n",
        "#     Negative_train_all = np.array(Negative_train_all)[:, :, :, np.newaxis]\n",
        "#     pred = model.predict(x=[Anchor_train_all,Positive_train_all,Negative_train_all],verbose=0)\n",
        "\n",
        "#     data_length_new_triplets = len(pred)\n",
        "\n",
        "#     pred_anchor = pred[:,0:int(total_width*1/3)]\n",
        "#     pred_a = pred[:,int(total_width*1/3):int(total_width*2/3)]\n",
        "#     pred_b = pred[:,int(total_width*2/3):int(total_width*3/3)]\n",
        "\n",
        "#     dist_pos = np.sqrt(np.sum(np.power(pred_anchor-pred_a, 2),axis=1))\n",
        "#     dist_neg = np.sqrt(np.sum(np.power(pred_anchor-pred_b, 2),axis=1))\n",
        "#     dist_diff = dist_neg - dist_pos\n",
        "\n",
        "#     df_dist = pd.DataFrame()\n",
        "#     df_dist[\"dist\"] = dist_diff\n",
        "#     # Select the views that satisfy the condition\n",
        "#     # if not enough number of triplets, sort by the distance and and select the top\n",
        "#     if  len(df_dist[df_dist[\"dist\"] < margin]) >= math.ceil(num_triplets/2):\n",
        "#       df_dist = df_dist[df_dist[\"dist\"] < margin]\n",
        "#       rand_index = random.sample(range(0, len(df_dist)), math.ceil(num_triplets/2))\n",
        "#       rand_index.sort()\n",
        "#       new50_index = df_dist.iloc[rand_index].index\n",
        "#     else:\n",
        "#       new50_index = df_dist.sort_values('dist', ascending=False).head(math.ceil(num_triplets/2))[\"dist\"].index.tolist()\n",
        "#       new50_index.sort()\n",
        "\n",
        "#     for s in range(len(new50_index)):\n",
        "#       Anchor_train_new.append(Anchor_train_all[new50_index[s],:,:,:])\n",
        "#       Positive_train_new.append(Positive_train_all[new50_index[s],:,:,:])\n",
        "#       Negative_train_new.append(Negative_train_all[new50_index[s],:,:,:])\n",
        "#     new_triplets[i] = [new_triplets[i][s] for s in new50_index]\n",
        "\n",
        "    \n",
        "#   remaining_triplets_flat = list(itertools.chain.from_iterable(remaining_triplets))\n",
        "#   new_triplets_flat = list(itertools.chain.from_iterable(new_triplets))\n",
        "  \n",
        "#   cur_triplets_flat = list(itertools.chain.from_iterable(cur_triplets))\n",
        "#   cur_triplets_flat = [l.tolist() for l in cur_triplets_flat]\n",
        "\n",
        "#   print(\"\\nNew 50% length: \", len(Anchor_train_new))\n",
        "\n",
        "#   Anchor_train_all_new = []\n",
        "#   Positive_train_all_new = []\n",
        "#   Negative_train_all_new = []\n",
        "#   new_triplets_all = []\n",
        "#   print(\"len(Anchor_train_retain)\", len(Anchor_train_retain))\n",
        "#   print(\"len(Anchor_train_new)\", len(Anchor_train_new))\n",
        "#   if len(Anchor_train_retain) < len(Anchor_train_new):\n",
        "#     j = 0\n",
        "#     for i in range(len(Anchor_train_retain)):\n",
        "#       Anchor_train_all_new.append(Anchor_train_retain[i])\n",
        "#       Anchor_train_all_new.append(Anchor_train_new[j])\n",
        "#       Anchor_train_all_new.append(Anchor_train_new[j+1])\n",
        "#       Positive_train_all_new.append(Positive_train_retain[i])\n",
        "#       Positive_train_all_new.append(Positive_train_new[j])\n",
        "#       Positive_train_all_new.append(Positive_train_new[j+1])\n",
        "#       Negative_train_all_new.append(Negative_train_retain[i])\n",
        "#       Negative_train_all_new.append(Negative_train_new[j])\n",
        "#       Negative_train_all_new.append(Negative_train_new[j+1])\n",
        "#       new_triplets_all.append(remaining_triplets_flat[j+1])\n",
        "#       new_triplets_all.append(new_triplets_flat[j+1])\n",
        "#       new_triplets_all.append(new_triplets_flat[j+1])\n",
        "#       j += 2\n",
        "#   else:\n",
        "#     for i in range(len(Anchor_train_retain)):\n",
        "#       Anchor_train_all_new.append(Anchor_train_retain[i])\n",
        "#       Anchor_train_all_new.append(Anchor_train_new[i])\n",
        "#       Positive_train_all_new.append(Positive_train_retain[i])\n",
        "#       Positive_train_all_new.append(Positive_train_new[i])\n",
        "#       Negative_train_all_new.append(Negative_train_retain[i])\n",
        "#       Negative_train_all_new.append(Negative_train_new[i])\n",
        "#       new_triplets_all.append(remaining_triplets_flat[j+1])\n",
        "#       new_triplets_all.append(new_triplets_flat[j+1])\n",
        "    \n",
        "#     if i%50 == 0:\n",
        "#       print(\"\\r{0}\".format(i), \"/\", len(Anchor_train_retain),  end=\"\")\n",
        "      \n",
        "#   new_triplets_all = np.array(new_triplets_all).reshape(-1, num_triplets, 3)\n",
        "                              \n",
        "#   print(\"Anchor_train_all_new shape:\", np.array(Anchor_train_all_new).shape)\n",
        "#   return [[np.array(Anchor_train_all_new), np.array(Positive_train_all_new), np.array(Negative_train_all_new)], new_triplets_all]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeiuNgRF0AVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Regenerate training set every 10 epoch as proposed in the paper \n",
        "\n",
        "def regenerate_set(fold_id, train_set, cur_triplets, num_triplets, view_ids=[1,2,3]):\n",
        "  margin = 0.2 # margin for gap criteria\n",
        "#   print(\"Current:\", cur_triplets)\n",
        "  \n",
        "  df_train = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_\" + category + \"/df_train_\" + str(fold_id) + \".csv\").reset_index(drop=\"True\")\n",
        "  \n",
        "  ### Create image dataset for triplet learning \n",
        "  img_size = 128\n",
        "  img_mode = \"L\" #[\"RGB\", \"L\"]\n",
        "  img_path = \"drive/My Drive/Style_data2_views/\" + category + \"_90views_sep/\"\n",
        "  \n",
        "  print(\"=========== Regenerating training set ===========\")\n",
        "  \n",
        "  #########################  Select 50% of view combinations from each triplet to retain  ##################################\n",
        "  Anchor_train_all = train_set[0]\n",
        "  Positive_train_all = train_set[1]\n",
        "  Negative_train_all = train_set[2]\n",
        "  pred = model.predict(x=[Anchor_train_all,Positive_train_all,Negative_train_all],verbose=1)\n",
        "  \n",
        "  total_width = pred.shape[1]\n",
        "  data_length = len(pred)\n",
        "\n",
        "  pred_a = pred[:,int(total_width*1/3):int(total_width*2/3)]\n",
        "  pred_b = pred[:,int(total_width*2/3):int(total_width*3/3)]\n",
        "  \n",
        "  dist_pos_neg = np.sqrt(np.sum(np.power(pred_a - pred_b, 2),axis=1))\n",
        "\n",
        "  df_dist = pd.DataFrame()\n",
        "  df_dist[\"dist\"] = dist_pos_neg\n",
        "  index_retain = []\n",
        "  index_cur = []\n",
        "  for i in range(0, data_length, num_triplets):\n",
        "    df_dist_sub = df_dist.iloc[i:i+num_triplets]\n",
        "    df_dist_sub = df_dist_sub.sort_values(by=\"dist\", ascending=False)\n",
        "    index_retain.append(df_dist_sub.head(math.floor(num_triplets/4)).index.values.tolist())\n",
        "    index_retain.append(df_dist_sub.tail(math.floor(num_triplets/4)).index.values.tolist())\n",
        "    index_cur.append(df_dist_sub.index.values.tolist()) # used later\n",
        "  \n",
        "  cur_triplets_flat = list(itertools.chain.from_iterable(cur_triplets))\n",
        "  cur_triplets_flat = [l.tolist() for l in cur_triplets_flat]\n",
        "\n",
        "  index_retain = list(itertools.chain.from_iterable(index_retain))\n",
        "  index_retain.sort()\n",
        "#   print(\"index_retain: \", index_retain)\n",
        "  print(np.array(index_retain).shape)\n",
        "  print(Anchor_train_all.shape)\n",
        "  Anchor_train_retain = Anchor_train_all[index_retain]\n",
        "  Positive_train_retain = Positive_train_all[index_retain]\n",
        "  Negative_train_retain = Negative_train_all[index_retain]\n",
        "\n",
        "  remaining_triplets = np.array(list(itertools.chain.from_iterable(np.array(cur_triplets_flat)[index_retain]))).reshape(-1, math.floor(num_triplets/4)*2, 3).tolist()\n",
        "\n",
        "  print(\"\\nRemaining 50% length:\", len(Anchor_train_retain))\n",
        "#   print(\"Remaining:\", remaining_triplets)\n",
        "  \n",
        "  \n",
        "  ###################################     Select new 50% of view combinations for each triplet    ############################################\n",
        "  # All combinations of views\n",
        "  view_comb = []\n",
        "  new_triplets = []\n",
        "  Anchor_train_new = []\n",
        "  Positive_train_new = []\n",
        "  Negative_train_new = []\n",
        "  for i in itertools.product(view_ids, repeat=3):\n",
        "    view_comb.append(i)\n",
        "  view_comb = np.array(view_comb)\n",
        "  print(\"view_comb shape: \", view_comb.shape)\n",
        "  for i in range(int(data_length/num_triplets)):\n",
        "    new_triplets_sub = [item for item in view_comb.tolist() if item not in cur_triplets[i].tolist()]\n",
        "    new_triplets.append(new_triplets_sub)\n",
        "\n",
        "  # Compute the distance of all the view combinations that are not in the current triplet\n",
        "  name_list = [\"query\", \"pos\", \"neg\"]\n",
        "  \n",
        "  for i in range(int(data_length/num_triplets)):\n",
        "    print(\"\\r{0}\".format(i), \"/\", int(data_length/num_triplets),  end=\"\")\n",
        "    out_new_triplets = [[], [], []]\n",
        "    new50_index = []\n",
        "    \n",
        "    j1 = 0 \n",
        "    j2 = 0\n",
        "    key_list_j2 = list(range(len(new_triplets[i])))\n",
        "    random.shuffle(key_list_j2)\n",
        "    while j1 < math.ceil(num_triplets/2) and j2 < len(new_triplets[i]):\n",
        "      key_j2 = key_list_j2[j2]\n",
        "      view_out = new_triplets[i][key_j2]\n",
        "      for k in range(0,3):\n",
        "          data_id = df_train.loc[:,name_list[k]][i]\n",
        "          img_file = data_id + \"/\" + data_id + \"_\" + str(view_out[k]) + \".png\"\n",
        "          img = img_path + img_file\n",
        "          img = Image.open(img)\n",
        "          img = ImageOps.expand(img, border=30, fill=0)  # padding with 0s\n",
        "          img = img.convert(img_mode)\n",
        "          img = img.resize((img_size, img_size))\n",
        "          data = np.asarray(img)/255\n",
        "          \n",
        "          # Randomly crop 112x112 images\n",
        "          rand1 = random.randrange(17)\n",
        "          rand2 = random.randrange(17)\n",
        "          data_crop = np.asarray(data[rand1:rand1+112, rand2:rand2+112])\n",
        "\n",
        "          # Flip image by 50% probability\n",
        "          rand_flip = random.randrange(2)\n",
        "          if rand_flip==0:\n",
        "            out_new_triplets[k] = data_crop\n",
        "          else:\n",
        "            out_new_triplets[k] = np.fliplr(data_crop) ## Flipped image\n",
        "     \n",
        "      # Generate triplets for all view combinations\n",
        "      Anchor_train_all = out_new_triplets[0]\n",
        "      Positive_train_all = out_new_triplets[1]\n",
        "      Negative_train_all = out_new_triplets[2]\n",
        "\n",
        "      # add a new dimension\n",
        "      Anchor_train_all = np.array(Anchor_train_all)[np.newaxis, :, :, np.newaxis]\n",
        "      Positive_train_all = np.array(Positive_train_all)[np.newaxis, :, :, np.newaxis]\n",
        "      Negative_train_all = np.array(Negative_train_all)[np.newaxis, :, :, np.newaxis]\n",
        "      pred = model.predict(x=[Anchor_train_all,Positive_train_all,Negative_train_all],verbose=0)\n",
        "\n",
        "      data_length_new_triplets = len(pred)\n",
        "\n",
        "      pred_anchor = pred[:,0:int(total_width*1/3)]\n",
        "      pred_a = pred[:,int(total_width*1/3):int(total_width*2/3)]\n",
        "      pred_b = pred[:,int(total_width*2/3):int(total_width*3/3)]\n",
        "\n",
        "      dist_pos = np.sqrt(np.sum(np.power(pred_anchor-pred_a, 2),axis=1))\n",
        "      dist_neg = np.sqrt(np.sum(np.power(pred_anchor-pred_b, 2),axis=1))\n",
        "      dist_diff = dist_neg - dist_pos\n",
        "     \n",
        "      # Select the views that satisfy the condition\n",
        "      # if not enough number of triplets, sort by the distance and and select the top\n",
        "      if dist_diff <= margin:\n",
        "        new50_index.append(key_j2)\n",
        "        Anchor_train_new.append(Anchor_train_all[0,:,:,:])\n",
        "        Positive_train_new.append(Positive_train_all[0,:,:,:])\n",
        "        Negative_train_new.append(Negative_train_all[0,:,:,:])\n",
        "        j1 += 1\n",
        "      j2 += 1\n",
        "    \n",
        "    # in case there is not enough number of triplets, add random triplets\n",
        "    extra_key_list = list(range(0, len(new_triplets[i])))\n",
        "    random.shuffle(extra_key_list)\n",
        "    j3 = 0\n",
        "    while len(new50_index) < math.ceil(num_triplets/2):\n",
        "      new50_index.append(extra_key_list[j3])\n",
        "      # Generate triplets for all view combinations\n",
        "      Anchor_train_all = out_new_triplets[0]\n",
        "      Positive_train_all = out_new_triplets[1]\n",
        "      Negative_train_all = out_new_triplets[2]\n",
        "\n",
        "      # add a new dimension\n",
        "      Anchor_train_all = np.array(Anchor_train_all)[np.newaxis, :, :, np.newaxis]\n",
        "      Positive_train_all = np.array(Positive_train_all)[np.newaxis, :, :, np.newaxis]\n",
        "      Negative_train_all = np.array(Negative_train_all)[np.newaxis, :, :, np.newaxis]\n",
        "      \n",
        "      Anchor_train_new.append(Anchor_train_all[0,:,:,:])\n",
        "      Positive_train_new.append(Positive_train_all[0,:,:,:])\n",
        "      Negative_train_new.append(Negative_train_all[0,:,:,:])\n",
        "      \n",
        "      j3 += 1\n",
        "    \n",
        "    new50_index.sort()    \n",
        "    \n",
        "    new_triplets[i] = [new_triplets[i][s] for s in new50_index]\n",
        "\n",
        "    \n",
        "  remaining_triplets_flat = list(itertools.chain.from_iterable(remaining_triplets))\n",
        "  new_triplets_flat = list(itertools.chain.from_iterable(new_triplets))\n",
        "  \n",
        "#   cur_triplets_flat = list(itertools.chain.from_iterable(cur_triplets))\n",
        "#   cur_triplets_flat = [l.tolist() for l in cur_triplets_flat]\n",
        "\n",
        "  print(\"\\nNew 50% length: \", len(Anchor_train_new))\n",
        "\n",
        "  Anchor_train_all_new = []\n",
        "  Positive_train_all_new = []\n",
        "  Negative_train_all_new = []\n",
        "  new_triplets_all = []\n",
        "  print(\"len(Anchor_train_retain)\", len(Anchor_train_retain))\n",
        "  print(\"len(Anchor_train_new)\", len(Anchor_train_new))\n",
        "  if len(Anchor_train_retain) < len(Anchor_train_new):\n",
        "    j = 0\n",
        "    for i in range(len(Anchor_train_retain)):\n",
        "      Anchor_train_all_new.append(Anchor_train_retain[i])\n",
        "      Anchor_train_all_new.append(Anchor_train_new[j])\n",
        "      Anchor_train_all_new.append(Anchor_train_new[j+1])\n",
        "      Positive_train_all_new.append(Positive_train_retain[i])\n",
        "      Positive_train_all_new.append(Positive_train_new[j])\n",
        "      Positive_train_all_new.append(Positive_train_new[j+1])\n",
        "      Negative_train_all_new.append(Negative_train_retain[i])\n",
        "      Negative_train_all_new.append(Negative_train_new[j])\n",
        "      Negative_train_all_new.append(Negative_train_new[j+1])\n",
        "      new_triplets_all.append(remaining_triplets_flat[i])\n",
        "      new_triplets_all.append(new_triplets_flat[j])\n",
        "      new_triplets_all.append(new_triplets_flat[j+1])\n",
        "      j += 2\n",
        "  else:\n",
        "    for i in range(len(Anchor_train_retain)):\n",
        "      Anchor_train_all_new.append(Anchor_train_retain[i])\n",
        "      Anchor_train_all_new.append(Anchor_train_new[i])\n",
        "      Positive_train_all_new.append(Positive_train_retain[i])\n",
        "      Positive_train_all_new.append(Positive_train_new[i])\n",
        "      Negative_train_all_new.append(Negative_train_retain[i])\n",
        "      Negative_train_all_new.append(Negative_train_new[i])\n",
        "      new_triplets_all.append(remaining_triplets_flat[i])\n",
        "      new_triplets_all.append(new_triplets_flat[i])\n",
        "    \n",
        "    if i%50 == 0:\n",
        "      print(\"\\r{0}\".format(i), \"/\", len(Anchor_train_retain),  end=\"\")\n",
        "      \n",
        "  new_triplets_all = np.array(new_triplets_all).reshape(-1, num_triplets, 3)\n",
        "                              \n",
        "  print(\"Anchor_train_all_new shape:\", np.array(Anchor_train_all_new).shape)\n",
        "  return [[np.array(Anchor_train_all_new), np.array(Positive_train_all_new), np.array(Negative_train_all_new)], new_triplets_all]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFluQG8PDNqN",
        "colab_type": "text"
      },
      "source": [
        "# 10-fold-cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kySp9S6VQaMU",
        "colab_type": "code",
        "outputId": "711728cb-97d7-4dc6-f4cf-5398a1568ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# %%capture\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "## Multi-view version\n",
        "## Training using all views\n",
        "\n",
        "# Time\n",
        "start = time.time()\n",
        "\n",
        "##### 10-fold cross-validation ###########\n",
        "for k in range(1,11):\n",
        "  \n",
        "  print(\"==================== Fold:\", k, \"/10 ===================\")\n",
        "  ## Generate k th fold\n",
        "  views = list(range(90))\n",
        "#   views = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
        "#   views = [0,2,4,6,8,10,12,14]\n",
        "#   views = [0,4,8,12]\n",
        "  num_triplets = 8   # number of view combinations to generate for each triplet\n",
        "  generated_data, cur_triplets = data_generator2(k,view_ids=views, num_triplets=num_triplets)\n",
        "  Anchor_train_all = generated_data[0]\n",
        "  Positive_train_all = generated_data[1]\n",
        "  Negative_train_all = generated_data[2]\n",
        "  # Use for RGB mode\n",
        "  Anchor_train_all = Anchor_train_all[:, :, :, np.newaxis]\n",
        "  Positive_train_all = Positive_train_all[:, :, :, np.newaxis]\n",
        "  Negative_train_all = Negative_train_all[:, :, :, np.newaxis]\n",
        "\n",
        "  Anchor_test_all = generated_data[3]\n",
        "  Positive_test_all = generated_data[4]\n",
        "  Negative_test_all = generated_data[5]\n",
        "  # Use for RGB mode\n",
        "  Anchor_test_all = Anchor_test_all[:, :, :, np.newaxis]\n",
        "  Positive_test_all = Positive_test_all[:, :, :, np.newaxis]\n",
        "  Negative_test_all = Negative_test_all[:, :, :, np.newaxis]\n",
        "\n",
        "\n",
        "  Y_dummy1 = np.zeros((Anchor_train_all.shape[0],1))\n",
        "  Y_dummy2 = np.zeros((Anchor_test_all.shape[0],1))\n",
        "\n",
        "  train_length =int(Anchor_train_all.shape[0]/3)\n",
        "  test_length =int(Anchor_test_all.shape[0]/3)\n",
        "  \n",
        "  num_epoch = 200\n",
        "  \n",
        "  ## Complie model\n",
        "  model = compile_model(img_channel=1)\n",
        "\n",
        "  df_history = pd.DataFrame(columns=['train_loss','val_loss','train_acc','val_acc'])\n",
        "  for i in range(1,num_epoch+1):\n",
        "    print(\"Epoch:\",i,\"/\",num_epoch)\n",
        "    \n",
        "    # Regenerate training set every 10 epoch\n",
        "    if i%10==0 and i>0:\n",
        "      [Anchor_train_all, Positive_train_all, Negative_train_all], cur_triplets = regenerate_set(fold_id=k,\n",
        "                                                                                                train_set=[Anchor_train_all, Positive_train_all, Negative_train_all],\n",
        "                                                                                                cur_triplets=cur_triplets,\n",
        "                                                                                                num_triplets=num_triplets,\n",
        "                                                                                                view_ids=views)\n",
        "      \n",
        "    \n",
        "    \n",
        "    val_acc = evaluate_triplet(X_test = [Negative_test_all, Positive_test_all, Anchor_test_all], \n",
        "                                Y_test = np.zeros((Anchor_test_all.shape[0],1)))\n",
        "    train_acc = evaluate_triplet(X_test = [Negative_train_all, Positive_train_all, Anchor_train_all], \n",
        "                                Y_test = np.zeros((Anchor_train_all.shape[0],1)))\n",
        "    \n",
        "#     evaluate_triplet_vote(X_test = [Negative_test_all, Positive_test_all, Anchor_test_all], \n",
        "#                                 Y_test = np.zeros((Anchor_test_all.shape[0],1)), num_views=len(views))\n",
        "    \n",
        "    history= model.fit(x=[Anchor_train_all, Positive_train_all, Negative_train_all], y=Y_dummy1,\n",
        "                       validation_data=([Anchor_test_all,Positive_test_all,Negative_test_all],Y_dummy2),\n",
        "                batch_size=64, epochs=1, verbose=1, shuffle=True)\n",
        "\n",
        "    df_history = df_history.append(pd.DataFrame([history.history['loss'], history.history['val_loss'], [train_acc], [val_acc]],\n",
        "                                                index=['train_loss','val_loss','train_acc','val_acc']).T)\n",
        "    df_history = df_history.reset_index(drop=\"Ture\")\n",
        "\n",
        "    # elapsed time\n",
        "    elapsed_time = time.time() - start\n",
        "    print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)), \"\\n\")\n",
        "    \n",
        "    if i%50==0 and i>5:\n",
        "      save_plot(\"Lun_\" + category, fold_id=k, view_id=1)\n",
        "  \n",
        "  gc.collect()\n",
        "  \n",
        "  # Save a list of accuracy\n",
        "  df_history.to_csv(\"drive/My Drive/MultiviewAccuracy_10CV/Lun_\" + category + \"/df_log_\" + str(k) + \".csv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================== Fold: 1 /10 ===================\n",
            "Training original size:  (403, 3)\n",
            "Test original size:  (70, 3)\n",
            "Generating training data...\n",
            "10 / 403"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvA8qh5CVmwj",
        "colab_type": "code",
        "outputId": "d8c5c28a-0e4d-402c-9159-f410f61bfc65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "## Read saved accuracy log\n",
        "pd.read_csv(\"drive/My Drive/MultiviewAccuracy_10CV/Lun_building/df_log_1.csv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_acc</th>\n",
              "      <th>val_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.236780</td>\n",
              "      <td>0.565798</td>\n",
              "      <td>0.479953</td>\n",
              "      <td>0.483333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.246455</td>\n",
              "      <td>0.531563</td>\n",
              "      <td>0.488994</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.242638</td>\n",
              "      <td>0.560275</td>\n",
              "      <td>0.490959</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.240937</td>\n",
              "      <td>0.545839</td>\n",
              "      <td>0.500393</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.239942</td>\n",
              "      <td>0.549002</td>\n",
              "      <td>0.493711</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  train_loss  val_loss  train_acc   val_acc\n",
              "0           0    0.236780  0.565798   0.479953  0.483333\n",
              "1           1    0.246455  0.531563   0.488994  0.466667\n",
              "2           2    0.242638  0.560275   0.490959  0.466667\n",
              "3           3    0.240937  0.545839   0.500393  0.416667\n",
              "4           4    0.239942  0.549002   0.493711  0.450000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0DVbPHgVNNo",
        "colab_type": "code",
        "outputId": "7b710ba0-009b-47f2-e1fd-1e1aedc8dac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        }
      },
      "source": [
        "id = 6\n",
        "data = Anchor_train_all[id][:,:,0]\n",
        "# plt.imshow(data, cmap=\"gray\")\n",
        "plt.imshow(data)\n",
        "plt.show()\n",
        "data = Positive_train_all[id][:,:,0]\n",
        "plt.imshow(data)\n",
        "plt.show()\n",
        "data = Negative_train_all[id][:,:,0]\n",
        "plt.imshow(data)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nq5oPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIfjYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2de3ycVbX3v3smk6S5NUnvtLUttFCu\n5VK5KgcBlYuKBxHleEFBOSigqCio5/14znnVgwoKeEGrqBUVQeQVVASBw0UtFIogFErpBUpbekkL\nbdKmSSYz+/1j7Z1kpjOZ2zOZSZ717aefyTzzXPY8M7PXb6+99lrGWouiKOElUukGKIpSWbQTUJSQ\no52AooQc7QQUJeRoJ6AoIUc7AUUJOWXpBIwxpxljVhpjVhtjrirHNRRFCQYTdJyAMSYKvAi8FdgA\nPAGcZ619PtALKYoSCDVlOOfRwGpr7VoAY8xvgLOArJ1Aramz9TSWoSmKoni6eH2btXZS+vZydALT\ngfVDnm8AjknfyRhzEXARQD0NHGNOKUNTFEXx3G9vX5dpe8Ucg9baRdbahdbahTHqKtUMRQk95egE\nNgIzhzyf4bYpilKFlKMTeAKYZ4yZY4ypBd4P3FWG6yiKEgCB+wSstf3GmEuBe4Eo8FNr7XNBX0dR\nlGAoh2MQa+3dwN3lOLeiKMGiEYOKEnK0E1CUkKOdgKKEHO0EFCXkaCegKCFHOwFFCTnaCShKyClL\nnIBSXZga+Zj3nHYkyZgBwEbltcbbl1aqWUqVoEpAUUKOKoEQYPv7Abj/RzfSa+Mpr73n9mMr0SSl\nilAloCghR5VACIg0atYmJTvaCYSBRCLnLjWzJAWErXcJXlzuycSLa8rWLKU60OGAooQcVQJjiK73\ni5Mv6T7VPZOkj59+80rZTjLrsWfd8yQAs2MdAOxINgBw0/5zytJWpXpQJaAoIUeVwBjiwWu/C0Dc\npvoA3vf792U9xhxxMAAfbREl4KcQ1/V3uj1UCYx1VAkoSshRJTCGiAz06alKYM5tm2VrhmpTV9/x\nUwDi1pS1bUr1okpAUUKOKoEQcPW0hwGImigH/+FTALzwzu8DMDcmqiFJqkrYnGgCoGbqFNkQiwFg\nx0kcgcYPjB1UCShKyFElMIqI7r8fAN3z2gFI1EofHu3x8/9PZjyuzogVP/gXl9K6Ucb+JzwhiuDR\n//4ewF4LixJWzn3cX6R83cSaLgDaa3YBGj8wllAloCghR5XAKGLdOTI+X/rJb6ds/2XnfinP/SzB\nzmQfAMdf91kA6g3018s+9dvFB3DiFZcAcMwVywD46pS/AbCqbyoAV06Q4lFeKXQkZFnyTRo/MGZQ\nJaAoIUeVwCiitz1z7L8fp/u1Ab/smg3Ajd97t+xQn/vcS69ZCMDyr0m6sY+NXwtAPC22oNvnJVPG\nDKoEFCXkqBIYRSTr9o74AzircRsAx3790wAkvOXPoACMO8WOedL/t65KVRef+fIlKc8fuUbiCXpt\n6n7ReftKm1pTE5bYJ54d5h0o1YgqAUUJOUUrAWPMTOAXwBTAAoustdcbY9qBW4HZwMvAudba10tv\naniomTkDgMSUVnkcJ/P80Qm9KfsNzP8vvhSA+jzG/p767fntd9iiywB48uPXAbCxXyz/5mvl2jcf\ntgiAiIs4vHzfN8uBydzZjJTqoBQl0A98zlp7EHAscIkx5iDgKuABa+084AH3XFGUKqVoJWCt3QRs\ncn93GWNWANOBs4CT3G6LgYeAK0tqZch44euTALjrzT8AYFJUxuMn/FXG6zEjHvqj/0estE8LWAiJ\nPI9pfVGufcrnJcLwG1/7IQBvnPoKALNqjGuT+yqpAhh1BOIYNMbMBo4AlgJTXAcBsBkZLmQ65iLg\nIoB6GoJohqIoRVByJ2CMaQJ+B1xure00ZnBdurXWGmMyurSttYuARQAtpj2z2zuk1NSKNfVWFsTy\n93fWAnDM1TILkCxCAXhskQPBK798MQAtH9uQsj2iPuZRS0mfnDEmhnQAv7LW3uE2bzHGTHOvTwO2\nltZERVHKSSmzAwa4CVhhrR0azH4XcD5wtXu8s6QWhpBpbZ0pz3ckJV6/ZaV8XMna0q+RWZ/lT+dP\nZAbjjq/I43nNW0ptklIhShkOnAB8CHjWGPO02/Yl5Md/mzHmQmAdcG5pTVQUpZwYmyHv3EjTYtrt\nMeaUSjejaog+uA8Aqx+bBUD9tvLl/2t8NXstgmJoueWxQM+nBMf99vYnrbUL07erN0dRQo6uHahC\nXrl3NgD18eH3CwK/KNAENL3vcxL2b1YfwWhBlYCihBxVAlWICXaYPiy9LWIH6l8P5qLdC6S6ca0q\ngVGDKgFFCTmqBEJOooCVh/nQ2ypfqQBCGaoCX+m59VlZCJt4bmUlm1MWVAkoSshRJVCFJJ3HPjoC\nC/KKXUMwUuerNHsmyhvafeoEAKY+V8nWlAftBEKODx/2P96RdEpWM5EFBwKwc57ckB+eeRMA115/\ncMXaVC7GWL+tKEqhhFYJvH7+cQBE+8QUVlO4q7fKvlp4qYt98iHeJBer7ax8GHk1ENnyGgANM+XD\nWNm7TyWbU1ZUCShKyAmtEuhrEcvX+k6XBOmWCjbGsefdRwPQM1U8gg0bxUNoRiB8uL8hWCUQbWkB\nINHZmWPP6qTj7ZJSva+3G4DIGHaWqBJQlJATWiXQKzM+3H3QbQD8K0dXsDXC2V/7CwDfve80YHCq\nMDICSiAZcHWx/oOlYKl59J/BnniE2H6EKKLJrVLi7dEdvujrjgq1qHyoElCUkBNaJRCftweAqClf\nwo5Cibt1vdFu16YR7KKDnoHYM03ikUdrHulknfgAtm4X34b5uaSBb6Z6ZpGCQpWAooSc0CmBiKvV\nldg9et76SMYLBEV/nTS694w3AlB39xOBndvUyGcXaW6WaxwhnvzeNtne1+xCfae5NkwUq55odB7+\nOpl9aW4Tz/+kpt0AjK/dQ31Ukrp+pOlvACx+RMqqLfmOFF15+62HB/Y+qgVVAooSckaPOQwImxBr\nENkt4++Tn30fAE2srVibPFFX1DPaIxbMp/wa1yHbIwl5TLqiJOmRhbjHgZRhaVPbyeiQfbwvJO3Y\nRK380e8G84l699wtOe4bL23ob5XGmQaxnNEauVhLs1jXcTHZ3lDXA8DE/yNj69aY+GLGRWXKoz4S\np870D/ydD4mBRsvy3njy1byOKwbbKO8zbsdueTVVAooSckKnBHaecyQwaCWTiye7V8qnBKJtbQD0\nHiljV2fw6ZwlqTe6p4plS3AvAPH5Yk29vas/Tp7XOuva4q1rvcxht9fK63WRVIs6nIX1EXBJJye6\nXUWT2xafDMDpH1wCwPjonkLfbtH0JGMjdq18idbLPUyiEYOKooxRQqcEtrxdrOK4lVLN875vSQW1\nG764AICYs6aeeDKIW+St6fBjV3+tCw55NIBrDjKchX24Yx4AazbLPLgbAnPfj2SVZd1ZUkryvTOf\nynmusciUdln7kKiCIj3lQpWAooSc0CmBiZOkZ+9+QSxfzIgrPelG4MFY/upnXU87ABt3jgeg/tlx\nwODMQt1OsXw+Yu772/8FgEsOfxgIjyKYME78LdUUWRo0qgQUJeSEw+wNYfffRAH46LtIiPrBWKSf\nX6+RCL54Qkx+98YmAJrcBIJNM3iT/yS+k61n9gLwvX+cBMCcfbYBcMrksZeCeyiHjd8IwElXXAZA\ni64dUBRlrFGyEjDGRIFlwEZr7TuMMXOA3wATgCeBD1lr+0q9TlBY944jvZVtx0jiZzzu33LggALw\nNK+V5wPZhp1CSrjY/2ivbGhaJj6DXUfJTMfLmyQhw+Ox2QCcMGE1MHZ9Krd+4xoAzpj1BQCmX72k\nks0JlCCUwKeBFUOefwP4jrV2LhLXeWEA11AUpUyU1G0bY2YAZwJfAz5rjDHAycC/uV0WA/8J3FjK\ndcqBN1gnfv4SAM7+0n3A2LJkXgH87AWZ849EBue64/Go25b52D2TRQk0rZdjGrZIxFynPy4mAQXP\nviJZeJ9ZNx2Aw2bJGProtpcDeQ+VpiEiInZiRCIq214ce2sISlUC1wFfgIGYygnADmutj7jZAEzP\ndKAx5iJjzDJjzLI4IdLmilJlFG32jDHvALZaa580xpxU6PHW2kXAIoAW0z7i4Vh+PjzeKBZvY6/E\n90+OdY10UwLHKwA/EzBUAXiiy5uGPYfJYvCmPCBfmY7T3Q7u1MY5E7wyaIpJx76gZT0wehVWvisb\nRzOlfDInAO8yxpwB1AMtwPVAqzGmxqmBGcDG0pupKEq5KLoTsNZ+EfgigFMCV1hrP2CM+S1wDjJD\ncD5wZwDtLBsf+8xdKc9f6x/eQlYzfh3Ahh0SBZgpyG331kYA3LL/rKTHC3j86suB+IEzMg/lHl0j\n2YaXJCRL78eO/NvwFxwltDwp6z/6c+w3mihHnMCViJNwNeIjuKkM11AUJSACGahZax8CHnJ/r4Uq\nSOIfIl7tbQWGVwCe5hfdR54jFD7ffIY2IXbERDOvt/fbf/L0CQCcduDzAMyoez2/C1QJN+0UlWWb\nRmv+5OxoxKCihJzR6bItgle/cLz84Q2W6/72rZX18s/3ZpzJrErSvf8+CjDXQrd4PEpdwIvhptwr\nqwk3v1Xa5OMH0vGzB/e+cCAATc2SHemDcx+v6pkDv1rytm++DYBxM+R91j5XsSYFjioBRQk51dsF\nB8znLrgdgG8tPgcYjJUfTQxdAwDstQ4gJy82FnzNgczFOQLlJj0iFnP7qanxA9nY1SXpi3/41Ilc\nckT15yj436tvAODr244C4Il7Ay7eWEFG4U9BUZQgGfNKwPsCmiO/BAbnub2h+veHzgdg5sztAJw2\n7fkRbV8+DLcGIB8GYgN2FO4Q6G0VO1G/ffhsu3614aQ/S/xAx2n5h4L7HAVTJu0E4OyZTwPVGWW4\nI+5nB8ZOqHv13eWA2X1IT+qG9N9Bn3zJJ47bNTINKoB8wn/zoWl18R9zoraw/Qc62RxThynHOKfh\n1m2Syuye2MEAnDl1OVAdwwSfXmxV5yS3ZUPlGhMwOhxQlJAz5pWASY96SXtqktLDf+0NEt38/zqP\nGIlmDYsvUf6rlQuB3FN/2Ui691ZTwvKsYh2ofurQp3jPRxF4fMKS7716EgAfWrAUGCymUgl8yvF+\nd0PG0g9HlYCihJyx1KFlpLV1d+qGdKvaIpaqI1n5cFCvAG5dJaXSivUBeBIrpHR3KR+yF1L5ThWm\nM+FhcSq8dnJPjj0zXNtd/OZ/HgPAQW/YBMDx7VIyzvtMYq5R9UY+y4Szba3R7pTtrdHU70JzRNrU\n6BRGg0kQdd+Peie/6oy3k3ID1m2VVO378UrB76daUSWgKCFnzCuBxlpvHTIPrI2ztt5aVAJfCOTv\nG6RgaakKoLtTAnGaipgSzEa+U4Xp1PS497JNpg6ZmP/UmnXrmY/d7yUAptXLFOKn2v9ZUBty4+9T\n9p+DL0gayXdl1ShClYCihJwxrwRa6yVFti/BnZ4so3W8jBO7kmI908eZUTed4Et5N0fkfI0uAWWj\ny10ec+NKP86sJUls4Bg/ZpWL+5435sab/71Fltl6b36xSiASkes1PVtX1PHDUWi8QDqTH5fHrWfk\n3tcrgMNnS2qymQ2y7Lg/WXmbldg8rtJNCJzK31VFUSrKmFcCufBW57i6Pe5RSigkKWzsm5ncY02A\nBY3iad73oA4AHtg2H4CVHZPlLHkO7btelxmO5gJbmQ9BLbga/3dRXDtPyD5b8P5DlwGDXn6lvOhd\nVpSQM+aVwBvceLLHuvjzNKv6hvE7gKAsf2Ek0sIXvb/h3ZOfAiA6RV7vSoj1vOH5twCD8+fpCqHl\nGfEFlGOZdFBOcV/yPNkln4dpSPDeBU+m7JNNAdRERu4z8oVq44g/J27l2tE9Y69EuSoBRQk5Y14J\neK/+aCTh/BUNbgbiy4f+GYDnuiUV2v9bvQCAqIvLH02JUqb+VRq7+dTq/nwe3iPrGGrdbNE73y7r\nGJZ/qWJNCpxR9LVRFKUcjHklMLlWyop1JWR+16ZlhdqvqWOkmzSAH2fmi1cG88dJAYwvHiqPX33q\nTEDKQJWbYtcQZGPq/TVwZH77xoK6aAHEXS37qFOUTTWiytr+PgWA1094bcTbFDSqBBQl5Ix5JeBL\nS2ejpabw1W1BEbfBuNyjNcGPq+u3S9t82rBkjaiQfI2x3z/SH1ysfTJbbbQy4GeLfCRowjlc2mok\nwrTZKYLnXPq6fb65ZMTaFjSqBBQl5IxZJTDuYRmz7RNbB8CKnn1SXo/KlDzvaJGkls/HZbB7UJbi\nGdVMT5fEBwSZie+CL0ih1g+2rAFgfb9YxrkxuVavlVWXdUauuuCHlwHw+L9/W9pi5H6edMVlOa/l\nx/rxdIdNGg3R4VXdSOBXm45zbalgsqPAUCWgKCFnzCmB3edIFpoLp/0GGFw9mM7B//oCAJNcj/5Y\nj8y9HxTbUu4mDtAT1HB5BKba485epEdW+ue9bcU3YiTH+qWSHs04rmP05xdQJaAoIackJWCMaQV+\nAhyC5PG9AFgJ3ArMBl4GzrXWjlgd6v56sSrpCqCjT9bW9R4gzoCjx7+c8vqEaPXVHcgXszt4QecL\ntQ48d5eI21SfiX/+z3OvS9keKcC+3LdRVk2ePP3FYfdrio7cTE76ug5Put+idnd1RzzmQ6lK4Hrg\nHmvtfGABsAK4CnjAWjsPeMA9VxSlSinahBhjxgMnAh8BsNb2AX3GmLOAk9xui4GHgCtLaWQh7Jqe\nuV+LuJ79zPlS1cZ7pO/skmo3B9WPfEWZ7hze8HyJuPSIfmhdyIq/AePqjkm6pETREVxzsf2lNvkj\nR3X4aK4qp2n4GQpfM6CQlaLZojnTFWbd65XLTRkUpSiBOUAH8DNjzFPGmJ8YYxqBKdbaTW6fzcCU\nTAcbYy4yxiwzxiyLj6G6booy2ihlMFmDRH1fZq1daoy5njTpb621Zq8SQAOvLQIWAbSY9sBcrMks\nufBe3SN17uY0SuHR8S4H/XN7ZgBwfMOqkq7rx8DbkjLb0B7JfWuzZUAu+NpxOY8Lc6eQxMnf+ewP\nATihXg76+rZDAZiUlqO/WLYcLx/tM/96PTB4n075/KcG9pmyRNofe/Pw8QLNPrgjTw55+OMA1LjY\njyeO+3Hex2bTDN3uC+azDte9IOs3RnO4QClKYAOwwVq71D2/HekUthhjpgG4x61ZjlcUpQooWglY\nazcbY9YbYw6w1q4ETgGed//PB652j3cG0tI86Wsdfiw3IRaMhUtnl4uge9+VVwDw0DXf3cuTnk5P\niT6BqLNG0R43I+JOFylACbS7Kj1uiQArdk0F4MOt3raVNvNgEqlqJzpMwsRc8QLF1ob43pG/LviY\nRJZ1HZE0v4SNj36fQKlzS5cBvzLG1AJrgY8i6uI2Y8yFwDrg3BKvoShKGSmpE7DWPg0szPDSKaWc\ntxQSLZmt77Y9jQAk2zJbm9aB1YbF3ZKupFiIs798HwA9tp9ojjF/tmjGgvHGqYjTNZpUi//TWfe4\n5zL2LTb3ol9b8M9zrsux5yC54gUKzSfQ1ChTH8fUFa7+stn3Xp9fwN2XxLbtBZ+72tCIQUUJOWNu\n7YCpy2wt9sTlrfr14EHT56z6Ra0Sh5BLBcCQDMglMhAn4D7NQuIFGtKaWYmsy57ta6QmY7Z4gfSq\nwrno6ZP7u3CJzBIsOz7/2YEwoUpAUULOmFMCra2ZrUV3T+b6fL6+XV2JU/ZbEk0APNw9DYCzm1+k\nwQzv/e8rdXYgi9XOFi9Q021JuDe6Z6rIhEplT7j1G9cAkgPh42vfKxt/NEseTwzoIstlvUjrGnef\njs//0HgWFRVPBhPlWU2MuU4gS2wSvXtEGvoCoh5f0KI231pfObj2D+8C4LXT/zdnCe1kQELMR/j6\nd57N3zj7vNX8Yt8/AINBO90VWsY7NJhqd3z4aqe+s3uhVxLDvKl+LbD3Yqa9cDfk/qvzd07my66E\nGJX4qQcBELv/yeF2r2p0OKAoIWfMKYFxscwBnDY+fH9Xam+4sV8WwdR0i2Xtzha/PISgwobTSR9l\n+NnPNzQOruj25bV8efSRwgfh+MU9cZsg4YZk2w/NfD9u+cfR8ocr2X6NC0A65kBRBItn35tybu/c\nLCVD+W6b+aexo1+KvnbFJcH75otFWc68v/hrVRpVAooScsacEmgf151xe6RT3qoPOEmkjYXrczjx\ncjEQPDJfHJNXTXwi674+OefLfZMAaHWhu+ltKpWEEyNPfva7gFhI7/DKZwqzHHgrnRziurn74Fvk\nD1nVzaH3Xip/OMs/8OiJyvOlL84BYP7KT8j2frcQaZt81nXOMbrgt5cD8Py5ch9y+hKG4aTxUrr+\n9x1SMeXIfWQJeuVK2JSOKgFFCTljTglMqe/K/IIzJr7Md0Mk2BwGq3slbUJri1j1mIkOeOBf6Zcl\nsKfeLxZp6v1y27unyOs9E6RxnzhbCo5OqpH3UKoy6G+Q8/oQ3lGD89+Yemexd8n9qumU7eNXy+Ya\nn6l1rwkh2dA5W+5fyxo5LtuioExkW9yVPrvkFz2tv/0QAGaeszzva1QLqgQUJeSMOSVQF8k8O9Cy\n3w5gMFS3uz81eCiSJaV2vvgyVR1bxgNSdCPemGrJJ7jHuMQVDXjx63bIfj/96RnuBXnomivv5bIT\nxfU8JbbTXStNIWQRDAFlLxtxps/eBkBi8eS0VwrLPVP/muy/482ykCiIkGj/OTfH5Jx+Offh0zYC\nMBqXE6kSUJSQM+aUwNS6nRm3v3fOUynPd7pS5UGxpnsiAA2rxSV//BWP8fCNx2TcN96UY6zvXm5e\nIx/Pz1efBsCuw2Q8eunCBwH4wyZJBbZXhKAzeP0twS8G2pEUdXLSbZI85Zn33+AuGdy19lYAxRHb\nLVbapxc747nzALhyP/G9vG2czORk8pl0JzOHmXsObJQ0mmv3yAxPZ3wkCsOXB1UCihJyxpwSyMbE\nmHjcfSRfTzJ1Ga9Pe5UsbNg5QGefUxbu+G9OXcYxpCqBpF/qW2jX65TB+CfEOt3yyNtTX5eMYER7\nizz/MPiYhv3vvhiAZ0+Xufan3y/x+FEXX3HogzJX/7cT5fXmPBKtlhsfMWhWSkKZnqNklubShz4I\nQKxD3tuEZ+RDu/Hr17N/LL8ZmfSEMKOplFo6qgQUJeRUvrsOmD/9z0kAHPYZWcH3ttbnAKg3EkBf\n7wqQbqK1oPP62YMVbvjYkZBlqrvd2PGZdZIJo951qxsTe0cu9qdn8MiT2C6xVD4TWHrSkAEF4GcD\njnZ+kV3Dj2uHw7/fA37/SQAefue17pXUr4yPvnv2pB8BcNXmtwDwrWlLUl4vhK9+VZJ//Md/fLyg\n4zafLNdqeU4sfMMW8VNE98gNe/fMZwB4oV1iOv66Yn857m0uTuNLn+YTX7kdgKk1mX1LnvS1Idu6\nRW20FdTi6kCVgKKEnDGnBFpueQyAl104+iL2BeDDK9cDpUfhXXC9RP35lXnRPrEip/27zD6s3Ec8\n22fe8AXqTFpEW65Lu/1qu5zld0Y0W5P7WlzREacA/nTpN4HBhBhve+SyHBfcG68AFj52AQDLz/Lr\nDjJ/VToSIk8mReX1r0/9KwCXbpBcs9dPf9Adn//swT7RLFGfDp8Y5fUD5XnbIRJXcIBbN9JyoMzh\n+wIhT98vCUxjLoZkfqOUn19St5+cr1du4Oa39vOVR8+Sk7qbfuOJNwN7J4BJTz2+u0eUgSoBRVFG\nHWNOCQSN944fsfTDANQ7K+2tunm3xIj5tFPTGyUy8eXDJzDuPpk77s8SkuDH+F5N1HY6BeAukfiA\nnLvvLzIX7X0DHm+c6k8US+iz9Wxx1rm9rfCkqn/slmv9+qibAEjmiNLzCiCd70x/AIC4O76UVYv+\nffa2is3qPFa8/BPaUsvJ10dTo0W9x/6wU1YC8IMlJwMQ2SUnvPBUUSk/fvzNckCEQdnlhMuju+cC\ncFTDSynn7k3LN9DfP0rDM1EloCihR5WAI32F2bp+ef6hb8ja9vQ8QUc7H0B64smWGnHVj2/pJton\n3vmeial9rffm129PHSef+IXHUvdz5ujWhbLYwCcBiq0SaVEvAoC/HfEraYt7Cxtd0tNsuRWGY0tc\nZk3OaNhS8LGw9xqMQhSAP/Yza98DwO5pLg/idDnXpIOkTbFk6v20znrX18jUTbY5+zcetDbl+eIV\nEsfR2C7KYveOQcn28aPFt3HIuPV5tb2vs/iZmEqjSkBRQk5olEChswKzamT/gTl4d/j+F7wA7K0A\nIi7lb01EXPptDXvoNqm+4jq3qs2vg399vvTBZ71rSeY2uz46Vidj3X5XQCU+TyxXz5TMxUt8ae9x\nNfnnEfBW+OJWsZa9RUZODhQctYWvyvR5BzfdJanHu2fIse3zxTeSSGa2WcVGeS6YLiv//CzCspf2\nJzrD5YPIshrVs1fq8SLbUA2oElCUkBMaJVAoflagv14s29EfER9AtiKiA2XCneVbu24yk6NurLpd\nXvP5Bd74STlXc01PXm05e55EP972/FGp12wRS3/wQxfJdcZJ8MLHDvi7tL2ARQTegpeSf28o3qr3\n2txKwKuQ15NyPxreLmP/uiyWPx2fYbrY+H1/3JHHrBpQBTcuOwmA6064JeMxiTT7GXtt9P6UVAko\nSsgpqfsyxnwG+BgyInoW+CgwDfgNkkjnSeBD1tq+rCepUrxF/MRlvwfgiU7JbOuta41JtXDpEWRN\nK2uxkVQFcObH/+rOXdicsrc6fgIjW7Gknj0yh/G9pyV+v6V5MN/h0Pc0HKXmBfCzLJEijPIvd0p+\nhGxj/2w0xoL5evUnI7ypfQ0A7zxu+OpR/ekzFIG0oDIUrQSMMdOBTwELrbWHAFHg/cA3gO9Ya+cC\nrwMXBtFQRVHKQ6kDmRpgnDEmDjQAm4CTgX9zry8G/hO4scTrjDjeIq7pkbUAF056JON+u61Y3+d7\nZgDwveX/AkBNFE69XMbm3pIXqgDSidWmzhLkotftV0iW3VIZqCtQQHyAP+biVpl52RJvAWBpx2wA\nuuPDl3BvjBWXOXrmOKnIdGjjhoFt5zXLjMEtXVnqozt8LUKf07J+WwjzCVhrNwLXAK8gP/6diPzf\nYa318ysbyFJt3hhzkTFmmfdQY6IAAA/ESURBVDFmWZxg038ripI/RSsBY0wbcBYwB9gB/BY4Ld/j\nrbWLgEUALaa94kOqYsfCjS5Pwa3rxXPfv0lq1U17y6t7eZBL5bB9XgXgH+veMPyOzsN9w+G/AYLN\n/5eLbud3GF9ERaf0dh4/SWIWlnTIStDdfaK60itP10blmrlmB6bWdwJwZNM6ACJp1yukSvS46PDR\niaOJUr6lpwIvWWs7rLVx4A7gBKDVGOM7lxnAxhLbqChKGSnFJ/AKcKwxpgHYA5wCLAMeBM5BZgjO\nB+4stZGVZFWXrKpjfObXfaTgh2dJ3P8HD3kZgDNXvJePtEkkYEdS1IH3CWx3sf1Lu2Q9e00kP0s9\nt1Eq3j2ZFCWQXlC4vVVW1TXXyfBqQqTwtQNBEYQfwiupYya9DAzGYNz/6gHDHufn+k+bINWA0i1+\nPtQbsfQ/33QCAB+Z9veU19MVQMOWiovZoinFJ7AUuB34BzI9GEHk/ZXAZ40xq5FpwpsCaKeiKGWi\npNkBa+1XgK+kbV4LHF3KeUcj57fIODPuIuTa6va2wr4iss9f9942qVzsZxj+0inz5H4OOl0heMt4\n1JxXgCG+AWf5vAIohqBmELpcIH9TERMhiRyz7f79n7qP5Ad4ZIus9X91l8i0f5v5OBBMncn3NMkS\nze++1Dzsfl51JEdvwKCGDefi5R3t8seM4ffzgTg+MGdrd3POabI+ou4YOfbM8U8DUIs8f6Vfrv3c\nHrn4Llc6zQ8L/mFmArDfFPnCpocJj4/4BURj62P2ncGfDpEl1E/1SpLPV+LtgV3DOynfNEWckz95\n9UQALnXJUvyP/48PLQRgv589Gti1RxoNG1aUkDO2TEQJ+AVDEWe9fVqtutjwS0oHy1WJ/D/wDklC\n8shZ1/JakRrRKwQ/bJjaLI83d4iT6ktT7wVg+1yxgKs7JxV1naHEnfooJQ0YQIcr7zYlWng59O5k\nYYuXYu4+JUpssyeTA/Gl3VJGtvdfNgNwLQe7V+R7sR+P7XXMaEOVgKKEnNApAR8Q4sfhv3rsOACe\nvjjz/v0XuOKY/5Hf+X9+5o/cdcofSLIzPnxR1cYCVvF4h2bMKaKdSQmCSrfNXt2s75c0ZKt6pQZa\n1Cmnv++Qac+fzLo772uXim9DEA7BdPoSY/8nokpAUULO2O/mHL+aLx723e9xySV/txSA/Xl82ON6\nW4e3pn56r9tKwYzWSH6JQophS8/w01Wl0O2mCJudejj7yisKOt6vjdrjk6peUbgSKNSLcNduKSe2\nZKeoj1Pbni/4mkOJmr2nKFdvl5Lz0yku8epoQJWAooSc0CgBj1cA+WJz3KGetMKUjWZwNiEor3U6\nK+Pise5LDB+R01DAIp4Gl6nEBw098K0bgMG4h3TSF/ukBxv5V9PLlA1HosB4JR+yva2nqbADlRRU\nCShKyAmdEiiU9MzS6fRY8ab7cNmhvWrQSqDLFTPZ3C9hsgvGywLNRzrmlnzuemfx0y18PolChyMf\nBeDpyiW7HD6s+qxGiZT8dSSo5Kh7x4Ts3toYyLmrGVUCihJyVAnkILZr+Ndf7JE56mPrZQFRLLWe\nZaD4clvNbgZiWu2OjPv5+IRCEozmwp/rlGffB8AP5v8agH0r8A1KX1jVl0uuKcOiSkBRQo4qgRxM\n+a4kBjnv4E8A8D9v+S0A82pl3vjmv74JgJ4TxDdw+YS/AT5iMNg+1qfXmhAVedKVrA/0/EPxln/+\n3Z8E4M63fheAew4RBeALhly1WdYzTK2V1F2fc4k8ilEfO4p8P0FFZvpEIinbXh37PxFVAooScsZ+\nNxcQ+18skYU/Y5bbIo9tf5C01ae3PFP2NvQnUvvsjv7MEYS2BMv4WlI85G977OMAvHDGD4DBsuce\nP4vw9al/Tdn+pqc+DMDDh/8yZb8g8es+PA01wRQfiZoMbR39eURzokpAUUKOKoESqa0Rq/R0j6T6\nOiC2vGzXirsIwdbInsDO6cf2P+uU+PuPjZdMOsuO/7Fcs8AovgcP/wUA1722AIBL2iVbUj55Cor1\ncQTle8nkE2heN3oTiOaLKgFFCTmqBEqk5XQpYPnr808H4H3/t4xKIJ46H74tXvqqQq8AzmhcIdfI\nM2ovF5e3P+vOvz8A57esynlM+jqMbNS73Il+vUJLbXDKKIyoElCUkKNKICASbjg71G/tcwUGhU3K\nuLrOecdf68sc155episTfhbA+wCCUgAePyvgFcAvneL4YMuarMf4dRiF0pMo7rh0fEm5obSurFwB\nl5FClYCihBxVAgERbyz/hHIymdpnRzLNa+fARwJevOZsAG6bd0fpDcuDM5qkYEjCqaNMswVdifxm\nB5qiqdmbJtflWOBRBH7WJLpb8haOXEnXkUeVgKKEHFUCAdEzeeTmk+udAujIklHHV8fJVFrs4vUn\nA3D7vN8DI2fh2iPyVfOlyzNlPQq6lHuhNETikObHMa9sqkxjRhBVAooSclQJBESirvxKID3JT220\n8JV6/zHtzwAkK/TRD5f3sDvPOIHWaKrHvjUWjAc/mqMg6lglpxIwxvzUGLPVGLN8yLZ2Y8x9xphV\n7rHNbTfGmBuMMauNMc8YY44sZ+MVRSmdfIYDPwdOS9t2FfCAtXYe8IB7DnA6MM/9vwi4MZhmVj+2\nxmJrrOQRGKFr9vTH6Onfe458XE2ccTV7x8HHTJQZNXXMqKnb67VyE3H/ykHSmkByCtQPWZ0YwRDB\nkOjcRaIz+NmHaiLnp2KtfQR4LW3zWcBi9/di4N1Dtv/CCo8BrcaYaUE1VlGU4Cm2a55irfVu083A\nFPf3dGD9kP02uG17YYy5yBizzBizLE7wNeRGGhu12OjIjCmjRv732wj9GVbQZbOMt++aStwmAsk5\nWA52JerZlUesQHt0F+3RQes8MbaLibmSQeZBXSYxYZN7O2PGGCXrM2uthcI9KtbaRdbahdbahTFG\nXp4qiiIU6yLeYoyZZq3d5OT+Vrd9IzBzyH4z3LaxT2wEZgesrxI0/H5NMR/lVj0W7DVX5bg1Msom\npDLEWow1ilUCdwHnu7/PB+4csv3DbpbgWGDnkGGDoihVSM5u2RhzC3ASMNEYswH4CnA1cJsx5kJg\nHXCu2/1u4AxgNdANfLQMba5KTKz8VtdEUq1SPEctwnQOrn01yOYEhp81iOdZP6AlrfJzQyQYn1K9\nGXQKJEMUM5CzE7DWnpflpVMy7GuBS0pt1GikrjGYZJfDkuf3cnws9UfiFw3FTD8Roinb0um1qVOL\nO9yS4y63eKnX1SBfE58EwNo+eVzf0y77x8cBsGqHbN/dKwFAdx/54/wanweDS47lfUbyWDpdKJEw\nZBh1aNiwooScUealqV7G1ZVfCZi0KUhffivdEmZbYjwpmiRq5CN/x/SjAm5dPOWxhU73KNSu9wlR\nxIofvPhSABo3GmK7pP0dx8jU5bnHPT7slaZGO91fcs76DMlA8iHq7tuORAMAp1z7eToPlvbPfIMU\nOx3HS0WdezShSkBRQo4qgYBoqR/5gKeBIiPOotU4BfDwn48A4M1b5XH8WrFuF193O8v3zBjhVgof\nmnlCyvM5PLrXPm0uBvW2G48G4APHyT5xm+q/qBsI75Wvb3Rg6XRh4/gbvir+7Nab5TpTWcLUgs4w\nNlAloCghR5VAQPQ777kP5FkVb6MjISPi+bUSKrHb5rdUNhuxmFjA9BG/VwCcsgGAWWzIePzP/jwr\n4/ZqY/9PiE/gCTeT0X+/FHY5ZsLLwGBSFU/U3ZFEWkKQrqTMVHz7njMBmPvZx1Jeb82gRsKIKgFF\nCTmqBAKi8TRJ3X0xb8rwqkRSv/hDGet+9S2/A2C/mERbF5ua3KcW3/CAWMoZWRTAaKfm1FcA+ONn\n5d5e/plUCz4wG+J8B//1xDsBmPuhp+SRVAWgpKJKQFFCjiqBEcSXN//FwBqrmSmv51IK6bEItW9d\nB8AM1pWlvdXGtG8vAWD9ZRJrcEitzAbctP+clP3m8tTINmyUo0pAUUKOKoEqIl0pdL3vvQBseouM\nedv22VmZhlUZMecDSARUkjzs6F1UlJCjSqCKab71Mfcoz73P4J65B1aqSVXB52cfW+kmjClUCShK\nyFElMIrwPoO7mFDhlihjCVUCihJyjK2CRIrGmA5gN7Ct0m3JwkS0bcVQrW2r1nZBeds2y1o7KX1j\nVXQCAMaYZdbahZVuRya0bcVRrW2r1nZBZdqmwwFFCTnaCShKyKmmTmBRpRswDNq24qjWtlVru6AC\nbasan4CiKJWhmpSAoigVQDsBRQk5VdEJGGNOM8asNMasNsZcVcF2zDTGPGiMed4Y85wx5tNue7sx\n5j5jzCr32FbBNkaNMU8ZY/7ons8xxix19+5WY0xpiQyLb1erMeZ2Y8wLxpgVxpjjquW+GWM+4z7P\n5caYW4wx9ZW6b8aYnxpjthpjlg/ZlvE+uZqeN7g2PmOMObIcbap4J2CMiQLfB04HDgLOM8YcVKHm\n9AOfs9YeBBwLXOLachXwgLV2HvCAe14pPg2sGPL8G8B3rLVzgdeBCyvSKrgeuMdaOx9YgLSx4vfN\nGDMd+BSw0Fp7CBAF3k/l7tvPgdPStmW7T6cD89z/i4Aby9Iia21F/wPHAfcOef5F4IuVbpdry53A\nW4GVwDS3bRqwskLtmeG+JCcDf0RK8GwDajLdyxFs13jgJZyjecj2it83YDqwHmhH1sr8EXh7Je8b\nMBtYnus+AT8Czsu0X5D/K64EGPyQPBvctopijJkNHAEsBabYwRLrm4EpFWrWdcAXGMw6PgHYYa3t\nd88rde/mAB3Az9xQ5SfGmEaq4L5ZazcC1wCvAJuAncCTVMd982S7TyPy26iGTqDqMMY0Ab8DLrfW\ndg59zUqXPOLzqsaYdwBbrbVPjvS186AGOBK40Vp7BLIOJEX6V/C+tQFnIR3VPkAje8vxqqES96ka\nOoGNpGbcnOG2VQRjTAzpAH5lrb3Dbd5ijJnmXp8GbK1A004A3mWMeRn4DTIkuB5oNcb4JeGVuncb\ngA3W2qXu+e1Ip1AN9+1U4CVrbYe1Ng7cgdzLarhvnmz3aUR+G9XQCTwBzHPe2lrEaXNXJRpijDHA\nTcAKa+23h7x0F3C++/t8xFcwolhrv2itnWGtnY3co/+11n4AeBA4p8Jt2wysN8Yc4DadAjxPFdw3\nZBhwrDGmwX2+vm0Vv29DyHaf7gI+7GYJjgV2Dhk2BMdIO2qyOErOAF4E1gBfrmA73oRIsWeAp93/\nM5Cx9wPAKuB+oL3C9+sk4I/u732Bx4HVwG+Bugq16XBgmbt3vwfaquW+Af8FvAAsB24G6ip134Bb\nEN9EHFFQF2a7T4jj9/vud/EsMsMReJs0bFhRQk41DAcURakg2gkoSsjRTkBRQo52AooScrQTUJSQ\no52AooQc7QQUJeT8f6hTCxGA6GlgAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_bQQ7wg_X_H",
        "colab_type": "code",
        "outputId": "93556e81-77f5-4b73-a3b9-feedbff3cae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Evaluate on test data\n",
        "# Anchor_test = input_data2_test\n",
        "# Positive_test = input_data1_test\n",
        "# Negative_test = input_data3_test\n",
        "y_true = np.zeros((Anchor_test.shape[0],1))\n",
        "\n",
        "pred = model.predict(x=[Anchor_test,Positive_test,Negative_test],verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "105/105 [==============================] - 1s 7ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DftDxBrkC2KN",
        "colab_type": "code",
        "outputId": "d1379b38-1ece-45b6-c638-fcb1241b3970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Evaluate on training data\n",
        "# Anchor = input_data2_train\n",
        "# Positive = input_data1_train\n",
        "# Negative = input_data3_train\n",
        "y_true = np.zeros((Anchor.shape[0],1))\n",
        "\n",
        "pred = model.predict(x=[Anchor_train,Positive_train,Negative_train],verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "702/702 [==============================] - 19s 28ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA8DSVCQBB3O",
        "colab_type": "code",
        "outputId": "fa5b9fc7-ddf5-46e0-d796-be6e59d31ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Split into anchor, a, and b sets\n",
        "total_lenght = pred.shape[1]\n",
        "pred_anchor = pred[:,0:int(total_lenght*1/3)]\n",
        "pred_a = pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "pred_b = pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "pred_a.shape\n",
        "\n",
        "y_pred = []\n",
        "for i in range(pred.shape[0]):\n",
        "  dist_pos = distance.euclidean(pred_anchor[i], pred_a[i])\n",
        "  dist_neg = distance.euclidean(pred_anchor[i], pred_b[i])\n",
        "  print(\"Triplet\", i, \":\")\n",
        "  print(\"dist_pos\", dist_pos)\n",
        "  print(\"dist_neg\", dist_neg)\n",
        "  print(\"---------------------------------\")\n",
        "  if dist_pos < dist_neg:\n",
        "    y_pred.append(0)\n",
        "  else:\n",
        "    y_pred.append(1)   \n",
        "    \n",
        "\n",
        "print(accuracy_score(y_true, y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c418ec5b27ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_lenght\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred_anchor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNPPpNV_O3vj",
        "colab_type": "text"
      },
      "source": [
        "# Generate empeddings for each object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn5CdIQqPCd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## This model takes one object and outputs a embedding.\n",
        "model=Model(inputs=model.get_layer(\"model_3\").get_input_at(0),outputs=model.get_layer(\"model_3\").get_output_at(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNVxjBt3Tg4f",
        "colab_type": "code",
        "outputId": "7cd09f63-016f-4911-adc1-919d26d974b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pred_cat1 = model.predict(x=Anchor,verbose=1)\n",
        "pred_cat2 = model.predict(x=Positive,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "702/702 [==============================] - 6s 9ms/step\n",
            "702/702 [==============================] - 6s 9ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45OfMZBwTlvP",
        "colab_type": "code",
        "outputId": "ea334171-546a-425f-bd27-b8155999a657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "query = pred_cat1[0]\n",
        "df_dist = pd.DataFrame()\n",
        "for i in range(pred_cat2.shape[0]):\n",
        "  dist = distance.euclidean(query, pred_cat2[i])\n",
        "  df_dist = df_dist.append(pd.DataFrame([dist]))\n",
        "  print(\"Sample\", i, \":\")\n",
        "  print(\"dist\", dist)\n",
        "df_dist = df_dist.reset_index(drop=\"True\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample 0 :\n",
            "dist 11.942307472229004\n",
            "Sample 1 :\n",
            "dist 9.089716911315918\n",
            "Sample 2 :\n",
            "dist 5.0141496658325195\n",
            "Sample 3 :\n",
            "dist 11.207891464233398\n",
            "Sample 4 :\n",
            "dist 11.485407829284668\n",
            "Sample 5 :\n",
            "dist 11.594229698181152\n",
            "Sample 6 :\n",
            "dist 10.215923309326172\n",
            "Sample 7 :\n",
            "dist 8.77670955657959\n",
            "Sample 8 :\n",
            "dist 11.207891464233398\n",
            "Sample 9 :\n",
            "dist 9.089716911315918\n",
            "Sample 10 :\n",
            "dist 9.005356788635254\n",
            "Sample 11 :\n",
            "dist 8.812785148620605\n",
            "Sample 12 :\n",
            "dist 10.139031410217285\n",
            "Sample 13 :\n",
            "dist 11.737712860107422\n",
            "Sample 14 :\n",
            "dist 12.618820190429688\n",
            "Sample 15 :\n",
            "dist 6.905245780944824\n",
            "Sample 16 :\n",
            "dist 10.506524085998535\n",
            "Sample 17 :\n",
            "dist 12.860376358032227\n",
            "Sample 18 :\n",
            "dist 3.7555344104766846\n",
            "Sample 19 :\n",
            "dist 15.70533275604248\n",
            "Sample 20 :\n",
            "dist 9.249247550964355\n",
            "Sample 21 :\n",
            "dist 9.054219245910645\n",
            "Sample 22 :\n",
            "dist 10.1978759765625\n",
            "Sample 23 :\n",
            "dist 11.168105125427246\n",
            "Sample 24 :\n",
            "dist 14.29787540435791\n",
            "Sample 25 :\n",
            "dist 9.054219245910645\n",
            "Sample 26 :\n",
            "dist 5.6047210693359375\n",
            "Sample 27 :\n",
            "dist 8.992192268371582\n",
            "Sample 28 :\n",
            "dist 4.894937038421631\n",
            "Sample 29 :\n",
            "dist 10.372018814086914\n",
            "Sample 30 :\n",
            "dist 9.856998443603516\n",
            "Sample 31 :\n",
            "dist 8.756172180175781\n",
            "Sample 32 :\n",
            "dist 5.826895713806152\n",
            "Sample 33 :\n",
            "dist 12.618820190429688\n",
            "Sample 34 :\n",
            "dist 4.514033794403076\n",
            "Sample 35 :\n",
            "dist 5.0141496658325195\n",
            "Sample 36 :\n",
            "dist 6.086319923400879\n",
            "Sample 37 :\n",
            "dist 10.27345085144043\n",
            "Sample 38 :\n",
            "dist 14.197386741638184\n",
            "Sample 39 :\n",
            "dist 10.816109657287598\n",
            "Sample 40 :\n",
            "dist 11.129412651062012\n",
            "Sample 41 :\n",
            "dist 10.550861358642578\n",
            "Sample 42 :\n",
            "dist 9.572949409484863\n",
            "Sample 43 :\n",
            "dist 9.089716911315918\n",
            "Sample 44 :\n",
            "dist 10.07865047454834\n",
            "Sample 45 :\n",
            "dist 11.207891464233398\n",
            "Sample 46 :\n",
            "dist 9.691022872924805\n",
            "Sample 47 :\n",
            "dist 4.894937038421631\n",
            "Sample 48 :\n",
            "dist 11.485407829284668\n",
            "Sample 49 :\n",
            "dist 9.054219245910645\n",
            "Sample 50 :\n",
            "dist 10.833091735839844\n",
            "Sample 51 :\n",
            "dist 15.017980575561523\n",
            "Sample 52 :\n",
            "dist 8.019840240478516\n",
            "Sample 53 :\n",
            "dist 10.395532608032227\n",
            "Sample 54 :\n",
            "dist 8.920267105102539\n",
            "Sample 55 :\n",
            "dist 10.09423542022705\n",
            "Sample 56 :\n",
            "dist 12.336888313293457\n",
            "Sample 57 :\n",
            "dist 9.864704132080078\n",
            "Sample 58 :\n",
            "dist 9.039032936096191\n",
            "Sample 59 :\n",
            "dist 9.089716911315918\n",
            "Sample 60 :\n",
            "dist 11.129412651062012\n",
            "Sample 61 :\n",
            "dist 9.10306453704834\n",
            "Sample 62 :\n",
            "dist 7.31467342376709\n",
            "Sample 63 :\n",
            "dist 10.186614990234375\n",
            "Sample 64 :\n",
            "dist 6.924028396606445\n",
            "Sample 65 :\n",
            "dist 9.054219245910645\n",
            "Sample 66 :\n",
            "dist 9.864704132080078\n",
            "Sample 67 :\n",
            "dist 8.019840240478516\n",
            "Sample 68 :\n",
            "dist 11.24622917175293\n",
            "Sample 69 :\n",
            "dist 7.15024995803833\n",
            "Sample 70 :\n",
            "dist 11.168105125427246\n",
            "Sample 71 :\n",
            "dist 6.714141368865967\n",
            "Sample 72 :\n",
            "dist 8.992192268371582\n",
            "Sample 73 :\n",
            "dist 6.086319923400879\n",
            "Sample 74 :\n",
            "dist 7.15024995803833\n",
            "Sample 75 :\n",
            "dist 10.27345085144043\n",
            "Sample 76 :\n",
            "dist 9.089716911315918\n",
            "Sample 77 :\n",
            "dist 9.691022872924805\n",
            "Sample 78 :\n",
            "dist 9.004803657531738\n",
            "Sample 79 :\n",
            "dist 8.019840240478516\n",
            "Sample 80 :\n",
            "dist 9.550772666931152\n",
            "Sample 81 :\n",
            "dist 11.94116497039795\n",
            "Sample 82 :\n",
            "dist 8.920267105102539\n",
            "Sample 83 :\n",
            "dist 10.75291919708252\n",
            "Sample 84 :\n",
            "dist 10.310395240783691\n",
            "Sample 85 :\n",
            "dist 11.129412651062012\n",
            "Sample 86 :\n",
            "dist 10.206989288330078\n",
            "Sample 87 :\n",
            "dist 6.924028396606445\n",
            "Sample 88 :\n",
            "dist 14.807355880737305\n",
            "Sample 89 :\n",
            "dist 13.467585563659668\n",
            "Sample 90 :\n",
            "dist 8.460783004760742\n",
            "Sample 91 :\n",
            "dist 11.485407829284668\n",
            "Sample 92 :\n",
            "dist 9.691022872924805\n",
            "Sample 93 :\n",
            "dist 10.00361156463623\n",
            "Sample 94 :\n",
            "dist 6.229179382324219\n",
            "Sample 95 :\n",
            "dist 9.213743209838867\n",
            "Sample 96 :\n",
            "dist 10.287613868713379\n",
            "Sample 97 :\n",
            "dist 6.714141368865967\n",
            "Sample 98 :\n",
            "dist 8.976008415222168\n",
            "Sample 99 :\n",
            "dist 9.089716911315918\n",
            "Sample 100 :\n",
            "dist 10.550861358642578\n",
            "Sample 101 :\n",
            "dist 11.594229698181152\n",
            "Sample 102 :\n",
            "dist 8.16422176361084\n",
            "Sample 103 :\n",
            "dist 3.7555344104766846\n",
            "Sample 104 :\n",
            "dist 6.714141368865967\n",
            "Sample 105 :\n",
            "dist 8.992192268371582\n",
            "Sample 106 :\n",
            "dist 9.691022872924805\n",
            "Sample 107 :\n",
            "dist 8.812785148620605\n",
            "Sample 108 :\n",
            "dist 11.737712860107422\n",
            "Sample 109 :\n",
            "dist 8.812785148620605\n",
            "Sample 110 :\n",
            "dist 9.214948654174805\n",
            "Sample 111 :\n",
            "dist 10.423500061035156\n",
            "Sample 112 :\n",
            "dist 9.100789070129395\n",
            "Sample 113 :\n",
            "dist 9.659614562988281\n",
            "Sample 114 :\n",
            "dist 9.213743209838867\n",
            "Sample 115 :\n",
            "dist 10.206989288330078\n",
            "Sample 116 :\n",
            "dist 8.64344596862793\n",
            "Sample 117 :\n",
            "dist 9.550772666931152\n",
            "Sample 118 :\n",
            "dist 10.473353385925293\n",
            "Sample 119 :\n",
            "dist 10.053415298461914\n",
            "Sample 120 :\n",
            "dist 12.336888313293457\n",
            "Sample 121 :\n",
            "dist 9.550772666931152\n",
            "Sample 122 :\n",
            "dist 10.07865047454834\n",
            "Sample 123 :\n",
            "dist 10.814924240112305\n",
            "Sample 124 :\n",
            "dist 6.08117151260376\n",
            "Sample 125 :\n",
            "dist 15.017980575561523\n",
            "Sample 126 :\n",
            "dist 9.856998443603516\n",
            "Sample 127 :\n",
            "dist 6.714141368865967\n",
            "Sample 128 :\n",
            "dist 9.550772666931152\n",
            "Sample 129 :\n",
            "dist 5.922364711761475\n",
            "Sample 130 :\n",
            "dist 9.089716911315918\n",
            "Sample 131 :\n",
            "dist 7.31467342376709\n",
            "Sample 132 :\n",
            "dist 14.941376686096191\n",
            "Sample 133 :\n",
            "dist 8.64344596862793\n",
            "Sample 134 :\n",
            "dist 11.129412651062012\n",
            "Sample 135 :\n",
            "dist 8.992192268371582\n",
            "Sample 136 :\n",
            "dist 11.67253303527832\n",
            "Sample 137 :\n",
            "dist 10.053415298461914\n",
            "Sample 138 :\n",
            "dist 9.249247550964355\n",
            "Sample 139 :\n",
            "dist 11.594229698181152\n",
            "Sample 140 :\n",
            "dist 10.053415298461914\n",
            "Sample 141 :\n",
            "dist 9.054219245910645\n",
            "Sample 142 :\n",
            "dist 6.924028396606445\n",
            "Sample 143 :\n",
            "dist 9.659614562988281\n",
            "Sample 144 :\n",
            "dist 10.27345085144043\n",
            "Sample 145 :\n",
            "dist 11.91559886932373\n",
            "Sample 146 :\n",
            "dist 9.039032936096191\n",
            "Sample 147 :\n",
            "dist 13.467585563659668\n",
            "Sample 148 :\n",
            "dist 10.00361156463623\n",
            "Sample 149 :\n",
            "dist 10.00361156463623\n",
            "Sample 150 :\n",
            "dist 10.00361156463623\n",
            "Sample 151 :\n",
            "dist 5.0141496658325195\n",
            "Sample 152 :\n",
            "dist 9.10306453704834\n",
            "Sample 153 :\n",
            "dist 12.618820190429688\n",
            "Sample 154 :\n",
            "dist 14.807355880737305\n",
            "Sample 155 :\n",
            "dist 8.976008415222168\n",
            "Sample 156 :\n",
            "dist 9.550772666931152\n",
            "Sample 157 :\n",
            "dist 6.714141368865967\n",
            "Sample 158 :\n",
            "dist 11.91559886932373\n",
            "Sample 159 :\n",
            "dist 12.167671203613281\n",
            "Sample 160 :\n",
            "dist 9.214948654174805\n",
            "Sample 161 :\n",
            "dist 13.467585563659668\n",
            "Sample 162 :\n",
            "dist 14.807355880737305\n",
            "Sample 163 :\n",
            "dist 10.09423542022705\n",
            "Sample 164 :\n",
            "dist 11.162457466125488\n",
            "Sample 165 :\n",
            "dist 12.167671203613281\n",
            "Sample 166 :\n",
            "dist 9.249247550964355\n",
            "Sample 167 :\n",
            "dist 10.1978759765625\n",
            "Sample 168 :\n",
            "dist 9.864704132080078\n",
            "Sample 169 :\n",
            "dist 5.922364711761475\n",
            "Sample 170 :\n",
            "dist 9.089716911315918\n",
            "Sample 171 :\n",
            "dist 6.714141368865967\n",
            "Sample 172 :\n",
            "dist 8.460783004760742\n",
            "Sample 173 :\n",
            "dist 13.467585563659668\n",
            "Sample 174 :\n",
            "dist 11.129412651062012\n",
            "Sample 175 :\n",
            "dist 9.214948654174805\n",
            "Sample 176 :\n",
            "dist 9.10306453704834\n",
            "Sample 177 :\n",
            "dist 11.168105125427246\n",
            "Sample 178 :\n",
            "dist 14.605659484863281\n",
            "Sample 179 :\n",
            "dist 8.812785148620605\n",
            "Sample 180 :\n",
            "dist 5.922364711761475\n",
            "Sample 181 :\n",
            "dist 11.594229698181152\n",
            "Sample 182 :\n",
            "dist 9.739049911499023\n",
            "Sample 183 :\n",
            "dist 9.005356788635254\n",
            "Sample 184 :\n",
            "dist 8.903407096862793\n",
            "Sample 185 :\n",
            "dist 10.287613868713379\n",
            "Sample 186 :\n",
            "dist 11.347253799438477\n",
            "Sample 187 :\n",
            "dist 14.197386741638184\n",
            "Sample 188 :\n",
            "dist 7.5439677238464355\n",
            "Sample 189 :\n",
            "dist 10.585667610168457\n",
            "Sample 190 :\n",
            "dist 6.924028396606445\n",
            "Sample 191 :\n",
            "dist 11.099358558654785\n",
            "Sample 192 :\n",
            "dist 10.423500061035156\n",
            "Sample 193 :\n",
            "dist 8.992192268371582\n",
            "Sample 194 :\n",
            "dist 10.186614990234375\n",
            "Sample 195 :\n",
            "dist 9.10306453704834\n",
            "Sample 196 :\n",
            "dist 12.372003555297852\n",
            "Sample 197 :\n",
            "dist 12.618820190429688\n",
            "Sample 198 :\n",
            "dist 13.546152114868164\n",
            "Sample 199 :\n",
            "dist 13.467585563659668\n",
            "Sample 200 :\n",
            "dist 7.31467342376709\n",
            "Sample 201 :\n",
            "dist 12.649848937988281\n",
            "Sample 202 :\n",
            "dist 6.714141368865967\n",
            "Sample 203 :\n",
            "dist 9.039032936096191\n",
            "Sample 204 :\n",
            "dist 4.514033794403076\n",
            "Sample 205 :\n",
            "dist 10.00361156463623\n",
            "Sample 206 :\n",
            "dist 9.550772666931152\n",
            "Sample 207 :\n",
            "dist 9.039032936096191\n",
            "Sample 208 :\n",
            "dist 9.572949409484863\n",
            "Sample 209 :\n",
            "dist 8.920267105102539\n",
            "Sample 210 :\n",
            "dist 11.207891464233398\n",
            "Sample 211 :\n",
            "dist 10.565771102905273\n",
            "Sample 212 :\n",
            "dist 8.756172180175781\n",
            "Sample 213 :\n",
            "dist 7.356421947479248\n",
            "Sample 214 :\n",
            "dist 10.1978759765625\n",
            "Sample 215 :\n",
            "dist 10.813817024230957\n",
            "Sample 216 :\n",
            "dist 7.31467342376709\n",
            "Sample 217 :\n",
            "dist 9.10306453704834\n",
            "Sample 218 :\n",
            "dist 15.70533275604248\n",
            "Sample 219 :\n",
            "dist 10.27345085144043\n",
            "Sample 220 :\n",
            "dist 10.852871894836426\n",
            "Sample 221 :\n",
            "dist 7.826977729797363\n",
            "Sample 222 :\n",
            "dist 9.822888374328613\n",
            "Sample 223 :\n",
            "dist 10.287613868713379\n",
            "Sample 224 :\n",
            "dist 6.08117151260376\n",
            "Sample 225 :\n",
            "dist 9.249247550964355\n",
            "Sample 226 :\n",
            "dist 14.048661231994629\n",
            "Sample 227 :\n",
            "dist 13.467585563659668\n",
            "Sample 228 :\n",
            "dist 11.099358558654785\n",
            "Sample 229 :\n",
            "dist 5.6047210693359375\n",
            "Sample 230 :\n",
            "dist 7.45473051071167\n",
            "Sample 231 :\n",
            "dist 8.16422176361084\n",
            "Sample 232 :\n",
            "dist 11.594229698181152\n",
            "Sample 233 :\n",
            "dist 8.047513008117676\n",
            "Sample 234 :\n",
            "dist 4.514033794403076\n",
            "Sample 235 :\n",
            "dist 10.395532608032227\n",
            "Sample 236 :\n",
            "dist 9.572949409484863\n",
            "Sample 237 :\n",
            "dist 10.07865047454834\n",
            "Sample 238 :\n",
            "dist 10.758524894714355\n",
            "Sample 239 :\n",
            "dist 11.129412651062012\n",
            "Sample 240 :\n",
            "dist 10.372018814086914\n",
            "Sample 241 :\n",
            "dist 11.942307472229004\n",
            "Sample 242 :\n",
            "dist 10.215923309326172\n",
            "Sample 243 :\n",
            "dist 9.856998443603516\n",
            "Sample 244 :\n",
            "dist 6.714141368865967\n",
            "Sample 245 :\n",
            "dist 10.395532608032227\n",
            "Sample 246 :\n",
            "dist 12.618820190429688\n",
            "Sample 247 :\n",
            "dist 8.728707313537598\n",
            "Sample 248 :\n",
            "dist 9.856998443603516\n",
            "Sample 249 :\n",
            "dist 9.328691482543945\n",
            "Sample 250 :\n",
            "dist 14.197386741638184\n",
            "Sample 251 :\n",
            "dist 4.514033794403076\n",
            "Sample 252 :\n",
            "dist 6.086319923400879\n",
            "Sample 253 :\n",
            "dist 10.564896583557129\n",
            "Sample 254 :\n",
            "dist 11.594229698181152\n",
            "Sample 255 :\n",
            "dist 6.086319923400879\n",
            "Sample 256 :\n",
            "dist 8.976008415222168\n",
            "Sample 257 :\n",
            "dist 7.356421947479248\n",
            "Sample 258 :\n",
            "dist 10.473353385925293\n",
            "Sample 259 :\n",
            "dist 12.167671203613281\n",
            "Sample 260 :\n",
            "dist 12.618820190429688\n",
            "Sample 261 :\n",
            "dist 6.086319923400879\n",
            "Sample 262 :\n",
            "dist 6.924028396606445\n",
            "Sample 263 :\n",
            "dist 8.019840240478516\n",
            "Sample 264 :\n",
            "dist 9.691022872924805\n",
            "Sample 265 :\n",
            "dist 6.905245780944824\n",
            "Sample 266 :\n",
            "dist 12.336888313293457\n",
            "Sample 267 :\n",
            "dist 10.437501907348633\n",
            "Sample 268 :\n",
            "dist 10.58375072479248\n",
            "Sample 269 :\n",
            "dist 3.7555344104766846\n",
            "Sample 270 :\n",
            "dist 8.992192268371582\n",
            "Sample 271 :\n",
            "dist 9.039032936096191\n",
            "Sample 272 :\n",
            "dist 6.722531318664551\n",
            "Sample 273 :\n",
            "dist 11.594229698181152\n",
            "Sample 274 :\n",
            "dist 9.10306453704834\n",
            "Sample 275 :\n",
            "dist 10.186614990234375\n",
            "Sample 276 :\n",
            "dist 9.089716911315918\n",
            "Sample 277 :\n",
            "dist 12.860376358032227\n",
            "Sample 278 :\n",
            "dist 9.005356788635254\n",
            "Sample 279 :\n",
            "dist 9.659614562988281\n",
            "Sample 280 :\n",
            "dist 7.356421947479248\n",
            "Sample 281 :\n",
            "dist 9.739049911499023\n",
            "Sample 282 :\n",
            "dist 9.856998443603516\n",
            "Sample 283 :\n",
            "dist 9.864704132080078\n",
            "Sample 284 :\n",
            "dist 7.356421947479248\n",
            "Sample 285 :\n",
            "dist 8.865540504455566\n",
            "Sample 286 :\n",
            "dist 10.27345085144043\n",
            "Sample 287 :\n",
            "dist 10.09423542022705\n",
            "Sample 288 :\n",
            "dist 9.039032936096191\n",
            "Sample 289 :\n",
            "dist 8.019840240478516\n",
            "Sample 290 :\n",
            "dist 11.594229698181152\n",
            "Sample 291 :\n",
            "dist 10.07865047454834\n",
            "Sample 292 :\n",
            "dist 10.423500061035156\n",
            "Sample 293 :\n",
            "dist 9.864704132080078\n",
            "Sample 294 :\n",
            "dist 8.019840240478516\n",
            "Sample 295 :\n",
            "dist 6.905245780944824\n",
            "Sample 296 :\n",
            "dist 9.089716911315918\n",
            "Sample 297 :\n",
            "dist 5.791200160980225\n",
            "Sample 298 :\n",
            "dist 10.565771102905273\n",
            "Sample 299 :\n",
            "dist 9.214948654174805\n",
            "Sample 300 :\n",
            "dist 8.756172180175781\n",
            "Sample 301 :\n",
            "dist 6.714141368865967\n",
            "Sample 302 :\n",
            "dist 11.162457466125488\n",
            "Sample 303 :\n",
            "dist 9.864704132080078\n",
            "Sample 304 :\n",
            "dist 10.437501907348633\n",
            "Sample 305 :\n",
            "dist 10.506524085998535\n",
            "Sample 306 :\n",
            "dist 12.167671203613281\n",
            "Sample 307 :\n",
            "dist 10.473353385925293\n",
            "Sample 308 :\n",
            "dist 8.812785148620605\n",
            "Sample 309 :\n",
            "dist 9.214948654174805\n",
            "Sample 310 :\n",
            "dist 13.546152114868164\n",
            "Sample 311 :\n",
            "dist 10.565771102905273\n",
            "Sample 312 :\n",
            "dist 9.213743209838867\n",
            "Sample 313 :\n",
            "dist 9.659614562988281\n",
            "Sample 314 :\n",
            "dist 5.826895713806152\n",
            "Sample 315 :\n",
            "dist 7.444174289703369\n",
            "Sample 316 :\n",
            "dist 9.10306453704834\n",
            "Sample 317 :\n",
            "dist 9.214948654174805\n",
            "Sample 318 :\n",
            "dist 11.099358558654785\n",
            "Sample 319 :\n",
            "dist 14.359901428222656\n",
            "Sample 320 :\n",
            "dist 11.207891464233398\n",
            "Sample 321 :\n",
            "dist 9.739049911499023\n",
            "Sample 322 :\n",
            "dist 10.816109657287598\n",
            "Sample 323 :\n",
            "dist 8.812785148620605\n",
            "Sample 324 :\n",
            "dist 9.054219245910645\n",
            "Sample 325 :\n",
            "dist 12.860376358032227\n",
            "Sample 326 :\n",
            "dist 8.16422176361084\n",
            "Sample 327 :\n",
            "dist 10.11634349822998\n",
            "Sample 328 :\n",
            "dist 7.45473051071167\n",
            "Sample 329 :\n",
            "dist 10.053415298461914\n",
            "Sample 330 :\n",
            "dist 10.27345085144043\n",
            "Sample 331 :\n",
            "dist 9.659614562988281\n",
            "Sample 332 :\n",
            "dist 8.16422176361084\n",
            "Sample 333 :\n",
            "dist 10.372018814086914\n",
            "Sample 334 :\n",
            "dist 10.814924240112305\n",
            "Sample 335 :\n",
            "dist 10.11634349822998\n",
            "Sample 336 :\n",
            "dist 12.860376358032227\n",
            "Sample 337 :\n",
            "dist 10.565771102905273\n",
            "Sample 338 :\n",
            "dist 10.585667610168457\n",
            "Sample 339 :\n",
            "dist 9.822664260864258\n",
            "Sample 340 :\n",
            "dist 10.09423542022705\n",
            "Sample 341 :\n",
            "dist 8.812785148620605\n",
            "Sample 342 :\n",
            "dist 15.017980575561523\n",
            "Sample 343 :\n",
            "dist 10.833091735839844\n",
            "Sample 344 :\n",
            "dist 11.207891464233398\n",
            "Sample 345 :\n",
            "dist 9.864704132080078\n",
            "Sample 346 :\n",
            "dist 7.801884174346924\n",
            "Sample 347 :\n",
            "dist 12.372003555297852\n",
            "Sample 348 :\n",
            "dist 9.054219245910645\n",
            "Sample 349 :\n",
            "dist 8.395001411437988\n",
            "Sample 350 :\n",
            "dist 10.206989288330078\n",
            "Sample 351 :\n",
            "dist 12.123115539550781\n",
            "Sample 352 :\n",
            "dist 10.816109657287598\n",
            "Sample 353 :\n",
            "dist 7.444174289703369\n",
            "Sample 354 :\n",
            "dist 11.207891464233398\n",
            "Sample 355 :\n",
            "dist 7.356421947479248\n",
            "Sample 356 :\n",
            "dist 12.860376358032227\n",
            "Sample 357 :\n",
            "dist 9.822888374328613\n",
            "Sample 358 :\n",
            "dist 10.852871894836426\n",
            "Sample 359 :\n",
            "dist 10.310395240783691\n",
            "Sample 360 :\n",
            "dist 11.942307472229004\n",
            "Sample 361 :\n",
            "dist 5.0141496658325195\n",
            "Sample 362 :\n",
            "dist 12.336888313293457\n",
            "Sample 363 :\n",
            "dist 10.437501907348633\n",
            "Sample 364 :\n",
            "dist 10.550861358642578\n",
            "Sample 365 :\n",
            "dist 10.053415298461914\n",
            "Sample 366 :\n",
            "dist 8.16422176361084\n",
            "Sample 367 :\n",
            "dist 12.366839408874512\n",
            "Sample 368 :\n",
            "dist 5.826895713806152\n",
            "Sample 369 :\n",
            "dist 8.920267105102539\n",
            "Sample 370 :\n",
            "dist 11.485407829284668\n",
            "Sample 371 :\n",
            "dist 7.45473051071167\n",
            "Sample 372 :\n",
            "dist 8.812785148620605\n",
            "Sample 373 :\n",
            "dist 12.372003555297852\n",
            "Sample 374 :\n",
            "dist 8.812785148620605\n",
            "Sample 375 :\n",
            "dist 7.15024995803833\n",
            "Sample 376 :\n",
            "dist 9.213743209838867\n",
            "Sample 377 :\n",
            "dist 10.506524085998535\n",
            "Sample 378 :\n",
            "dist 11.942307472229004\n",
            "Sample 379 :\n",
            "dist 9.054219245910645\n",
            "Sample 380 :\n",
            "dist 5.922364711761475\n",
            "Sample 381 :\n",
            "dist 13.467585563659668\n",
            "Sample 382 :\n",
            "dist 13.467585563659668\n",
            "Sample 383 :\n",
            "dist 4.894937038421631\n",
            "Sample 384 :\n",
            "dist 11.129412651062012\n",
            "Sample 385 :\n",
            "dist 10.852871894836426\n",
            "Sample 386 :\n",
            "dist 8.812785148620605\n",
            "Sample 387 :\n",
            "dist 6.647155284881592\n",
            "Sample 388 :\n",
            "dist 11.207891464233398\n",
            "Sample 389 :\n",
            "dist 11.099358558654785\n",
            "Sample 390 :\n",
            "dist 10.506524085998535\n",
            "Sample 391 :\n",
            "dist 8.64344596862793\n",
            "Sample 392 :\n",
            "dist 9.856998443603516\n",
            "Sample 393 :\n",
            "dist 5.791200160980225\n",
            "Sample 394 :\n",
            "dist 8.047513008117676\n",
            "Sample 395 :\n",
            "dist 11.594229698181152\n",
            "Sample 396 :\n",
            "dist 8.992192268371582\n",
            "Sample 397 :\n",
            "dist 9.100789070129395\n",
            "Sample 398 :\n",
            "dist 9.214948654174805\n",
            "Sample 399 :\n",
            "dist 7.801884174346924\n",
            "Sample 400 :\n",
            "dist 9.100789070129395\n",
            "Sample 401 :\n",
            "dist 7.15024995803833\n",
            "Sample 402 :\n",
            "dist 5.0141496658325195\n",
            "Sample 403 :\n",
            "dist 8.756172180175781\n",
            "Sample 404 :\n",
            "dist 10.852871894836426\n",
            "Sample 405 :\n",
            "dist 14.848955154418945\n",
            "Sample 406 :\n",
            "dist 5.060171127319336\n",
            "Sample 407 :\n",
            "dist 5.0141496658325195\n",
            "Sample 408 :\n",
            "dist 9.856998443603516\n",
            "Sample 409 :\n",
            "dist 9.039032936096191\n",
            "Sample 410 :\n",
            "dist 10.09423542022705\n",
            "Sample 411 :\n",
            "dist 10.550861358642578\n",
            "Sample 412 :\n",
            "dist 10.813817024230957\n",
            "Sample 413 :\n",
            "dist 12.366839408874512\n",
            "Sample 414 :\n",
            "dist 15.017980575561523\n",
            "Sample 415 :\n",
            "dist 10.186614990234375\n",
            "Sample 416 :\n",
            "dist 9.039032936096191\n",
            "Sample 417 :\n",
            "dist 10.00361156463623\n",
            "Sample 418 :\n",
            "dist 11.67253303527832\n",
            "Sample 419 :\n",
            "dist 10.547954559326172\n",
            "Sample 420 :\n",
            "dist 13.343537330627441\n",
            "Sample 421 :\n",
            "dist 9.100789070129395\n",
            "Sample 422 :\n",
            "dist 13.467585563659668\n",
            "Sample 423 :\n",
            "dist 9.039032936096191\n",
            "Sample 424 :\n",
            "dist 9.550772666931152\n",
            "Sample 425 :\n",
            "dist 10.585667610168457\n",
            "Sample 426 :\n",
            "dist 10.206989288330078\n",
            "Sample 427 :\n",
            "dist 11.162457466125488\n",
            "Sample 428 :\n",
            "dist 7.356421947479248\n",
            "Sample 429 :\n",
            "dist 9.856998443603516\n",
            "Sample 430 :\n",
            "dist 7.45473051071167\n",
            "Sample 431 :\n",
            "dist 10.75291919708252\n",
            "Sample 432 :\n",
            "dist 6.714141368865967\n",
            "Sample 433 :\n",
            "dist 5.060171127319336\n",
            "Sample 434 :\n",
            "dist 10.186614990234375\n",
            "Sample 435 :\n",
            "dist 9.054219245910645\n",
            "Sample 436 :\n",
            "dist 7.15024995803833\n",
            "Sample 437 :\n",
            "dist 10.215923309326172\n",
            "Sample 438 :\n",
            "dist 12.336888313293457\n",
            "Sample 439 :\n",
            "dist 10.06309700012207\n",
            "Sample 440 :\n",
            "dist 11.084732055664062\n",
            "Sample 441 :\n",
            "dist 7.801884174346924\n",
            "Sample 442 :\n",
            "dist 7.5439677238464355\n",
            "Sample 443 :\n",
            "dist 10.00361156463623\n",
            "Sample 444 :\n",
            "dist 7.801884174346924\n",
            "Sample 445 :\n",
            "dist 10.415663719177246\n",
            "Sample 446 :\n",
            "dist 14.197386741638184\n",
            "Sample 447 :\n",
            "dist 10.565771102905273\n",
            "Sample 448 :\n",
            "dist 11.91559886932373\n",
            "Sample 449 :\n",
            "dist 9.089716911315918\n",
            "Sample 450 :\n",
            "dist 11.942307472229004\n",
            "Sample 451 :\n",
            "dist 11.737712860107422\n",
            "Sample 452 :\n",
            "dist 5.922364711761475\n",
            "Sample 453 :\n",
            "dist 14.197386741638184\n",
            "Sample 454 :\n",
            "dist 11.91559886932373\n",
            "Sample 455 :\n",
            "dist 7.45473051071167\n",
            "Sample 456 :\n",
            "dist 10.1978759765625\n",
            "Sample 457 :\n",
            "dist 8.992192268371582\n",
            "Sample 458 :\n",
            "dist 9.864704132080078\n",
            "Sample 459 :\n",
            "dist 9.004803657531738\n",
            "Sample 460 :\n",
            "dist 11.129412651062012\n",
            "Sample 461 :\n",
            "dist 9.856998443603516\n",
            "Sample 462 :\n",
            "dist 10.287613868713379\n",
            "Sample 463 :\n",
            "dist 9.739049911499023\n",
            "Sample 464 :\n",
            "dist 6.924028396606445\n",
            "Sample 465 :\n",
            "dist 7.356421947479248\n",
            "Sample 466 :\n",
            "dist 11.099358558654785\n",
            "Sample 467 :\n",
            "dist 9.004803657531738\n",
            "Sample 468 :\n",
            "dist 7.356421947479248\n",
            "Sample 469 :\n",
            "dist 9.739049911499023\n",
            "Sample 470 :\n",
            "dist 9.10306453704834\n",
            "Sample 471 :\n",
            "dist 11.594229698181152\n",
            "Sample 472 :\n",
            "dist 5.0141496658325195\n",
            "Sample 473 :\n",
            "dist 11.162457466125488\n",
            "Sample 474 :\n",
            "dist 9.328691482543945\n",
            "Sample 475 :\n",
            "dist 10.186614990234375\n",
            "Sample 476 :\n",
            "dist 10.06309700012207\n",
            "Sample 477 :\n",
            "dist 12.167671203613281\n",
            "Sample 478 :\n",
            "dist 10.09423542022705\n",
            "Sample 479 :\n",
            "dist 10.423500061035156\n",
            "Sample 480 :\n",
            "dist 10.07865047454834\n",
            "Sample 481 :\n",
            "dist 5.826895713806152\n",
            "Sample 482 :\n",
            "dist 10.186614990234375\n",
            "Sample 483 :\n",
            "dist 6.647155284881592\n",
            "Sample 484 :\n",
            "dist 6.086319923400879\n",
            "Sample 485 :\n",
            "dist 8.865540504455566\n",
            "Sample 486 :\n",
            "dist 10.215923309326172\n",
            "Sample 487 :\n",
            "dist 11.24622917175293\n",
            "Sample 488 :\n",
            "dist 10.547954559326172\n",
            "Sample 489 :\n",
            "dist 10.473353385925293\n",
            "Sample 490 :\n",
            "dist 10.139031410217285\n",
            "Sample 491 :\n",
            "dist 9.054219245910645\n",
            "Sample 492 :\n",
            "dist 6.647155284881592\n",
            "Sample 493 :\n",
            "dist 9.215288162231445\n",
            "Sample 494 :\n",
            "dist 7.31467342376709\n",
            "Sample 495 :\n",
            "dist 12.123115539550781\n",
            "Sample 496 :\n",
            "dist 9.864704132080078\n",
            "Sample 497 :\n",
            "dist 11.594229698181152\n",
            "Sample 498 :\n",
            "dist 6.714141368865967\n",
            "Sample 499 :\n",
            "dist 10.415663719177246\n",
            "Sample 500 :\n",
            "dist 10.628581047058105\n",
            "Sample 501 :\n",
            "dist 14.29787540435791\n",
            "Sample 502 :\n",
            "dist 9.659614562988281\n",
            "Sample 503 :\n",
            "dist 10.816109657287598\n",
            "Sample 504 :\n",
            "dist 8.77670955657959\n",
            "Sample 505 :\n",
            "dist 9.039032936096191\n",
            "Sample 506 :\n",
            "dist 11.168105125427246\n",
            "Sample 507 :\n",
            "dist 9.004803657531738\n",
            "Sample 508 :\n",
            "dist 9.691022872924805\n",
            "Sample 509 :\n",
            "dist 12.123115539550781\n",
            "Sample 510 :\n",
            "dist 7.356421947479248\n",
            "Sample 511 :\n",
            "dist 8.16422176361084\n",
            "Sample 512 :\n",
            "dist 10.186614990234375\n",
            "Sample 513 :\n",
            "dist 9.215288162231445\n",
            "Sample 514 :\n",
            "dist 13.467585563659668\n",
            "Sample 515 :\n",
            "dist 9.856998443603516\n",
            "Sample 516 :\n",
            "dist 12.336888313293457\n",
            "Sample 517 :\n",
            "dist 7.489496231079102\n",
            "Sample 518 :\n",
            "dist 10.1978759765625\n",
            "Sample 519 :\n",
            "dist 10.813817024230957\n",
            "Sample 520 :\n",
            "dist 9.039032936096191\n",
            "Sample 521 :\n",
            "dist 14.197386741638184\n",
            "Sample 522 :\n",
            "dist 9.572949409484863\n",
            "Sample 523 :\n",
            "dist 8.976008415222168\n",
            "Sample 524 :\n",
            "dist 8.992192268371582\n",
            "Sample 525 :\n",
            "dist 9.822888374328613\n",
            "Sample 526 :\n",
            "dist 7.826977729797363\n",
            "Sample 527 :\n",
            "dist 10.36896800994873\n",
            "Sample 528 :\n",
            "dist 11.162457466125488\n",
            "Sample 529 :\n",
            "dist 8.019840240478516\n",
            "Sample 530 :\n",
            "dist 8.903407096862793\n",
            "Sample 531 :\n",
            "dist 12.167671203613281\n",
            "Sample 532 :\n",
            "dist 5.922364711761475\n",
            "Sample 533 :\n",
            "dist 12.123115539550781\n",
            "Sample 534 :\n",
            "dist 8.976008415222168\n",
            "Sample 535 :\n",
            "dist 9.039032936096191\n",
            "Sample 536 :\n",
            "dist 8.77670955657959\n",
            "Sample 537 :\n",
            "dist 9.659614562988281\n",
            "Sample 538 :\n",
            "dist 8.77670955657959\n",
            "Sample 539 :\n",
            "dist 3.7555344104766846\n",
            "Sample 540 :\n",
            "dist 10.423500061035156\n",
            "Sample 541 :\n",
            "dist 9.864704132080078\n",
            "Sample 542 :\n",
            "dist 10.186614990234375\n",
            "Sample 543 :\n",
            "dist 15.017980575561523\n",
            "Sample 544 :\n",
            "dist 6.722531318664551\n",
            "Sample 545 :\n",
            "dist 11.737712860107422\n",
            "Sample 546 :\n",
            "dist 9.659614562988281\n",
            "Sample 547 :\n",
            "dist 10.565771102905273\n",
            "Sample 548 :\n",
            "dist 9.214948654174805\n",
            "Sample 549 :\n",
            "dist 9.691022872924805\n",
            "Sample 550 :\n",
            "dist 10.814924240112305\n",
            "Sample 551 :\n",
            "dist 9.572949409484863\n",
            "Sample 552 :\n",
            "dist 11.108943939208984\n",
            "Sample 553 :\n",
            "dist 7.444174289703369\n",
            "Sample 554 :\n",
            "dist 10.565771102905273\n",
            "Sample 555 :\n",
            "dist 5.922364711761475\n",
            "Sample 556 :\n",
            "dist 10.592268943786621\n",
            "Sample 557 :\n",
            "dist 10.206989288330078\n",
            "Sample 558 :\n",
            "dist 11.594229698181152\n",
            "Sample 559 :\n",
            "dist 10.547954559326172\n",
            "Sample 560 :\n",
            "dist 10.852871894836426\n",
            "Sample 561 :\n",
            "dist 10.565771102905273\n",
            "Sample 562 :\n",
            "dist 9.328691482543945\n",
            "Sample 563 :\n",
            "dist 8.019840240478516\n",
            "Sample 564 :\n",
            "dist 8.64344596862793\n",
            "Sample 565 :\n",
            "dist 11.942307472229004\n",
            "Sample 566 :\n",
            "dist 9.249247550964355\n",
            "Sample 567 :\n",
            "dist 9.214948654174805\n",
            "Sample 568 :\n",
            "dist 10.053415298461914\n",
            "Sample 569 :\n",
            "dist 15.772144317626953\n",
            "Sample 570 :\n",
            "dist 10.11634349822998\n",
            "Sample 571 :\n",
            "dist 12.167671203613281\n",
            "Sample 572 :\n",
            "dist 12.167671203613281\n",
            "Sample 573 :\n",
            "dist 7.826977729797363\n",
            "Sample 574 :\n",
            "dist 7.801884174346924\n",
            "Sample 575 :\n",
            "dist 7.45473051071167\n",
            "Sample 576 :\n",
            "dist 9.659614562988281\n",
            "Sample 577 :\n",
            "dist 9.328691482543945\n",
            "Sample 578 :\n",
            "dist 12.618820190429688\n",
            "Sample 579 :\n",
            "dist 10.585667610168457\n",
            "Sample 580 :\n",
            "dist 9.659614562988281\n",
            "Sample 581 :\n",
            "dist 9.005356788635254\n",
            "Sample 582 :\n",
            "dist 6.714141368865967\n",
            "Sample 583 :\n",
            "dist 11.099358558654785\n",
            "Sample 584 :\n",
            "dist 8.019840240478516\n",
            "Sample 585 :\n",
            "dist 10.395532608032227\n",
            "Sample 586 :\n",
            "dist 7.444174289703369\n",
            "Sample 587 :\n",
            "dist 9.039032936096191\n",
            "Sample 588 :\n",
            "dist 8.992192268371582\n",
            "Sample 589 :\n",
            "dist 9.054219245910645\n",
            "Sample 590 :\n",
            "dist 7.444174289703369\n",
            "Sample 591 :\n",
            "dist 6.714141368865967\n",
            "Sample 592 :\n",
            "dist 9.328691482543945\n",
            "Sample 593 :\n",
            "dist 10.423500061035156\n",
            "Sample 594 :\n",
            "dist 10.550861358642578\n",
            "Sample 595 :\n",
            "dist 9.856998443603516\n",
            "Sample 596 :\n",
            "dist 10.852871894836426\n",
            "Sample 597 :\n",
            "dist 10.585667610168457\n",
            "Sample 598 :\n",
            "dist 10.215923309326172\n",
            "Sample 599 :\n",
            "dist 10.05289363861084\n",
            "Sample 600 :\n",
            "dist 8.16422176361084\n",
            "Sample 601 :\n",
            "dist 6.086319923400879\n",
            "Sample 602 :\n",
            "dist 7.295797348022461\n",
            "Sample 603 :\n",
            "dist 8.64344596862793\n",
            "Sample 604 :\n",
            "dist 9.572949409484863\n",
            "Sample 605 :\n",
            "dist 10.813817024230957\n",
            "Sample 606 :\n",
            "dist 8.812785148620605\n",
            "Sample 607 :\n",
            "dist 10.11634349822998\n",
            "Sample 608 :\n",
            "dist 9.249247550964355\n",
            "Sample 609 :\n",
            "dist 11.485407829284668\n",
            "Sample 610 :\n",
            "dist 9.822664260864258\n",
            "Sample 611 :\n",
            "dist 12.372003555297852\n",
            "Sample 612 :\n",
            "dist 7.356421947479248\n",
            "Sample 613 :\n",
            "dist 7.826977729797363\n",
            "Sample 614 :\n",
            "dist 8.756172180175781\n",
            "Sample 615 :\n",
            "dist 8.865540504455566\n",
            "Sample 616 :\n",
            "dist 10.814924240112305\n",
            "Sample 617 :\n",
            "dist 6.714141368865967\n",
            "Sample 618 :\n",
            "dist 12.860376358032227\n",
            "Sample 619 :\n",
            "dist 10.1978759765625\n",
            "Sample 620 :\n",
            "dist 8.16422176361084\n",
            "Sample 621 :\n",
            "dist 8.992192268371582\n",
            "Sample 622 :\n",
            "dist 11.24622917175293\n",
            "Sample 623 :\n",
            "dist 11.129412651062012\n",
            "Sample 624 :\n",
            "dist 12.226571083068848\n",
            "Sample 625 :\n",
            "dist 11.207891464233398\n",
            "Sample 626 :\n",
            "dist 7.45473051071167\n",
            "Sample 627 :\n",
            "dist 11.942307472229004\n",
            "Sample 628 :\n",
            "dist 8.16422176361084\n",
            "Sample 629 :\n",
            "dist 10.565771102905273\n",
            "Sample 630 :\n",
            "dist 15.22869873046875\n",
            "Sample 631 :\n",
            "dist 7.31467342376709\n",
            "Sample 632 :\n",
            "dist 10.1978759765625\n",
            "Sample 633 :\n",
            "dist 4.329094409942627\n",
            "Sample 634 :\n",
            "dist 11.737712860107422\n",
            "Sample 635 :\n",
            "dist 8.865540504455566\n",
            "Sample 636 :\n",
            "dist 8.920267105102539\n",
            "Sample 637 :\n",
            "dist 10.09423542022705\n",
            "Sample 638 :\n",
            "dist 9.039032936096191\n",
            "Sample 639 :\n",
            "dist 14.605659484863281\n",
            "Sample 640 :\n",
            "dist 9.822888374328613\n",
            "Sample 641 :\n",
            "dist 7.31467342376709\n",
            "Sample 642 :\n",
            "dist 8.16422176361084\n",
            "Sample 643 :\n",
            "dist 7.928226947784424\n",
            "Sample 644 :\n",
            "dist 10.1978759765625\n",
            "Sample 645 :\n",
            "dist 8.047513008117676\n",
            "Sample 646 :\n",
            "dist 9.691022872924805\n",
            "Sample 647 :\n",
            "dist 7.45473051071167\n",
            "Sample 648 :\n",
            "dist 9.039032936096191\n",
            "Sample 649 :\n",
            "dist 15.017980575561523\n",
            "Sample 650 :\n",
            "dist 10.628581047058105\n",
            "Sample 651 :\n",
            "dist 5.922364711761475\n",
            "Sample 652 :\n",
            "dist 8.16422176361084\n",
            "Sample 653 :\n",
            "dist 9.100789070129395\n",
            "Sample 654 :\n",
            "dist 10.423500061035156\n",
            "Sample 655 :\n",
            "dist 9.856998443603516\n",
            "Sample 656 :\n",
            "dist 10.585667610168457\n",
            "Sample 657 :\n",
            "dist 9.739049911499023\n",
            "Sample 658 :\n",
            "dist 10.36896800994873\n",
            "Sample 659 :\n",
            "dist 9.039032936096191\n",
            "Sample 660 :\n",
            "dist 5.826895713806152\n",
            "Sample 661 :\n",
            "dist 11.942307472229004\n",
            "Sample 662 :\n",
            "dist 11.24622917175293\n",
            "Sample 663 :\n",
            "dist 9.822664260864258\n",
            "Sample 664 :\n",
            "dist 5.0141496658325195\n",
            "Sample 665 :\n",
            "dist 10.547954559326172\n",
            "Sample 666 :\n",
            "dist 11.737712860107422\n",
            "Sample 667 :\n",
            "dist 7.45473051071167\n",
            "Sample 668 :\n",
            "dist 9.214948654174805\n",
            "Sample 669 :\n",
            "dist 9.739049911499023\n",
            "Sample 670 :\n",
            "dist 9.005356788635254\n",
            "Sample 671 :\n",
            "dist 9.822664260864258\n",
            "Sample 672 :\n",
            "dist 11.594229698181152\n",
            "Sample 673 :\n",
            "dist 10.423500061035156\n",
            "Sample 674 :\n",
            "dist 7.801884174346924\n",
            "Sample 675 :\n",
            "dist 7.356421947479248\n",
            "Sample 676 :\n",
            "dist 8.395001411437988\n",
            "Sample 677 :\n",
            "dist 9.005356788635254\n",
            "Sample 678 :\n",
            "dist 10.00361156463623\n",
            "Sample 679 :\n",
            "dist 10.585667610168457\n",
            "Sample 680 :\n",
            "dist 5.6047210693359375\n",
            "Sample 681 :\n",
            "dist 9.089716911315918\n",
            "Sample 682 :\n",
            "dist 11.942307472229004\n",
            "Sample 683 :\n",
            "dist 9.659614562988281\n",
            "Sample 684 :\n",
            "dist 6.714141368865967\n",
            "Sample 685 :\n",
            "dist 9.089716911315918\n",
            "Sample 686 :\n",
            "dist 4.894937038421631\n",
            "Sample 687 :\n",
            "dist 9.039032936096191\n",
            "Sample 688 :\n",
            "dist 9.100789070129395\n",
            "Sample 689 :\n",
            "dist 8.019840240478516\n",
            "Sample 690 :\n",
            "dist 10.506524085998535\n",
            "Sample 691 :\n",
            "dist 11.737712860107422\n",
            "Sample 692 :\n",
            "dist 9.659614562988281\n",
            "Sample 693 :\n",
            "dist 10.1978759765625\n",
            "Sample 694 :\n",
            "dist 11.67253303527832\n",
            "Sample 695 :\n",
            "dist 11.129412651062012\n",
            "Sample 696 :\n",
            "dist 9.739049911499023\n",
            "Sample 697 :\n",
            "dist 11.207891464233398\n",
            "Sample 698 :\n",
            "dist 9.572949409484863\n",
            "Sample 699 :\n",
            "dist 10.816109657287598\n",
            "Sample 700 :\n",
            "dist 12.123115539550781\n",
            "Sample 701 :\n",
            "dist 9.328691482543945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CULsI8xHWm9w",
        "colab_type": "code",
        "outputId": "daf8e8eb-6208-4154-8eee-15523714d0cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df_dist.sort_values(by=0, ascending=\"True\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3.755534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>3.755534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>3.755534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>3.755534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633</th>\n",
              "      <td>4.329094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>4.514034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>4.514034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>4.514034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>4.514034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>4.894937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>686</th>\n",
              "      <td>4.894937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>4.894937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4.894937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>407</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>5.060171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>5.060171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>680</th>\n",
              "      <td>5.604721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5.604721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>5.604721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>5.791200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>5.791200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>5.826896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>660</th>\n",
              "      <td>5.826896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>13.467586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>13.546152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>13.546152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>14.048661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>14.297875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>14.297875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>14.359901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639</th>\n",
              "      <td>14.605659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>14.605659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>14.807356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>14.807356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>14.807356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>405</th>\n",
              "      <td>14.848955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>14.941377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630</th>\n",
              "      <td>15.228699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>15.705333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>15.705333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>15.772144</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>702 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0\n",
              "18    3.755534\n",
              "269   3.755534\n",
              "103   3.755534\n",
              "539   3.755534\n",
              "633   4.329094\n",
              "34    4.514034\n",
              "251   4.514034\n",
              "204   4.514034\n",
              "234   4.514034\n",
              "47    4.894937\n",
              "686   4.894937\n",
              "383   4.894937\n",
              "28    4.894937\n",
              "151   5.014150\n",
              "664   5.014150\n",
              "35    5.014150\n",
              "472   5.014150\n",
              "361   5.014150\n",
              "402   5.014150\n",
              "407   5.014150\n",
              "2     5.014150\n",
              "406   5.060171\n",
              "433   5.060171\n",
              "680   5.604721\n",
              "26    5.604721\n",
              "229   5.604721\n",
              "393   5.791200\n",
              "297   5.791200\n",
              "481   5.826896\n",
              "660   5.826896\n",
              "..         ...\n",
              "514  13.467586\n",
              "198  13.546152\n",
              "310  13.546152\n",
              "226  14.048661\n",
              "521  14.197387\n",
              "446  14.197387\n",
              "38   14.197387\n",
              "453  14.197387\n",
              "250  14.197387\n",
              "187  14.197387\n",
              "501  14.297875\n",
              "24   14.297875\n",
              "319  14.359901\n",
              "639  14.605659\n",
              "178  14.605659\n",
              "88   14.807356\n",
              "162  14.807356\n",
              "154  14.807356\n",
              "405  14.848955\n",
              "132  14.941377\n",
              "51   15.017981\n",
              "342  15.017981\n",
              "125  15.017981\n",
              "543  15.017981\n",
              "414  15.017981\n",
              "649  15.017981\n",
              "630  15.228699\n",
              "19   15.705333\n",
              "218  15.705333\n",
              "569  15.772144\n",
              "\n",
              "[702 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}