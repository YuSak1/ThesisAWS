{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Replication_Lim.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxQ32n5QT3Eo",
        "colab_type": "text"
      },
      "source": [
        "# This is a replication of Lim's method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8AUbb50-UfE",
        "colab_type": "code",
        "outputId": "81e47010-dd2b-4bd0-e2fb-504a3aa495c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "# Check GPU for Google Colab\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()\n",
        " "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=1c1f9736e153b3284da8969d499abfae7a32fbdc7557fcd13a7085813b3813a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 14.0 GB  | Proc size: 15.9 GB\n",
            "GPU RAM Free: 13667MB | Used: 2613MB | Util  16% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lvn4_zUPHP6u",
        "colab_type": "code",
        "outputId": "b0e35139-b5aa-413b-dd14-29c09d3fbd0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qGHk7xnG_0p",
        "colab_type": "code",
        "outputId": "2dc97116-3e77-472a-8fd3-00703eb4b608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import random\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D, concatenate\n",
        "from keras.layers import Activation, BatchNormalization, ZeroPadding2D, Concatenate, Dropout, UpSampling2D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD, Adam, Adadelta\n",
        "from keras.losses import binary_crossentropy\n",
        "from sklearn import preprocessing\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "plt.interactive(False)\n",
        "\n",
        "from itertools import permutations\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from scipy.spatial import distance\n",
        "from PIL import Image, ImageOps\n",
        "import gc\n",
        "import math\n",
        "import time\n",
        "import itertools\n",
        "from operator import itemgetter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7QN81ZvHhkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CNN module as specified in the paper\n",
        "def create_base_network(in_dims):\n",
        "    \"\"\"\n",
        "    Base network to be shared.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        " \n",
        "    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=(in_dims[0],in_dims[1],in_dims[2])))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    model.add(Dropout(0.1))\n",
        "    \n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    model.add(Dropout(0.3))\n",
        "    \n",
        "    model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=3))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(512, (1, 1), padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('elu'))\n",
        "    \n",
        "    model.add(Flatten(name='embeddings'))\n",
        "    \n",
        "    print(model.summary())\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rt78b7oHhpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compile_model(img_channel=1):\n",
        "  print(\"Compiling model...\")\n",
        "  img_size = 112\n",
        "\n",
        "  optim = Adadelta(lr=1.0, rho=0.95, epsilon=0.000001, decay=0.0) # as specified in the paper\n",
        "\n",
        "  anchor_input = Input((img_size,img_size,img_channel, ), name='anchor_input')\n",
        "  positive_input = Input((img_size,img_size,img_channel, ), name='positive_input')\n",
        "  negative_input = Input((img_size,img_size,img_channel, ), name='negative_input')\n",
        "\n",
        "  # Shared embedding layer for positive and negative items\n",
        "  Shared_DNN = create_base_network([img_size,img_size,img_channel])\n",
        "\n",
        "  encoded_anchor = Shared_DNN(anchor_input)\n",
        "  encoded_positive = Shared_DNN(positive_input)\n",
        "  encoded_negative = Shared_DNN(negative_input)\n",
        "\n",
        "  merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
        "\n",
        "  model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
        "  model.compile(loss=triplet_loss, optimizer=optim)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nmqRSI127gHU",
        "colab": {}
      },
      "source": [
        "def evaluate_triplet(X_test, Y_test):\n",
        "  \n",
        "#   X_test = [Negative_train, Positive_train, Anchor_train]\n",
        "#   Y_test = np.zeros((Anchor_train.shape[0],1))\n",
        "  \n",
        "  pred = model.predict(x=X_test, verbose=1)\n",
        "  \n",
        "  # Split into anchor, a, and b sets\n",
        "  total_lenght = pred.shape[1]\n",
        "  pred_anchor = pred[:,0:int(total_lenght*1/3)]\n",
        "  pred_a = pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "  pred_b = pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "  \n",
        "  \n",
        "  # Normalize vectors as specified in the paper\n",
        "  for i in range(len(pred_anchor)):\n",
        "    if np.sum(pred_anchor[i]**2) >= 1:\n",
        "      pred_anchor[i] = preprocessing.normalize([pred_anchor[i]], norm='l2')[0]\n",
        "    if np.sum(pred_a[i]**2) >= 1:\n",
        "      pred_a[i] = preprocessing.normalize([pred_a[i]], norm='l2')[0]\n",
        "    if np.sum(pred_b[i]**2) >= 1:\n",
        "      pred_b[i] = preprocessing.normalize([pred_b[i]], norm='l2')[0]\n",
        "\n",
        "  \n",
        "  result = []\n",
        "  for i in range(pred.shape[0]):\n",
        "    dist1 = distance.euclidean(pred_anchor[i], pred_a[i])\n",
        "    dist2 = distance.euclidean(pred_anchor[i], pred_b[i])\n",
        "\n",
        "    if dist1 < dist2:\n",
        "      result.append(0)\n",
        "    else:\n",
        "      result.append(1)   \n",
        "  \n",
        "  accuracy = accuracy_score(Y_test, result)\n",
        "  print(\"Evaluation accuracy: \", accuracy)\n",
        "\n",
        "  return accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sCxnEs8Mrt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Create two plots; accuracy and loss\n",
        "def save_plot(eval_name, fold_id, view_id):\n",
        "  fig = plt.figure()\n",
        "\n",
        "  # top\n",
        "  ax1 = fig.add_subplot(2, 1, 1)\n",
        "  ax1.plot(df_history['train_loss'])\n",
        "  ax1.plot(df_history['val_loss'])\n",
        "  ax1.set_xlabel(\"epoch\")\n",
        "  ax1.set_ylabel(\"loss\")\n",
        "  plt.legend(['train_loss', 'val_loss'], loc='upper right', fontsize='x-small')\n",
        "  plt.title(\"Fold-\"+str(fold_id)+\", View-\"+str(view_id))\n",
        "\n",
        "  # bottom\n",
        "  ax2 = fig.add_subplot(2, 1, 2)\n",
        "  ax2.plot(df_history['val_acc'])\n",
        "  ax2.set_xlabel(\"epoch\")\n",
        "  ax2.set_ylabel(\"accuracy\")\n",
        "  plt.legend(['val_accuracy'], loc='upper left', fontsize='x-small')\n",
        "\n",
        "#   fig.show()\n",
        "  fig.savefig(\"drive/My Drive/Saved_Images/CV_eval_img/\" + eval_name + \"/Fold-\"+str(fold_id) + \"-View-\"+str(view_id))\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEK0_ssqreZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def data_generator(fold_id,view_ids=[1,6,8]):  \n",
        "#   ## Generate training/test data\n",
        "#   df_train = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_building/df_train_\" + str(fold_id) + \".csv\")\n",
        "#   df_test = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_building/df_test_\" + str(fold_id) + \".csv\")\n",
        "# #   df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
        "# #   df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
        "#   df_train = df_train.reset_index(drop=\"True\")\n",
        "#   df_test = df_test.reset_index(drop=\"True\")\n",
        "\n",
        "#   ### Create image dataset for triplet learning \n",
        "#   img_size = 128\n",
        "#   img_mode = \"L\" #[\"RGB\", \"L\"]\n",
        "#   img_path = \"drive/My Drive/Style_data2_views/building/\"\n",
        "\n",
        "#   out_list = []\n",
        "#   ################ Generate Training Data ################\n",
        "#   name_list = [\"query\", \"pos\", \"neg\"]\n",
        "#   data_length = len(df_train)\n",
        "\n",
        "#   for k in range(0,3):\n",
        "#     print(\"Generating training data\", k+1, \"/3...\")\n",
        "#     input_data = []\n",
        "#     for i in range(data_length):\n",
        "#       for j in view_ids:\n",
        "#           data_id = df_train.loc[:,name_list[k]][i]\n",
        "#           img_file = data_id + \"_\" + str(j) + \".png\"\n",
        "#           img = img_path + img_file\n",
        "#           img = Image.open(img)\n",
        "#           img = ImageOps.expand(img, border=30, fill=0)  # padding with 0s\n",
        "#           img = img.convert(img_mode)\n",
        "#           img = img.resize((img_size, img_size))\n",
        "#           data = np.asarray(img)/255\n",
        "          \n",
        "#           # Randomly crop 112x112 images\n",
        "#           for i in range(2):             # num of cropped images\n",
        "#             rand1 = random.randrange(17)\n",
        "#             rand2 = random.randrange(17)\n",
        "#             data_crop = np.asarray(data[rand1:rand1+112, rand2:rand2+112])\n",
        "\n",
        "#             input_data.append(data_crop)\n",
        "#             input_data.append(np.fliplr(data_crop)) ## Flipped image\n",
        "\n",
        "#     out_list.append(np.array(input_data))\n",
        "    \n",
        "\n",
        "\n",
        "#   ############### Generate Test Data #########################\n",
        "#   data_length = len(df_test)\n",
        "\n",
        "#   for k in range(0,3):\n",
        "#     print(\"Generating test data\", k+1, \"/3...\")\n",
        "#     input_data = []\n",
        "#     for i in range(data_length):\n",
        "#       for j in view_ids:\n",
        "#           data_id = df_test.loc[:,name_list[k]][i]\n",
        "#           img_file = data_id + \"_\" + str(j) + \".png\"\n",
        "#           img = img_path + img_file\n",
        "#           img = Image.open(img)\n",
        "#           img = ImageOps.expand(img, border=25, fill=0) # padding with 0s\n",
        "#           img = img.convert(img_mode)\n",
        "#           img = img.resize((img_size, img_size))\n",
        "#           data = np.asarray(img)/255\n",
        "#           data_crop = np.asarray(data[8:120, 8:120]) # crop a image on the centre\n",
        "#           input_data.append(data_crop)\n",
        "          \n",
        "#     out_list.append(np.array(input_data))\n",
        "\n",
        "#   return out_list\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOji9Qq2HhfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
        "    \"\"\"\n",
        "    Implementation of the triplet loss function\n",
        "    Arguments:\n",
        "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
        "    y_pred -- python list containing three objects:\n",
        "            anchor -- the encodings for the anchor data\n",
        "            positive -- the encodings for the positive data (similar to anchor)\n",
        "            negative -- the encodings for the negative data (different from anchor)\n",
        "    Returns:\n",
        "    loss -- real number, value of the loss\n",
        "    \"\"\"\n",
        "    \n",
        "#     total_lenght = y_pred.shape.as_list()[-1]\n",
        "    total_lenght = 512*3\n",
        "    \n",
        "    anchor = y_pred[:,0:int(total_lenght*1/3)]\n",
        "    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "     \n",
        "    # Normalize vectors as specified in the paper\n",
        "    anchor = K.switch(K.greater_equal(K.sum(K.square(anchor)), 1), lambda: K.l2_normalize(anchor, axis=0), lambda: anchor)\n",
        "    positive = K.switch(K.greater_equal(K.sum(K.square(positive)), 1), lambda: K.l2_normalize(positive, axis=0), lambda: positive)\n",
        "    negative = K.switch(K.greater_equal(K.sum(K.square(negative)), 1), lambda: K.l2_normalize(negative, axis=0), lambda: negative)\n",
        "\n",
        "    # distance between the anchor and the positive\n",
        "    pos_dist = K.sqrt(K.sum(K.square(anchor-positive),axis=1))\n",
        "\n",
        "    # distance between the anchor and the negative\n",
        "    neg_dist = K.sqrt(K.sum(K.square(anchor-negative),axis=1))\n",
        "\n",
        "    # compute loss\n",
        "    basic_loss = pos_dist-neg_dist+alpha\n",
        "    loss = K.maximum(basic_loss,0.0)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Vv9DnMEOAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_triplet_vote(X_test, Y_test, num_views):\n",
        "  \n",
        "  pred = model.predict(x=X_test, verbose=1)\n",
        "  \n",
        "  # Split into anchor, a, and b sets\n",
        "  total_lenght = pred.shape[1]\n",
        "  pred_anchor = pred[:,0:int(total_lenght*1/3)]\n",
        "  pred_a = pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "  pred_b = pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "  \n",
        "  # Normalize vectors as specified in the paper\n",
        "  for i in range(len(pred_anchor)):\n",
        "    if np.sum(pred_anchor[i]**2) >= 1:\n",
        "      pred_anchor[i] = preprocessing.normalize([pred_anchor[i]], norm='l2')[0]\n",
        "    if np.sum(pred_a[i]**2) >= 1:\n",
        "      pred_a[i] = preprocessing.normalize([pred_a[i]], norm='l2')[0]\n",
        "    if np.sum(pred_b[i]**2) >= 1:\n",
        "      pred_b[i] = preprocessing.normalize([pred_b[i]], norm='l2')[0]\n",
        "  \n",
        "  \n",
        "  result_tmp = []\n",
        "  result = []\n",
        "  for i in range(pred.shape[0]):\n",
        "    dist1 = distance.euclidean(pred_anchor[i], pred_a[i])\n",
        "    dist2 = distance.euclidean(pred_anchor[i], pred_b[i])\n",
        "\n",
        "    if dist1 < dist2:\n",
        "      result_tmp.append(0)\n",
        "    else:\n",
        "      result_tmp.append(1)\n",
        "      \n",
        "    if len(result_tmp)==num_views:\n",
        "      if (sum(result_tmp) < math.ceil(num_views/2)):\n",
        "        result.append(0)\n",
        "      else:\n",
        "        result.append(1)\n",
        "      \n",
        "      result_tmp=[]\n",
        "\n",
        "  print(\"Voting evaluation accuracy: \", accuracy_score(Y_test[0:int(len(Y_test)/num_views)], result))\n",
        "\n",
        "  return accuracy_score(Y_test[0:int(len(Y_test)/num_views)], result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxp6A0QgU8P0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator2(fold_id,view_ids=[1,2,3], num_triplets=5):  \n",
        "  ## Generate training/test data\n",
        "  df_train = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_building/df_train_\" + str(fold_id) + \".csv\")\n",
        "  df_test = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_building/df_test_\" + str(fold_id) + \".csv\")\n",
        "\n",
        "  df_train = df_train.reset_index(drop=\"True\")\n",
        "  df_test = df_test.reset_index(drop=\"True\")\n",
        "\n",
        "  print(\"Training original size: \", df_train.shape)\n",
        "  print(\"Test original size: \", df_test.shape)\n",
        "  ### Create image dataset for triplet learning \n",
        "  img_size = 128\n",
        "  img_mode = \"L\" #[\"RGB\", \"L\"]\n",
        "  img_path = \"drive/My Drive/Style_data2_views/building/\"\n",
        "  \n",
        "  ### All combinations of views ###\n",
        "  view_comb = []\n",
        "  for i in itertools.product(view_ids, repeat=3):\n",
        "    view_comb.append(i)\n",
        "\n",
        "  out_list = [] # final output\n",
        "  ################ Generate Training Data ################\n",
        "  name_list = [\"query\", \"pos\", \"neg\"]\n",
        "  data_length = len(df_train)\n",
        "  \n",
        "  print(\"Generating training data...\")\n",
        "  \n",
        "  out = [[], [], []]\n",
        "  cur_triplets_all = []\n",
        "  \n",
        "  for i in range(data_length):\n",
        "    cur_triplets_sub = []\n",
        "    rand_list = random.sample(range(0, len(view_comb)), num_triplets)\n",
        "    for j in range(num_triplets):\n",
        "      view_out = view_comb[rand_list[j]]           # randomly selected view combination\n",
        "      cur_triplets_sub.append(view_out)\n",
        "      \n",
        "      for k in range(0,3):\n",
        "          data_id = df_train.loc[:,name_list[k]][i]\n",
        "          img_file = data_id + \"_\" + str(view_out[k]) + \".png\"\n",
        "          img = img_path + img_file\n",
        "          img = Image.open(img)\n",
        "          img = ImageOps.expand(img, border=30, fill=0)  # padding with 0s\n",
        "          img = img.convert(img_mode)\n",
        "          img = img.resize((img_size, img_size))\n",
        "          data = np.asarray(img)/255\n",
        "          \n",
        "          # Randomly crop 112x112 images\n",
        "          rand1 = random.randrange(17)\n",
        "          rand2 = random.randrange(17)\n",
        "          data_crop = np.asarray(data[rand1:rand1+112, rand2:rand2+112])\n",
        "\n",
        "          # Flip image by 50% probability\n",
        "          rand_flip = random.randrange(2)\n",
        "          if rand_flip==0:\n",
        "            out[k].append(data_crop)\n",
        "          else:\n",
        "            out[k].append(np.fliplr(data_crop)) ## Flipped image\n",
        "            \n",
        "    cur_triplets_all.append(cur_triplets_sub)\n",
        "    \n",
        "    if i%5 == 0:\n",
        "      print(\"\\r{0}\".format(i), \"/\", data_length,  end=\"\")\n",
        "    \n",
        "  out_list.append(np.array(out[0]))\n",
        "  out_list.append(np.array(out[1]))\n",
        "  out_list.append(np.array(out[2]))\n",
        "    \n",
        "\n",
        "\n",
        "  ############### Generate Test Data #########################\n",
        "  data_length = len(df_test)\n",
        "\n",
        "  for k in range(0,3):\n",
        "    print(\"Generating test data\", k+1, \"/3...\")\n",
        "    out345 = []\n",
        "    for i in range(data_length):\n",
        "      for j in view_ids:\n",
        "          data_id = df_test.loc[:,name_list[k]][i]\n",
        "          img_file = data_id + \"_\" + str(j) + \".png\"\n",
        "          img = img_path + img_file\n",
        "          img = Image.open(img)\n",
        "          img = ImageOps.expand(img, border=25, fill=0) # padding with 0s\n",
        "          img = img.convert(img_mode)\n",
        "          img = img.resize((img_size, img_size))\n",
        "          data = np.asarray(img)/255\n",
        "          data_crop = np.asarray(data[8:120, 8:120]) # crop a image on the centre\n",
        "          out345.append(data_crop)\n",
        "          \n",
        "    out_list.append(np.array(out345))\n",
        "\n",
        "  return [out_list, np.array(cur_triplets_all)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJFZDCu_MCjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Regenerate training set every 10 epoch as proposed in the paper \n",
        "\n",
        "def regenerate_set(fold_id, train_set, cur_triplets, num_triplets, view_ids=[1,2,3]):\n",
        "  margin = 0.2 # margin for gap criteria\n",
        "#   print(\"Current:\", cur_triplets)\n",
        "  \n",
        "  df_train = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_building/df_train_\" + str(fold_id) + \".csv\").reset_index(drop=\"True\")\n",
        "  \n",
        "  ### Create image dataset for triplet learning \n",
        "  img_size = 128\n",
        "  img_mode = \"L\" #[\"RGB\", \"L\"]\n",
        "  img_path = \"drive/My Drive/Style_data2_views/building/\"\n",
        "  \n",
        "  print(\"=========== Regenerating training set ===========\")\n",
        "  \n",
        "  #########################  Select 50% of view combinations from each triplet to retain  ##################################\n",
        "  Anchor_train_all = train_set[0]\n",
        "  Positive_train_all = train_set[1]\n",
        "  Negative_train_all = train_set[2]\n",
        "  pred = model.predict(x=[Anchor_train_all,Positive_train_all,Negative_train_all],verbose=1)\n",
        "  \n",
        "  total_width = pred.shape[1]\n",
        "  data_length = len(pred)\n",
        "\n",
        "  pred_a = pred[:,int(total_width*1/3):int(total_width*2/3)]\n",
        "  pred_b = pred[:,int(total_width*2/3):int(total_width*3/3)]\n",
        "  \n",
        "  dist_pos_neg = np.sqrt(np.sum(np.power(pred_a - pred_b, 2),axis=1))\n",
        "\n",
        "  df_dist = pd.DataFrame()\n",
        "  df_dist[\"dist\"] = dist_pos_neg\n",
        "  index_retain = []\n",
        "  index_cur = []\n",
        "  for i in range(0, data_length, num_triplets):\n",
        "    df_dist_sub = df_dist.iloc[i:i+num_triplets]\n",
        "    df_dist_sub = df_dist_sub.sort_values(by=\"dist\", ascending=False)\n",
        "    index_retain.append(df_dist_sub.head(math.floor(num_triplets/4)).index.values.tolist())\n",
        "    index_retain.append(df_dist_sub.tail(math.floor(num_triplets/4)).index.values.tolist())\n",
        "    index_cur.append(df_dist_sub.index.values.tolist()) # used later\n",
        "  \n",
        "  cur_triplets_flat = list(itertools.chain.from_iterable(cur_triplets))\n",
        "  cur_triplets_flat = [l.tolist() for l in cur_triplets_flat]\n",
        "\n",
        "  index_retain = list(itertools.chain.from_iterable(index_retain))\n",
        "  index_retain.sort()\n",
        "  print(\"index_retain: \", index_retain)\n",
        "  print(np.array(index_retain).shape)\n",
        "  print(Anchor_train_all.shape)\n",
        "  Anchor_train_retain = Anchor_train_all[index_retain]\n",
        "  Positive_train_retain = Positive_train_all[index_retain]\n",
        "  Negative_train_retain = Negative_train_all[index_retain]\n",
        "\n",
        "  remaining_triplets = np.array(list(itertools.chain.from_iterable(np.array(cur_triplets_flat)[index_retain]))).reshape(-1, math.floor(num_triplets/4)*2, 3).tolist()\n",
        "\n",
        "  print(\"\\nRemaining 50% length:\", len(Anchor_train_retain))\n",
        "#   print(\"Remaining:\", remaining_triplets)\n",
        "  \n",
        "  \n",
        "  ###################################     Select new 50% of view combinations for each triplet    ############################################\n",
        "  # All combinations of views\n",
        "  view_comb = []\n",
        "  new_triplets = []\n",
        "  Anchor_train_new = []\n",
        "  Positive_train_new = []\n",
        "  Negative_train_new = []\n",
        "  for i in itertools.product(view_ids, repeat=3):\n",
        "    view_comb.append(i)\n",
        "  view_comb = np.array(view_comb)\n",
        "  print(\"view_comb shape: \", view_comb.shape)\n",
        "  for i in range(int(data_length/num_triplets)):\n",
        "    new_triplets_sub = [item for item in view_comb.tolist() if item not in cur_triplets[i].tolist()]\n",
        "    new_triplets.append(new_triplets_sub)\n",
        "\n",
        "  # Compute the distance of all the view combinations that are not in the current triplet\n",
        "  name_list = [\"query\", \"pos\", \"neg\"]\n",
        "  \n",
        "  for i in range(int(data_length/num_triplets)):\n",
        "    print(\"\\r{0}\".format(i), \"/\", int(data_length/num_triplets),  end=\"\")\n",
        "    out_new_triplets = [[], [], []]\n",
        "    for j in range(len(new_triplets[i])):\n",
        "      view_out = new_triplets[i][j]\n",
        "      for k in range(0,3):\n",
        "          data_id = df_train.loc[:,name_list[k]][i]\n",
        "          img_file = data_id + \"_\" + str(view_out[k]) + \".png\"\n",
        "          img = img_path + img_file\n",
        "          img = Image.open(img)\n",
        "          img = ImageOps.expand(img, border=30, fill=0)  # padding with 0s\n",
        "          img = img.convert(img_mode)\n",
        "          img = img.resize((img_size, img_size))\n",
        "          data = np.asarray(img)/255\n",
        "          \n",
        "          # Randomly crop 112x112 images\n",
        "          rand1 = random.randrange(17)\n",
        "          rand2 = random.randrange(17)\n",
        "          data_crop = np.asarray(data[rand1:rand1+112, rand2:rand2+112])\n",
        "\n",
        "          # Flip image by 50% probability\n",
        "          rand_flip = random.randrange(2)\n",
        "          if rand_flip==0:\n",
        "            out_new_triplets[k].append(data_crop)\n",
        "          else:\n",
        "            out_new_triplets[k].append(np.fliplr(data_crop)) ## Flipped image\n",
        "     \n",
        "    # Generate triplets for all view combinations\n",
        "    Anchor_train_all = out_new_triplets[0]\n",
        "    Positive_train_all = out_new_triplets[1]\n",
        "    Negative_train_all = out_new_triplets[2]\n",
        "\n",
        "    # add a new dimension\n",
        "    Anchor_train_all = np.array(Anchor_train_all)[:, :, :, np.newaxis]\n",
        "    Positive_train_all = np.array(Positive_train_all)[:, :, :, np.newaxis]\n",
        "    Negative_train_all = np.array(Negative_train_all)[:, :, :, np.newaxis]\n",
        "    pred = model.predict(x=[Anchor_train_all,Positive_train_all,Negative_train_all],verbose=0)\n",
        "\n",
        "    data_length_new_triplets = len(pred)\n",
        "\n",
        "    pred_anchor = pred[:,0:int(total_width*1/3)]\n",
        "    pred_a = pred[:,int(total_width*1/3):int(total_width*2/3)]\n",
        "    pred_b = pred[:,int(total_width*2/3):int(total_width*3/3)]\n",
        "\n",
        "    dist_pos = np.sqrt(np.sum(np.power(pred_anchor-pred_a, 2),axis=1))\n",
        "    dist_neg = np.sqrt(np.sum(np.power(pred_anchor-pred_b, 2),axis=1))\n",
        "    dist_diff = dist_neg - dist_pos\n",
        "\n",
        "    df_dist = pd.DataFrame()\n",
        "    df_dist[\"dist\"] = dist_diff\n",
        "    if  len(df_dist[df_dist[\"dist\"] >= margin]) >= math.ceil(num_triplets/2):\n",
        "      df_dist = df_dist[df_dist[\"dist\"] >= margin]\n",
        "      rand_index = random.sample(range(0, len(df_dist)), math.ceil(num_triplets/2))\n",
        "      rand_index.sort()\n",
        "      new50_index = df_dist.iloc[rand_index].index\n",
        "    else:\n",
        "      new50_index = df_dist.sort_values('dist', ascending=False).head(math.ceil(num_triplets/2))[\"dist\"].index.tolist()\n",
        "      new50_index.sort()\n",
        "      \n",
        "    Anchor_train_new.append(Anchor_train_all[new50_index[0],:,:,:])\n",
        "    Anchor_train_new.append(Anchor_train_all[new50_index[1],:,:,:])\n",
        "    Positive_train_new.append(Positive_train_all[new50_index[0],:,:,:])\n",
        "    Positive_train_new.append(Positive_train_all[new50_index[1],:,:,:])\n",
        "    Negative_train_new.append(Negative_train_all[new50_index[0],:,:,:])\n",
        "    Negative_train_new.append(Negative_train_all[new50_index[1],:,:,:])\n",
        "    new_triplets[i] = [new_triplets[i][s] for s in new50_index]\n",
        "\n",
        "    \n",
        "  remaining_triplets_flat = list(itertools.chain.from_iterable(remaining_triplets))\n",
        "  new_triplets_flat = list(itertools.chain.from_iterable(new_triplets))\n",
        "  \n",
        "  cur_triplets_flat = list(itertools.chain.from_iterable(cur_triplets))\n",
        "  cur_triplets_flat = [l.tolist() for l in cur_triplets_flat]\n",
        "\n",
        "  print(\"\\nNew 50% length: \", len(Anchor_train_new))\n",
        "\n",
        "  Anchor_train_all_new = []\n",
        "  Positive_train_all_new = []\n",
        "  Negative_train_all_new = []\n",
        "  new_triplets_all = []\n",
        "  if len(Anchor_train_retain) < len(Anchor_train_new):\n",
        "    j = 0\n",
        "    for i in range(len(Anchor_train_retain)):\n",
        "      Anchor_train_all_new.append(Anchor_train_retain[i])\n",
        "      Anchor_train_all_new.append(Anchor_train_new[j])\n",
        "      Anchor_train_all_new.append(Anchor_train_new[j+1])\n",
        "      Positive_train_all_new.append(Positive_train_retain[i])\n",
        "      Positive_train_all_new.append(Positive_train_new[j])\n",
        "      Positive_train_all_new.append(Positive_train_new[j+1])\n",
        "      Negative_train_all_new.append(Negative_train_retain[i])\n",
        "      Negative_train_all_new.append(Negative_train_new[j])\n",
        "      Negative_train_all_new.append(Negative_train_new[j+1])\n",
        "      new_triplets_all.append(remaining_triplets_flat[j+1])\n",
        "      new_triplets_all.append(new_triplets_flat[j+1])\n",
        "      new_triplets_all.append(new_triplets_flat[j+1])\n",
        "      j += 2\n",
        "  else:\n",
        "    for i in range(len(Anchor_train_retain)):\n",
        "      Anchor_train_all_new.append(Anchor_train_retain[i])\n",
        "      Anchor_train_all_new.append(Anchor_train_new[i])\n",
        "      Positive_train_all_new.append(Positive_train_retain[i])\n",
        "      Positive_train_all_new.append(Positive_train_new[i])\n",
        "      Negative_train_all_new.append(Negative_train_retain[i])\n",
        "      Negative_train_all_new.append(Negative_train_new[i])\n",
        "      new_triplets_all.append(remaining_triplets_flat[j+1])\n",
        "      new_triplets_all.append(new_triplets_flat[j+1])\n",
        "    \n",
        "    if i%50 == 0:\n",
        "      print(\"\\r{0}\".format(i), \"/\", len(Anchor_train_retain),  end=\"\")\n",
        "      \n",
        "  new_triplets_all = np.array(new_triplets_all).reshape(-1, num_triplets, 3)\n",
        "                              \n",
        "  print(\"Anchor_train_all_new shape:\", np.array(Anchor_train_all_new).shape)\n",
        "  return [[np.array(Anchor_train_all_new), np.array(Positive_train_all_new), np.array(Negative_train_all_new)], new_triplets_all]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFluQG8PDNqN",
        "colab_type": "text"
      },
      "source": [
        "# 10-fold-cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kySp9S6VQaMU",
        "colab_type": "code",
        "outputId": "8944342e-b36c-4c32-bf9b-4cef601eb5f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# %%capture\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "## Multi-view version\n",
        "## Training using all views\n",
        "\n",
        "# Time\n",
        "start = time.time()\n",
        "\n",
        "##### 10-fold cross-validation ###########\n",
        "for k in range(1,11):\n",
        "  \n",
        "  print(\"==================== Fold:\", k, \"/10 ===================\")\n",
        "  ## Generate k th fold\n",
        "#   views = [0,2,4,6,8,10,12,14]\n",
        "  views = [0,4,8,12]\n",
        "  num_triplets = 12   # number of view combinations to generate for each triplet\n",
        "  generated_data, cur_triplets = data_generator2(k,view_ids=views, num_triplets=num_triplets)\n",
        "  Anchor_train_all = generated_data[0]\n",
        "  Positive_train_all = generated_data[1]\n",
        "  Negative_train_all = generated_data[2]\n",
        "  # Use for RGB mode\n",
        "  Anchor_train_all = Anchor_train_all[:, :, :, np.newaxis]\n",
        "  Positive_train_all = Positive_train_all[:, :, :, np.newaxis]\n",
        "  Negative_train_all = Negative_train_all[:, :, :, np.newaxis]\n",
        "\n",
        "  Anchor_test_all = generated_data[3]\n",
        "  Positive_test_all = generated_data[4]\n",
        "  Negative_test_all = generated_data[5]\n",
        "  # Use for RGB mode\n",
        "  Anchor_test_all = Anchor_test_all[:, :, :, np.newaxis]\n",
        "  Positive_test_all = Positive_test_all[:, :, :, np.newaxis]\n",
        "  Negative_test_all = Negative_test_all[:, :, :, np.newaxis]\n",
        "\n",
        "\n",
        "  Y_dummy1 = np.zeros((Anchor_train_all.shape[0],1))\n",
        "  Y_dummy2 = np.zeros((Anchor_test_all.shape[0],1))\n",
        "\n",
        "  train_length =int(Anchor_train_all.shape[0]/3)\n",
        "  test_length =int(Anchor_test_all.shape[0]/3)\n",
        "  \n",
        "  num_epoch = 300\n",
        "  \n",
        "  ## Complie model\n",
        "  model = compile_model(img_channel=1)\n",
        "\n",
        "  df_history = pd.DataFrame(columns=['train_loss','val_loss','train_acc','val_acc'])\n",
        "  for i in range(1,num_epoch+1):\n",
        "    print(\"Epoch:\",i,\"/\",num_epoch)\n",
        "    \n",
        "    # Regenerate training set every 10 epoch\n",
        "    if i%10==0 and i>0:\n",
        "      [Anchor_train_all, Positive_train_all, Negative_train_all], cur_triplets = regenerate_set(fold_id=k,\n",
        "                                                                                                train_set=[Anchor_train_all, Positive_train_all, Negative_train_all],\n",
        "                                                                                                cur_triplets=cur_triplets,\n",
        "                                                                                                num_triplets=num_triplets,\n",
        "                                                                                                view_ids=views)\n",
        "      \n",
        "    \n",
        "    \n",
        "    val_acc = evaluate_triplet(X_test = [Negative_test_all, Positive_test_all, Anchor_test_all], \n",
        "                                Y_test = np.zeros((Anchor_test_all.shape[0],1)))\n",
        "    train_acc = evaluate_triplet(X_test = [Negative_train_all, Positive_train_all, Anchor_train_all], \n",
        "                                Y_test = np.zeros((Anchor_train_all.shape[0],1)))\n",
        "    \n",
        "#     evaluate_triplet_vote(X_test = [Negative_test_all, Positive_test_all, Anchor_test_all], \n",
        "#                                 Y_test = np.zeros((Anchor_test_all.shape[0],1)), num_views=len(views))\n",
        "    \n",
        "    history= model.fit(x=[Anchor_train_all, Positive_train_all, Negative_train_all], y=Y_dummy1,\n",
        "                       validation_data=([Anchor_test_all,Positive_test_all,Negative_test_all],Y_dummy2),\n",
        "                batch_size=64, epochs=1, verbose=1, shuffle=True)\n",
        "\n",
        "    df_history = df_history.append(pd.DataFrame([history.history['loss'], history.history['val_loss'], [train_acc], [val_acc]],\n",
        "                                                index=['train_loss','val_loss','train_acc','val_acc']).T)\n",
        "    df_history = df_history.reset_index(drop=\"Ture\")\n",
        "\n",
        "    # elapsed time\n",
        "    elapsed_time = time.time() - start\n",
        "    print(\"Elapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)), \"\\n\")\n",
        "    \n",
        "  save_plot(\"Lun_building\", fold_id=k, view_id=1)\n",
        "  \n",
        "  gc.collect()\n",
        "  \n",
        "  # Save a list of accuracy\n",
        "  df_history.to_csv(\"drive/My Drive/MultiviewAccuracy_10CV/Lun_building/df_log_\" + str(k) + \".csv\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================== Fold: 1 /10 ===================\n",
            "Training original size:  (636, 3)\n",
            "Test original size:  (30, 3)\n",
            "Generating training data...\n",
            "635 / 636Generating test data 1 /3...\n",
            "Generating test data 2 /3...\n",
            "Generating test data 3 /3...\n",
            "Compiling model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 112, 112, 32)      320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 37, 37, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 37, 37, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 37, 37, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 37, 37, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 1, 1, 512)         131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1, 1, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "embeddings (Flatten)         (None, 512)               0         \n",
            "=================================================================\n",
            "Total params: 523,392\n",
            "Trainable params: 521,408\n",
            "Non-trainable params: 1,984\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Epoch: 1 / 300\n",
            "120/120 [==============================] - 5s 39ms/step\n",
            "Evaluation accuracy:  0.5083333333333333\n",
            "7632/7632 [==============================] - 3s 439us/step\n",
            "Evaluation accuracy:  0.49777253668763105\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 7632 samples, validate on 120 samples\n",
            "Epoch 1/1\n",
            "7632/7632 [==============================] - 11s 1ms/step - loss: 0.2475 - val_loss: 0.5746\n",
            "Elapsed time: 00:01:50 \n",
            "\n",
            "Epoch: 2 / 300\n",
            "120/120 [==============================] - 0s 443us/step\n",
            "Evaluation accuracy:  0.48333333333333334\n",
            "7632/7632 [==============================] - 3s 414us/step\n",
            "Evaluation accuracy:  0.49580712788259956\n",
            "Train on 7632 samples, validate on 120 samples\n",
            "Epoch 1/1\n",
            "7632/7632 [==============================] - 8s 1ms/step - loss: 0.2384 - val_loss: 0.5377\n",
            "Elapsed time: 00:02:04 \n",
            "\n",
            "Epoch: 3 / 300\n",
            "120/120 [==============================] - 0s 432us/step\n",
            "Evaluation accuracy:  0.525\n",
            "7632/7632 [==============================] - 3s 415us/step\n",
            "Evaluation accuracy:  0.4944968553459119\n",
            "Train on 7632 samples, validate on 120 samples\n",
            "Epoch 1/1\n",
            "7632/7632 [==============================] - 8s 1ms/step - loss: 0.2391 - val_loss: 0.5436\n",
            "Elapsed time: 00:02:18 \n",
            "\n",
            "Epoch: 4 / 300\n",
            "120/120 [==============================] - 0s 447us/step\n",
            "Evaluation accuracy:  0.4666666666666667\n",
            "7632/7632 [==============================] - 3s 415us/step\n",
            "Evaluation accuracy:  0.48991090146750527\n",
            "Train on 7632 samples, validate on 120 samples\n",
            "Epoch 1/1\n",
            "7632/7632 [==============================] - 8s 1ms/step - loss: 0.2422 - val_loss: 0.6017\n",
            "Elapsed time: 00:02:32 \n",
            "\n",
            "Epoch: 5 / 300\n",
            "120/120 [==============================] - 0s 435us/step\n",
            "Evaluation accuracy:  0.4583333333333333\n",
            "7632/7632 [==============================] - 3s 413us/step\n",
            "Evaluation accuracy:  0.49724842767295596\n",
            "Train on 7632 samples, validate on 120 samples\n",
            "Epoch 1/1\n",
            "7632/7632 [==============================] - 8s 1ms/step - loss: 0.2449 - val_loss: 0.5984\n",
            "Elapsed time: 00:02:46 \n",
            "\n",
            "Epoch: 6 / 300\n",
            "120/120 [==============================] - 0s 446us/step\n",
            "Evaluation accuracy:  0.4666666666666667\n",
            "7632/7632 [==============================] - 3s 415us/step\n",
            "Evaluation accuracy:  0.4985587002096436\n",
            "Train on 7632 samples, validate on 120 samples\n",
            "Epoch 1/1\n",
            "7632/7632 [==============================] - 8s 1ms/step - loss: 0.2327 - val_loss: 0.5952\n",
            "Elapsed time: 00:03:00 \n",
            "\n",
            "Epoch: 7 / 300\n",
            "120/120 [==============================] - 0s 438us/step\n",
            "Evaluation accuracy:  0.48333333333333334\n",
            "7632/7632 [==============================] - 3s 412us/step\n",
            "Evaluation accuracy:  0.49842767295597484\n",
            "Train on 7632 samples, validate on 120 samples\n",
            "Epoch 1/1\n",
            "7632/7632 [==============================] - 8s 1ms/step - loss: 0.2358 - val_loss: 0.5564\n",
            "Elapsed time: 00:03:14 \n",
            "\n",
            "Epoch: 8 / 300\n",
            "120/120 [==============================] - 0s 423us/step\n",
            "Evaluation accuracy:  0.4666666666666667\n",
            "7632/7632 [==============================] - 3s 415us/step\n",
            "Evaluation accuracy:  0.5010482180293501\n",
            "Train on 7632 samples, validate on 120 samples\n",
            "Epoch 1/1\n",
            "7632/7632 [==============================] - 8s 1ms/step - loss: 0.2420 - val_loss: 0.5514\n",
            "Elapsed time: 00:03:28 \n",
            "\n",
            "Epoch: 9 / 300\n",
            "120/120 [==============================] - 0s 430us/step\n",
            "Evaluation accuracy:  0.4583333333333333\n",
            "7632/7632 [==============================] - 3s 414us/step\n",
            "Evaluation accuracy:  0.49724842767295596\n",
            "Train on 7632 samples, validate on 120 samples\n",
            "Epoch 1/1\n",
            "7632/7632 [==============================] - 8s 1ms/step - loss: 0.2353 - val_loss: 0.5777\n",
            "Elapsed time: 00:03:42 \n",
            "\n",
            "Epoch: 10 / 300\n",
            "=========== Regenerating training set ===========\n",
            "7632/7632 [==============================] - 3s 414us/step\n",
            "index_retain:  [0, 2, 3, 5, 10, 11, 13, 15, 16, 17, 19, 20, 24, 27, 28, 29, 31, 32, 36, 37, 38, 40, 44, 45, 49, 51, 52, 53, 55, 57, 60, 63, 65, 67, 69, 71, 72, 73, 75, 76, 78, 81, 86, 88, 91, 92, 93, 94, 96, 97, 98, 102, 105, 107, 111, 115, 116, 117, 118, 119, 120, 124, 126, 128, 129, 130, 133, 134, 136, 138, 142, 143, 147, 148, 149, 150, 151, 152, 156, 157, 158, 160, 162, 164, 169, 174, 176, 177, 178, 179, 182, 183, 184, 185, 186, 187, 192, 193, 195, 197, 198, 200, 204, 205, 206, 207, 208, 214, 217, 218, 220, 221, 225, 227, 229, 232, 236, 237, 238, 239, 242, 243, 246, 247, 250, 251, 252, 253, 256, 258, 260, 262, 264, 265, 271, 272, 273, 275, 276, 279, 281, 283, 286, 287, 290, 293, 294, 295, 296, 299, 302, 304, 305, 307, 310, 311, 312, 315, 316, 317, 319, 320, 325, 326, 328, 329, 333, 334, 336, 337, 339, 345, 346, 347, 349, 351, 353, 355, 356, 359, 360, 361, 363, 364, 365, 368, 374, 375, 376, 377, 380, 381, 385, 386, 387, 388, 390, 392, 396, 397, 398, 400, 401, 407, 409, 410, 411, 412, 414, 416, 420, 425, 426, 427, 428, 430, 432, 433, 435, 439, 442, 443, 444, 445, 446, 449, 453, 455, 459, 460, 461, 462, 463, 467, 468, 470, 472, 476, 477, 479, 480, 481, 482, 483, 487, 489, 494, 495, 496, 497, 498, 501, 504, 505, 507, 510, 511, 512, 519, 520, 521, 524, 525, 527, 528, 529, 533, 536, 537, 538, 540, 541, 546, 547, 548, 549, 555, 557, 558, 560, 561, 563, 566, 567, 569, 571, 572, 575, 579, 580, 581, 584, 586, 587, 588, 590, 592, 594, 595, 597, 600, 603, 604, 605, 607, 611, 613, 615, 617, 618, 620, 621, 627, 628, 629, 631, 632, 634, 637, 638, 640, 641, 644, 647, 650, 651, 652, 653, 656, 657, 660, 661, 663, 666, 670, 671, 672, 673, 675, 680, 681, 682, 684, 687, 691, 693, 694, 695, 697, 702, 703, 705, 706, 707, 710, 712, 713, 714, 715, 719, 720, 722, 724, 726, 727, 730, 734, 736, 737, 739, 741, 743, 744, 745, 747, 750, 751, 753, 756, 757, 758, 760, 764, 766, 769, 770, 773, 775, 776, 779, 780, 782, 783, 785, 786, 791, 792, 794, 795, 800, 802, 803, 805, 807, 808, 813, 814, 815, 816, 817, 818, 821, 825, 827, 828, 829, 830, 831, 834, 837, 841, 842, 843, 844, 846, 847, 853, 855, 856, 857, 860, 862, 867, 870, 871, 872, 874, 875, 876, 877, 878, 879, 883, 887, 888, 889, 894, 896, 898, 899, 900, 903, 905, 907, 908, 910, 913, 914, 915, 916, 917, 923, 926, 927, 929, 931, 932, 934, 937, 938, 939, 944, 946, 947, 948, 949, 951, 952, 955, 956, 961, 962, 964, 965, 966, 970, 972, 975, 976, 978, 979, 981, 985, 986, 987, 989, 990, 995, 997, 999, 1000, 1002, 1003, 1005, 1010, 1011, 1012, 1016, 1017, 1018, 1022, 1024, 1026, 1028, 1030, 1031, 1033, 1034, 1039, 1040, 1041, 1042, 1047, 1049, 1050, 1051, 1052, 1054, 1058, 1059, 1061, 1062, 1066, 1067, 1070, 1071, 1073, 1075, 1076, 1079, 1080, 1081, 1084, 1085, 1090, 1091, 1092, 1093, 1094, 1097, 1099, 1101, 1104, 1107, 1108, 1110, 1111, 1112, 1116, 1117, 1121, 1124, 1125, 1127, 1128, 1131, 1132, 1134, 1136, 1139, 1140, 1143, 1144, 1146, 1148, 1149, 1152, 1155, 1157, 1158, 1161, 1163, 1164, 1167, 1168, 1170, 1171, 1174, 1178, 1181, 1182, 1184, 1185, 1187, 1188, 1191, 1192, 1193, 1194, 1197, 1200, 1203, 1206, 1207, 1208, 1209, 1212, 1213, 1216, 1217, 1218, 1222, 1224, 1225, 1228, 1230, 1232, 1235, 1237, 1238, 1239, 1242, 1244, 1245, 1248, 1252, 1254, 1255, 1258, 1259, 1260, 1262, 1264, 1267, 1270, 1271, 1272, 1275, 1276, 1278, 1279, 1280, 1284, 1286, 1287, 1289, 1293, 1295, 1296, 1298, 1300, 1301, 1303, 1304, 1309, 1310, 1312, 1314, 1316, 1319, 1320, 1322, 1325, 1326, 1329, 1331, 1332, 1338, 1339, 1340, 1341, 1342, 1344, 1345, 1349, 1350, 1351, 1355, 1358, 1360, 1363, 1364, 1365, 1366, 1369, 1370, 1372, 1373, 1375, 1377, 1380, 1383, 1385, 1388, 1390, 1391, 1393, 1394, 1398, 1399, 1401, 1402, 1404, 1405, 1408, 1410, 1412, 1413, 1416, 1418, 1419, 1420, 1422, 1425, 1429, 1430, 1432, 1433, 1435, 1436, 1440, 1442, 1443, 1448, 1449, 1450, 1453, 1454, 1456, 1457, 1462, 1463, 1464, 1467, 1470, 1471, 1473, 1475, 1478, 1479, 1480, 1483, 1484, 1485, 1488, 1489, 1490, 1492, 1494, 1496, 1500, 1501, 1502, 1503, 1506, 1511, 1515, 1517, 1519, 1521, 1522, 1523, 1527, 1528, 1529, 1531, 1532, 1533, 1538, 1541, 1543, 1544, 1545, 1547, 1548, 1549, 1550, 1551, 1554, 1559, 1561, 1563, 1564, 1566, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1582, 1584, 1586, 1587, 1590, 1593, 1595, 1598, 1600, 1601, 1602, 1604, 1606, 1608, 1610, 1611, 1612, 1613, 1618, 1620, 1622, 1623, 1626, 1629, 1631, 1633, 1634, 1638, 1639, 1641, 1643, 1645, 1648, 1651, 1652, 1653, 1654, 1658, 1659, 1661, 1662, 1665, 1666, 1668, 1669, 1670, 1674, 1675, 1679, 1681, 1683, 1684, 1685, 1687, 1691, 1693, 1696, 1697, 1698, 1699, 1700, 1705, 1708, 1709, 1710, 1711, 1713, 1718, 1719, 1720, 1721, 1723, 1727, 1729, 1730, 1731, 1732, 1733, 1739, 1740, 1743, 1744, 1747, 1750, 1751, 1753, 1754, 1755, 1756, 1758, 1760, 1764, 1767, 1768, 1769, 1770, 1773, 1776, 1779, 1782, 1783, 1784, 1786, 1788, 1792, 1793, 1794, 1795, 1797, 1801, 1802, 1803, 1804, 1808, 1809, 1813, 1816, 1817, 1820, 1822, 1823, 1824, 1826, 1827, 1829, 1832, 1835, 1837, 1838, 1841, 1842, 1846, 1847, 1850, 1851, 1852, 1853, 1854, 1859, 1862, 1866, 1867, 1868, 1869, 1870, 1875, 1879, 1880, 1881, 1882, 1883, 1885, 1888, 1889, 1890, 1891, 1895, 1899, 1900, 1902, 1904, 1905, 1906, 1909, 1911, 1914, 1916, 1917, 1919, 1920, 1922, 1923, 1926, 1929, 1930, 1933, 1935, 1938, 1939, 1940, 1941, 1945, 1946, 1947, 1949, 1952, 1953, 1956, 1957, 1961, 1964, 1966, 1967, 1968, 1969, 1971, 1972, 1973, 1977, 1980, 1981, 1983, 1986, 1988, 1991, 1993, 1997, 1998, 2000, 2002, 2003, 2004, 2006, 2009, 2013, 2014, 2015, 2017, 2020, 2021, 2023, 2025, 2027, 2029, 2030, 2032, 2033, 2038, 2039, 2041, 2042, 2044, 2045, 2047, 2048, 2052, 2055, 2057, 2060, 2061, 2062, 2064, 2067, 2069, 2073, 2074, 2075, 2077, 2081, 2082, 2083, 2084, 2086, 2088, 2091, 2092, 2096, 2097, 2099, 2101, 2103, 2106, 2107, 2108, 2110, 2113, 2115, 2118, 2119, 2120, 2123, 2125, 2126, 2130, 2131, 2132, 2135, 2137, 2138, 2141, 2142, 2143, 2146, 2148, 2149, 2151, 2152, 2153, 2156, 2165, 2166, 2167, 2168, 2169, 2170, 2173, 2175, 2177, 2179, 2180, 2182, 2184, 2186, 2187, 2189, 2191, 2194, 2196, 2197, 2200, 2202, 2204, 2205, 2208, 2209, 2210, 2211, 2213, 2218, 2220, 2221, 2222, 2223, 2224, 2231, 2232, 2236, 2237, 2238, 2239, 2243, 2244, 2246, 2249, 2250, 2251, 2253, 2258, 2259, 2262, 2263, 2264, 2266, 2268, 2271, 2273, 2275, 2276, 2278, 2282, 2283, 2285, 2287, 2288, 2289, 2294, 2297, 2298, 2299, 2301, 2303, 2304, 2307, 2308, 2312, 2313, 2315, 2316, 2317, 2318, 2320, 2325, 2327, 2328, 2329, 2331, 2332, 2333, 2336, 2340, 2341, 2342, 2344, 2348, 2350, 2352, 2356, 2358, 2359, 2362, 2363, 2364, 2366, 2369, 2372, 2373, 2375, 2377, 2379, 2380, 2382, 2384, 2386, 2390, 2392, 2393, 2394, 2395, 2396, 2400, 2402, 2403, 2406, 2409, 2411, 2412, 2413, 2414, 2415, 2417, 2420, 2424, 2428, 2429, 2430, 2432, 2434, 2438, 2440, 2442, 2443, 2445, 2446, 2449, 2450, 2452, 2453, 2456, 2457, 2460, 2462, 2464, 2467, 2469, 2471, 2472, 2473, 2477, 2480, 2482, 2483, 2485, 2486, 2489, 2493, 2494, 2495, 2496, 2497, 2499, 2502, 2505, 2506, 2508, 2509, 2510, 2515, 2516, 2517, 2520, 2522, 2524, 2527, 2528, 2531, 2532, 2537, 2538, 2540, 2542, 2543, 2544, 2547, 2549, 2550, 2551, 2552, 2556, 2557, 2558, 2562, 2564, 2565, 2569, 2571, 2573, 2575, 2577, 2578, 2582, 2583, 2588, 2589, 2590, 2591, 2592, 2593, 2595, 2597, 2599, 2602, 2604, 2607, 2609, 2611, 2613, 2615, 2617, 2618, 2619, 2622, 2623, 2627, 2632, 2633, 2634, 2636, 2637, 2638, 2641, 2643, 2646, 2649, 2650, 2651, 2653, 2654, 2656, 2658, 2660, 2662, 2665, 2668, 2669, 2670, 2673, 2674, 2676, 2677, 2678, 2680, 2686, 2687, 2689, 2690, 2696, 2697, 2698, 2699, 2700, 2702, 2704, 2707, 2708, 2711, 2712, 2714, 2715, 2718, 2720, 2723, 2728, 2729, 2731, 2732, 2734, 2735, 2736, 2737, 2738, 2740, 2742, 2746, 2748, 2749, 2751, 2754, 2756, 2757, 2762, 2763, 2765, 2767, 2768, 2771, 2774, 2775, 2777, 2779, 2780, 2783, 2787, 2790, 2791, 2792, 2793, 2794, 2797, 2798, 2801, 2804, 2806, 2807, 2810, 2811, 2813, 2814, 2816, 2817, 2821, 2825, 2826, 2827, 2828, 2830, 2833, 2836, 2837, 2838, 2840, 2841, 2844, 2846, 2847, 2848, 2849, 2854, 2858, 2859, 2860, 2862, 2865, 2867, 2868, 2870, 2872, 2873, 2877, 2879, 2880, 2881, 2883, 2885, 2886, 2890, 2892, 2893, 2894, 2895, 2896, 2900, 2905, 2910, 2912, 2913, 2914, 2915, 2917, 2919, 2920, 2921, 2925, 2927, 2928, 2931, 2933, 2937, 2938, 2939, 2940, 2941, 2944, 2948, 2949, 2950, 2954, 2955, 2956, 2960, 2961, 2962, 2966, 2968, 2969, 2970, 2973, 2974, 2976, 2977, 2978, 2980, 2984, 2987, 2988, 2989, 2993, 2996, 2997, 2998, 3001, 3002, 3003, 3004, 3010, 3011, 3014, 3015, 3019, 3020, 3022, 3023, 3024, 3025, 3029, 3030, 3032, 3033, 3037, 3038, 3040, 3042, 3043, 3044, 3048, 3050, 3053, 3056, 3057, 3059, 3060, 3061, 3066, 3069, 3070, 3071, 3075, 3076, 3077, 3079, 3081, 3082, 3086, 3087, 3088, 3093, 3094, 3095, 3097, 3098, 3100, 3101, 3105, 3107, 3108, 3111, 3112, 3113, 3114, 3117, 3121, 3123, 3124, 3127, 3128, 3131, 3132, 3136, 3139, 3140, 3142, 3143, 3144, 3145, 3147, 3149, 3151, 3153, 3157, 3158, 3160, 3162, 3165, 3166, 3168, 3172, 3175, 3176, 3178, 3179, 3181, 3182, 3183, 3184, 3186, 3190, 3193, 3194, 3195, 3197, 3198, 3200, 3205, 3206, 3210, 3211, 3212, 3215, 3216, 3217, 3218, 3220, 3223, 3226, 3229, 3230, 3232, 3233, 3235, 3237, 3240, 3243, 3244, 3246, 3248, 3249, 3252, 3253, 3254, 3255, 3257, 3263, 3265, 3266, 3268, 3269, 3273, 3275, 3276, 3277, 3279, 3280, 3282, 3285, 3290, 3291, 3292, 3293, 3296, 3298, 3303, 3305, 3306, 3307, 3308, 3309, 3312, 3314, 3315, 3316, 3319, 3320, 3325, 3327, 3329, 3330, 3332, 3333, 3336, 3342, 3343, 3345, 3346, 3347, 3348, 3352, 3353, 3357, 3358, 3359, 3360, 3361, 3362, 3364, 3366, 3367, 3374, 3375, 3379, 3380, 3381, 3383, 3384, 3387, 3388, 3390, 3391, 3393, 3398, 3399, 3400, 3401, 3405, 3406, 3409, 3413, 3414, 3415, 3416, 3419, 3420, 3421, 3422, 3423, 3428, 3431, 3433, 3434, 3440, 3441, 3442, 3443, 3446, 3448, 3451, 3452, 3453, 3454, 3456, 3457, 3460, 3462, 3465, 3467, 3470, 3474, 3475, 3476, 3477, 3478, 3480, 3482, 3484, 3487, 3489, 3491, 3493, 3494, 3495, 3496, 3497, 3498, 3504, 3507, 3509, 3512, 3514, 3515, 3519, 3520, 3522, 3525, 3526, 3527, 3529, 3530, 3531, 3534, 3537, 3539, 3540, 3541, 3543, 3544, 3550, 3551, 3552, 3553, 3555, 3556, 3558, 3561, 3564, 3565, 3567, 3569, 3573, 3575, 3576, 3577, 3580, 3582, 3584, 3586, 3588, 3590, 3593, 3594, 3595, 3598, 3600, 3603, 3604, 3608, 3609, 3611, 3613, 3615, 3618, 3619, 3622, 3623, 3625, 3626, 3629, 3631, 3632, 3635, 3636, 3637, 3640, 3642, 3643, 3644, 3648, 3651, 3653, 3654, 3655, 3656, 3660, 3662, 3663, 3665, 3667, 3669, 3672, 3673, 3677, 3678, 3681, 3682, 3686, 3688, 3689, 3691, 3693, 3695, 3696, 3698, 3699, 3704, 3706, 3707, 3711, 3715, 3716, 3717, 3718, 3719, 3722, 3724, 3727, 3728, 3729, 3730, 3736, 3737, 3738, 3740, 3741, 3742, 3748, 3749, 3750, 3752, 3753, 3754, 3756, 3757, 3758, 3759, 3760, 3764, 3770, 3772, 3773, 3775, 3776, 3777, 3781, 3783, 3785, 3786, 3789, 3791, 3792, 3795, 3797, 3798, 3799, 3803, 3806, 3808, 3809, 3810, 3813, 3815, 3816, 3818, 3819, 3820, 3822, 3824, 3828, 3832, 3833, 3836, 3837, 3838, 3841, 3843, 3844, 3846, 3848, 3850, 3853, 3857, 3858, 3859, 3860, 3863, 3865, 3867, 3868, 3871, 3873, 3874, 3876, 3877, 3879, 3884, 3885, 3887, 3888, 3889, 3893, 3894, 3897, 3899, 3900, 3901, 3904, 3907, 3910, 3911, 3913, 3914, 3915, 3916, 3917, 3922, 3926, 3928, 3929, 3931, 3934, 3935, 3937, 3939, 3941, 3942, 3944, 3947, 3950, 3951, 3953, 3955, 3956, 3959, 3961, 3962, 3963, 3964, 3968, 3971, 3975, 3979, 3980, 3981, 3982, 3983, 3984, 3986, 3988, 3989, 3991, 3995, 3997, 3998, 4000, 4001, 4002, 4004, 4008, 4009, 4014, 4015, 4017, 4018, 4020, 4022, 4024, 4026, 4028, 4029, 4033, 4034, 4035, 4037, 4039, 4043, 4045, 4046, 4047, 4050, 4051, 4054, 4056, 4059, 4061, 4065, 4066, 4067, 4069, 4070, 4071, 4073, 4076, 4079, 4080, 4082, 4083, 4086, 4089, 4090, 4093, 4094, 4095, 4098, 4101, 4103, 4104, 4106, 4107, 4108, 4110, 4115, 4117, 4119, 4121, 4124, 4126, 4127, 4128, 4130, 4131, 4133, 4134, 4136, 4140, 4142, 4144, 4147, 4149, 4150, 4153, 4154, 4158, 4160, 4162, 4163, 4165, 4167, 4168, 4169, 4173, 4175, 4176, 4181, 4183, 4184, 4186, 4187, 4188, 4190, 4192, 4193, 4194, 4196, 4205, 4206, 4208, 4209, 4210, 4211, 4212, 4213, 4216, 4218, 4219, 4220, 4224, 4227, 4228, 4229, 4231, 4235, 4238, 4240, 4242, 4243, 4246, 4247, 4249, 4250, 4253, 4254, 4255, 4257, 4260, 4262, 4264, 4265, 4267, 4271, 4272, 4274, 4276, 4278, 4281, 4283, 4284, 4287, 4290, 4292, 4293, 4295, 4298, 4300, 4301, 4302, 4303, 4306, 4308, 4309, 4313, 4314, 4315, 4317, 4322, 4324, 4328, 4329, 4330, 4331, 4334, 4336, 4337, 4339, 4340, 4341, 4345, 4346, 4349, 4353, 4354, 4355, 4356, 4357, 4358, 4360, 4361, 4364, 4369, 4373, 4375, 4376, 4378, 4379, 4382, 4383, 4384, 4386, 4388, 4390, 4393, 4394, 4395, 4397, 4399, 4400, 4404, 4409, 4410, 4412, 4414, 4415, 4416, 4417, 4421, 4423, 4426, 4427, 4428, 4432, 4433, 4435, 4438, 4439, 4442, 4443, 4444, 4445, 4446, 4447, 4453, 4455, 4456, 4458, 4460, 4461, 4465, 4467, 4468, 4471, 4474, 4475, 4476, 4478, 4481, 4483, 4484, 4487, 4489, 4491, 4493, 4494, 4496, 4497, 4501, 4502, 4504, 4505, 4506, 4507, 4515, 4516, 4518, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4529, 4534, 4538, 4539, 4541, 4542, 4545, 4547, 4548, 4550, 4553, 4554, 4556, 4558, 4563, 4564, 4568, 4569, 4570, 4571, 4573, 4574, 4575, 4577, 4578, 4580, 4584, 4590, 4591, 4592, 4594, 4595, 4596, 4597, 4598, 4601, 4603, 4606, 4609, 4611, 4612, 4613, 4616, 4619, 4621, 4622, 4623, 4624, 4629, 4630, 4632, 4633, 4634, 4637, 4638, 4642, 4645, 4647, 4649, 4651, 4653, 4655, 4657, 4658, 4661, 4664, 4665, 4667, 4669, 4671, 4673, 4677, 4678, 4679, 4682, 4683, 4686, 4687, 4690, 4691, 4692, 4696, 4697, 4699, 4701, 4702, 4704, 4705, 4709, 4711, 4712, 4715, 4717, 4719, 4722, 4724, 4726, 4727, 4730, 4732, 4735, 4736, 4737, 4739, 4740, 4744, 4747, 4748, 4749, 4750, 4752, 4753, 4754, 4756, 4759, 4760, 4764, 4765, 4768, 4769, 4770, 4774, 4779, 4780, 4781, 4782, 4786, 4787, 4788, 4790, 4791, 4793, 4796, 4798, 4800, 4802, 4803, 4807, 4809, 4811, 4814, 4816, 4818, 4820, 4821, 4822, 4826, 4827, 4830, 4832, 4833, 4834, 4838, 4840, 4841, 4844, 4845, 4846, 4849, 4851, 4853, 4854, 4855, 4856, 4860, 4861, 4864, 4866, 4867, 4871, 4872, 4875, 4876, 4878, 4880, 4881, 4886, 4889, 4890, 4891, 4893, 4895, 4896, 4897, 4904, 4905, 4906, 4907, 4908, 4910, 4915, 4917, 4918, 4919, 4921, 4922, 4923, 4924, 4926, 4928, 4932, 4935, 4936, 4937, 4940, 4942, 4947, 4949, 4951, 4952, 4954, 4955, 4958, 4961, 4962, 4963, 4966, 4967, 4968, 4969, 4971, 4972, 4974, 4976, 4982, 4983, 4984, 4985, 4986, 4989, 4993, 4995, 4996, 4997, 4999, 5001, 5004, 5006, 5007, 5009, 5012, 5015, 5018, 5020, 5022, 5024, 5025, 5027, 5028, 5030, 5032, 5034, 5038, 5039, 5043, 5047, 5048, 5049, 5050, 5051, 5053, 5056, 5058, 5059, 5060, 5062, 5066, 5067, 5070, 5072, 5074, 5075, 5077, 5078, 5079, 5082, 5083, 5085, 5089, 5090, 5091, 5094, 5097, 5098, 5100, 5101, 5102, 5103, 5104, 5105, 5112, 5113, 5114, 5115, 5116, 5118, 5124, 5125, 5126, 5127, 5130, 5132, 5137, 5138, 5139, 5140, 5141, 5144, 5148, 5150, 5151, 5152, 5156, 5158, 5162, 5163, 5166, 5168, 5169, 5170, 5172, 5175, 5177, 5181, 5182, 5183, 5184, 5185, 5187, 5192, 5194, 5195, 5196, 5197, 5201, 5203, 5204, 5205, 5209, 5211, 5214, 5216, 5218, 5219, 5222, 5223, 5226, 5227, 5229, 5231, 5232, 5234, 5235, 5236, 5238, 5239, 5244, 5245, 5248, 5252, 5253, 5254, 5257, 5259, 5261, 5264, 5265, 5266, 5270, 5272, 5274, 5275, 5278, 5279, 5281, 5282, 5284, 5288, 5290, 5291, 5294, 5297, 5298, 5299, 5302, 5303, 5305, 5306, 5307, 5312, 5314, 5315, 5316, 5317, 5319, 5320, 5321, 5323, 5328, 5329, 5333, 5334, 5335, 5337, 5340, 5344, 5347, 5348, 5349, 5351, 5354, 5355, 5356, 5358, 5361, 5363, 5365, 5366, 5367, 5368, 5369, 5370, 5377, 5378, 5380, 5382, 5383, 5384, 5392, 5393, 5394, 5396, 5398, 5399, 5400, 5401, 5405, 5406, 5408, 5411, 5412, 5415, 5416, 5420, 5421, 5423, 5426, 5427, 5428, 5430, 5433, 5434, 5438, 5440, 5441, 5442, 5443, 5444, 5448, 5449, 5451, 5455, 5457, 5459, 5461, 5462, 5465, 5466, 5470, 5471, 5472, 5474, 5475, 5477, 5480, 5481, 5486, 5487, 5491, 5492, 5494, 5495, 5498, 5501, 5502, 5503, 5504, 5507, 5508, 5509, 5511, 5512, 5514, 5516, 5520, 5521, 5524, 5525, 5529, 5530, 5536, 5537, 5538, 5540, 5542, 5543, 5544, 5545, 5547, 5548, 5549, 5555, 5556, 5557, 5559, 5564, 5565, 5567, 5568, 5569, 5573, 5575, 5578, 5579, 5582, 5583, 5584, 5585, 5586, 5590, 5592, 5593, 5596, 5599, 5602, 5603, 5605, 5608, 5610, 5611, 5612, 5613, 5617, 5618, 5621, 5623, 5624, 5625, 5629, 5631, 5633, 5634, 5635, 5639, 5640, 5641, 5642, 5645, 5648, 5649, 5653, 5654, 5656, 5659, 5662, 5663, 5664, 5666, 5667, 5671, 5673, 5674, 5678, 5679, 5682, 5683, 5685, 5686, 5689, 5690, 5692, 5693, 5694, 5698, 5702, 5703, 5704, 5706, 5710, 5711, 5713, 5714, 5717, 5719, 5720, 5721, 5725, 5729, 5730, 5732, 5733, 5734, 5737, 5738, 5739, 5740, 5745, 5747, 5752, 5753, 5754, 5755, 5757, 5758, 5760, 5762, 5763, 5765, 5766, 5771, 5775, 5777, 5780, 5781, 5782, 5783, 5784, 5787, 5788, 5789, 5791, 5794, 5796, 5798, 5800, 5803, 5804, 5805, 5810, 5811, 5812, 5815, 5816, 5819, 5822, 5824, 5826, 5828, 5829, 5831, 5834, 5836, 5837, 5838, 5840, 5843, 5846, 5847, 5848, 5849, 5850, 5855, 5857, 5858, 5863, 5865, 5866, 5867, 5869, 5871, 5873, 5875, 5876, 5877, 5881, 5882, 5883, 5885, 5890, 5891, 5892, 5893, 5894, 5896, 5901, 5903, 5904, 5905, 5906, 5909, 5910, 5914, 5917, 5921, 5922, 5924, 5925, 5926, 5931, 5932, 5934, 5936, 5937, 5939, 5940, 5943, 5946, 5948, 5950, 5951, 5953, 5957, 5958, 5959, 5961, 5962, 5965, 5967, 5969, 5972, 5973, 5974, 5980, 5981, 5983, 5984, 5985, 5987, 5988, 5989, 5991, 5994, 5997, 5999, 6001, 6002, 6003, 6004, 6007, 6008, 6012, 6015, 6017, 6018, 6021, 6022, 6024, 6025, 6027, 6032, 6033, 6034, 6037, 6038, 6041, 6045, 6046, 6047, 6050, 6051, 6052, 6054, 6057, 6059, 6060, 6063, 6064, 6067, 6068, 6071, 6073, 6075, 6077, 6078, 6079, 6083, 6084, 6086, 6087, 6090, 6092, 6093, 6096, 6100, 6101, 6102, 6103, 6107, 6110, 6114, 6116, 6117, 6118, 6119, 6123, 6124, 6128, 6129, 6130, 6131, 6134, 6136, 6137, 6138, 6140, 6141, 6145, 6147, 6149, 6150, 6152, 6153, 6156, 6159, 6160, 6162, 6164, 6165, 6168, 6169, 6175, 6176, 6178, 6179, 6182, 6185, 6188, 6189, 6190, 6191, 6193, 6195, 6196, 6197, 6200, 6202, 6205, 6206, 6207, 6210, 6213, 6215, 6216, 6218, 6219, 6220, 6221, 6226, 6228, 6229, 6231, 6234, 6235, 6236, 6243, 6246, 6247, 6248, 6249, 6251, 6253, 6254, 6255, 6257, 6262, 6263, 6265, 6268, 6269, 6271, 6272, 6274, 6276, 6277, 6280, 6282, 6284, 6285, 6289, 6290, 6291, 6292, 6295, 6296, 6300, 6301, 6302, 6305, 6306, 6309, 6315, 6316, 6318, 6319, 6322, 6323, 6324, 6325, 6329, 6330, 6331, 6333, 6338, 6339, 6340, 6342, 6343, 6344, 6348, 6350, 6353, 6354, 6358, 6359, 6361, 6366, 6367, 6368, 6369, 6371, 6376, 6377, 6378, 6379, 6381, 6383, 6385, 6387, 6388, 6389, 6393, 6394, 6396, 6397, 6400, 6401, 6403, 6407, 6409, 6410, 6413, 6415, 6417, 6419, 6421, 6422, 6423, 6424, 6426, 6428, 6432, 6433, 6435, 6436, 6437, 6438, 6444, 6446, 6450, 6451, 6452, 6453, 6456, 6457, 6459, 6461, 6465, 6466, 6468, 6470, 6474, 6475, 6477, 6479, 6483, 6484, 6486, 6489, 6490, 6491, 6492, 6493, 6494, 6498, 6501, 6503, 6505, 6506, 6508, 6512, 6513, 6515, 6516, 6517, 6520, 6521, 6523, 6524, 6528, 6529, 6530, 6531, 6536, 6537, 6542, 6543, 6544, 6545, 6547, 6551, 6552, 6554, 6555, 6558, 6559, 6560, 6564, 6567, 6569, 6572, 6574, 6575, 6578, 6579, 6582, 6584, 6586, 6587, 6590, 6592, 6593, 6594, 6595, 6599, 6600, 6601, 6606, 6607, 6609, 6610, 6613, 6614, 6616, 6617, 6620, 6621, 6624, 6625, 6627, 6631, 6632, 6633, 6636, 6637, 6641, 6642, 6644, 6645, 6649, 6651, 6654, 6655, 6656, 6657, 6662, 6663, 6664, 6667, 6670, 6671, 6674, 6677, 6678, 6679, 6681, 6683, 6684, 6686, 6691, 6692, 6694, 6695, 6697, 6698, 6699, 6704, 6705, 6707, 6708, 6711, 6716, 6717, 6718, 6719, 6720, 6723, 6727, 6728, 6729, 6730, 6733, 6736, 6737, 6738, 6739, 6742, 6745, 6746, 6747, 6749, 6751, 6754, 6758, 6760, 6761, 6764, 6765, 6766, 6768, 6770, 6771, 6773, 6777, 6778, 6781, 6785, 6786, 6788, 6789, 6790, 6794, 6795, 6797, 6799, 6801, 6802, 6804, 6807, 6808, 6810, 6812, 6813, 6817, 6818, 6821, 6822, 6823, 6826, 6828, 6829, 6831, 6833, 6834, 6838, 6842, 6843, 6844, 6845, 6849, 6850, 6855, 6856, 6857, 6860, 6861, 6863, 6867, 6868, 6869, 6870, 6871, 6874, 6876, 6879, 6881, 6883, 6885, 6887, 6889, 6891, 6893, 6895, 6896, 6897, 6903, 6905, 6906, 6907, 6908, 6911, 6912, 6916, 6917, 6919, 6920, 6922, 6925, 6929, 6930, 6931, 6933, 6934, 6938, 6939, 6940, 6942, 6945, 6946, 6948, 6949, 6950, 6952, 6957, 6959, 6962, 6966, 6967, 6968, 6969, 6971, 6972, 6973, 6977, 6978, 6979, 6982, 6984, 6985, 6987, 6990, 6994, 6995, 6996, 6998, 6999, 7002, 7003, 7007, 7009, 7010, 7011, 7012, 7013, 7019, 7020, 7021, 7022, 7023, 7025, 7029, 7033, 7034, 7035, 7036, 7038, 7039, 7045, 7046, 7049, 7051, 7052, 7053, 7057, 7060, 7061, 7064, 7065, 7066, 7068, 7072, 7074, 7075, 7077, 7079, 7081, 7082, 7086, 7087, 7088, 7091, 7093, 7094, 7095, 7097, 7099, 7102, 7104, 7107, 7110, 7111, 7113, 7115, 7116, 7118, 7119, 7121, 7122, 7127, 7130, 7131, 7134, 7135, 7136, 7139, 7140, 7143, 7147, 7148, 7149, 7150, 7152, 7153, 7158, 7160, 7161, 7163, 7164, 7165, 7166, 7170, 7172, 7175, 7178, 7179, 7180, 7181, 7184, 7187, 7188, 7189, 7190, 7191, 7196, 7197, 7200, 7201, 7204, 7205, 7206, 7208, 7212, 7214, 7217, 7218, 7220, 7223, 7224, 7229, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7243, 7246, 7247, 7251, 7252, 7253, 7256, 7258, 7259, 7261, 7262, 7263, 7264, 7269, 7271, 7272, 7273, 7274, 7275, 7279, 7282, 7284, 7287, 7288, 7292, 7293, 7295, 7296, 7297, 7299, 7302, 7304, 7306, 7308, 7309, 7312, 7313, 7315, 7319, 7321, 7322, 7325, 7327, 7328, 7329, 7332, 7337, 7339, 7340, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7350, 7356, 7357, 7359, 7361, 7364, 7366, 7370, 7372, 7374, 7375, 7377, 7379, 7380, 7382, 7383, 7384, 7386, 7387, 7392, 7395, 7396, 7398, 7399, 7400, 7405, 7408, 7409, 7411, 7414, 7415, 7416, 7419, 7421, 7423, 7426, 7427, 7429, 7431, 7433, 7434, 7435, 7438, 7440, 7443, 7445, 7448, 7449, 7450, 7453, 7455, 7457, 7461, 7462, 7463, 7464, 7468, 7472, 7473, 7474, 7475, 7479, 7480, 7481, 7482, 7484, 7487, 7489, 7491, 7492, 7495, 7496, 7499, 7501, 7502, 7505, 7507, 7509, 7511, 7513, 7515, 7516, 7517, 7518, 7522, 7524, 7525, 7527, 7530, 7533, 7535, 7537, 7540, 7541, 7542, 7543, 7547, 7548, 7549, 7551, 7553, 7554, 7556, 7560, 7563, 7565, 7569, 7570, 7571, 7573, 7575, 7578, 7579, 7582, 7583, 7586, 7587, 7590, 7591, 7592, 7594, 7597, 7601, 7602, 7603, 7604, 7607, 7609, 7610, 7613, 7614, 7616, 7619, 7620, 7624, 7625, 7626, 7628, 7630]\n",
            "(3816,)\n",
            "(7632, 112, 112, 1)\n",
            "\n",
            "Remaining 50% length: 3816\n",
            "view_comb shape:  (64, 3)\n",
            "635 / 636\n",
            "New 50% length:  1272\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7338df900fb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m                                                                                                 \u001b[0mcur_triplets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_triplets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                                                                                                 \u001b[0mnum_triplets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_triplets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                                                                                                 view_ids=views)\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-9e1731360812>\u001b[0m in \u001b[0;36mregenerate_set\u001b[0;34m(fold_id, train_set, cur_triplets, num_triplets, view_ids)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAnchor_train_retain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mAnchor_train_all_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAnchor_train_retain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m       \u001b[0mAnchor_train_all_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAnchor_train_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m       \u001b[0mPositive_train_all_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPositive_train_retain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0mPositive_train_all_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPositive_train_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeKM_4qc14ZG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "94b9af9f-87d6-42d2-b515-08992d715603"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgf7GICoAECC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_building/df_train_1.csv\")\n",
        "df_test = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_building/df_test_1.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8ordFYTgxAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_history.to_csv(\"drive/My Drive/MultiviewAccuracy_10CV/Lun_building/df_log_1.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvA8qh5CVmwj",
        "colab_type": "code",
        "outputId": "d8c5c28a-0e4d-402c-9159-f410f61bfc65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "## Read saved accuracy log\n",
        "pd.read_csv(\"drive/My Drive/MultiviewAccuracy_10CV/Lun_building/df_log_1.csv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_acc</th>\n",
              "      <th>val_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.236780</td>\n",
              "      <td>0.565798</td>\n",
              "      <td>0.479953</td>\n",
              "      <td>0.483333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.246455</td>\n",
              "      <td>0.531563</td>\n",
              "      <td>0.488994</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.242638</td>\n",
              "      <td>0.560275</td>\n",
              "      <td>0.490959</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.240937</td>\n",
              "      <td>0.545839</td>\n",
              "      <td>0.500393</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.239942</td>\n",
              "      <td>0.549002</td>\n",
              "      <td>0.493711</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  train_loss  val_loss  train_acc   val_acc\n",
              "0           0    0.236780  0.565798   0.479953  0.483333\n",
              "1           1    0.246455  0.531563   0.488994  0.466667\n",
              "2           2    0.242638  0.560275   0.490959  0.466667\n",
              "3           3    0.240937  0.545839   0.500393  0.416667\n",
              "4           4    0.239942  0.549002   0.493711  0.450000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vajPm9TvAc7b",
        "colab_type": "code",
        "outputId": "c0311eff-2d07-47fd-a970-98a8e50eba5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Check reversed duplicates\n",
        "df_tmp = df_train.copy()\n",
        "print(\"df_tmp:\", df_tmp.shape)\n",
        "df_reversed = df_tmp.loc[:,[\"query\",\"neg\",\"pos\"]].copy()\n",
        "df_reversed = df_reversed.rename(columns={\"pos\":\"neg\", \"neg\":\"pos\"})\n",
        "df_concat = pd.concat([df_tmp, df_reversed])\n",
        "df_concat = df_concat.reset_index(drop=True)\n",
        "print(int(df_concat.shape[0]))\n",
        "df_concat = df_concat.drop_duplicates(keep='first')\n",
        "\n",
        "print(int(df_concat.shape[0])-int(df_tmp.shape[0]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_tmp: (246, 3)\n",
            "492\n",
            "246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0DVbPHgVNNo",
        "colab_type": "code",
        "outputId": "efee38ac-a11f-4687-a2e3-c6d3f5ee6ba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        }
      },
      "source": [
        "id = 0\n",
        "data = Anchor_train_all[id][:,:,0]\n",
        "# plt.imshow(data, cmap=\"gray\")\n",
        "plt.imshow(data)\n",
        "plt.show()\n",
        "data = Positive_train_all[id][:,:,0]\n",
        "plt.imshow(data)\n",
        "plt.show()\n",
        "data = Negative_train_all[id][:,:,0]\n",
        "plt.imshow(data)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUAkckoCIpH7/wUUhab733aMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_bQQ7wg_X_H",
        "colab_type": "code",
        "outputId": "93556e81-77f5-4b73-a3b9-feedbff3cae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Evaluate on test data\n",
        "# Anchor_test = input_data2_test\n",
        "# Positive_test = input_data1_test\n",
        "# Negative_test = input_data3_test\n",
        "y_true = np.zeros((Anchor_test.shape[0],1))\n",
        "\n",
        "pred = model.predict(x=[Anchor_test,Positive_test,Negative_test],verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "105/105 [==============================] - 1s 7ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DftDxBrkC2KN",
        "colab_type": "code",
        "outputId": "d1379b38-1ece-45b6-c638-fcb1241b3970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Evaluate on training data\n",
        "# Anchor = input_data2_train\n",
        "# Positive = input_data1_train\n",
        "# Negative = input_data3_train\n",
        "y_true = np.zeros((Anchor.shape[0],1))\n",
        "\n",
        "pred = model.predict(x=[Anchor_train,Positive_train,Negative_train],verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "702/702 [==============================] - 19s 28ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA8DSVCQBB3O",
        "colab_type": "code",
        "outputId": "fa5b9fc7-ddf5-46e0-d796-be6e59d31ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Split into anchor, a, and b sets\n",
        "total_lenght = pred.shape[1]\n",
        "pred_anchor = pred[:,0:int(total_lenght*1/3)]\n",
        "pred_a = pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
        "pred_b = pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
        "pred_a.shape\n",
        "\n",
        "y_pred = []\n",
        "for i in range(pred.shape[0]):\n",
        "  dist_pos = distance.euclidean(pred_anchor[i], pred_a[i])\n",
        "  dist_neg = distance.euclidean(pred_anchor[i], pred_b[i])\n",
        "  print(\"Triplet\", i, \":\")\n",
        "  print(\"dist_pos\", dist_pos)\n",
        "  print(\"dist_neg\", dist_neg)\n",
        "  print(\"---------------------------------\")\n",
        "  if dist_pos < dist_neg:\n",
        "    y_pred.append(0)\n",
        "  else:\n",
        "    y_pred.append(1)   \n",
        "    \n",
        "\n",
        "print(accuracy_score(y_true, y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c418ec5b27ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_lenght\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred_anchor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lenght\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "367P5cEgaIwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_pickle(\"drive/My Drive/10CV_df/Lun_building/df_test_1.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNPPpNV_O3vj",
        "colab_type": "text"
      },
      "source": [
        "# Generate empeddings for each object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn5CdIQqPCd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## This model takes one object and outputs a embedding.\n",
        "model=Model(inputs=model.get_layer(\"model_3\").get_input_at(0),outputs=model.get_layer(\"model_3\").get_output_at(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNVxjBt3Tg4f",
        "colab_type": "code",
        "outputId": "7cd09f63-016f-4911-adc1-919d26d974b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pred_cat1 = model.predict(x=Anchor,verbose=1)\n",
        "pred_cat2 = model.predict(x=Positive,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "702/702 [==============================] - 6s 9ms/step\n",
            "702/702 [==============================] - 6s 9ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45OfMZBwTlvP",
        "colab_type": "code",
        "outputId": "ea334171-546a-425f-bd27-b8155999a657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "query = pred_cat1[0]\n",
        "df_dist = pd.DataFrame()\n",
        "for i in range(pred_cat2.shape[0]):\n",
        "  dist = distance.euclidean(query, pred_cat2[i])\n",
        "  df_dist = df_dist.append(pd.DataFrame([dist]))\n",
        "  print(\"Sample\", i, \":\")\n",
        "  print(\"dist\", dist)\n",
        "df_dist = df_dist.reset_index(drop=\"True\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample 0 :\n",
            "dist 11.942307472229004\n",
            "Sample 1 :\n",
            "dist 9.089716911315918\n",
            "Sample 2 :\n",
            "dist 5.0141496658325195\n",
            "Sample 3 :\n",
            "dist 11.207891464233398\n",
            "Sample 4 :\n",
            "dist 11.485407829284668\n",
            "Sample 5 :\n",
            "dist 11.594229698181152\n",
            "Sample 6 :\n",
            "dist 10.215923309326172\n",
            "Sample 7 :\n",
            "dist 8.77670955657959\n",
            "Sample 8 :\n",
            "dist 11.207891464233398\n",
            "Sample 9 :\n",
            "dist 9.089716911315918\n",
            "Sample 10 :\n",
            "dist 9.005356788635254\n",
            "Sample 11 :\n",
            "dist 8.812785148620605\n",
            "Sample 12 :\n",
            "dist 10.139031410217285\n",
            "Sample 13 :\n",
            "dist 11.737712860107422\n",
            "Sample 14 :\n",
            "dist 12.618820190429688\n",
            "Sample 15 :\n",
            "dist 6.905245780944824\n",
            "Sample 16 :\n",
            "dist 10.506524085998535\n",
            "Sample 17 :\n",
            "dist 12.860376358032227\n",
            "Sample 18 :\n",
            "dist 3.7555344104766846\n",
            "Sample 19 :\n",
            "dist 15.70533275604248\n",
            "Sample 20 :\n",
            "dist 9.249247550964355\n",
            "Sample 21 :\n",
            "dist 9.054219245910645\n",
            "Sample 22 :\n",
            "dist 10.1978759765625\n",
            "Sample 23 :\n",
            "dist 11.168105125427246\n",
            "Sample 24 :\n",
            "dist 14.29787540435791\n",
            "Sample 25 :\n",
            "dist 9.054219245910645\n",
            "Sample 26 :\n",
            "dist 5.6047210693359375\n",
            "Sample 27 :\n",
            "dist 8.992192268371582\n",
            "Sample 28 :\n",
            "dist 4.894937038421631\n",
            "Sample 29 :\n",
            "dist 10.372018814086914\n",
            "Sample 30 :\n",
            "dist 9.856998443603516\n",
            "Sample 31 :\n",
            "dist 8.756172180175781\n",
            "Sample 32 :\n",
            "dist 5.826895713806152\n",
            "Sample 33 :\n",
            "dist 12.618820190429688\n",
            "Sample 34 :\n",
            "dist 4.514033794403076\n",
            "Sample 35 :\n",
            "dist 5.0141496658325195\n",
            "Sample 36 :\n",
            "dist 6.086319923400879\n",
            "Sample 37 :\n",
            "dist 10.27345085144043\n",
            "Sample 38 :\n",
            "dist 14.197386741638184\n",
            "Sample 39 :\n",
            "dist 10.816109657287598\n",
            "Sample 40 :\n",
            "dist 11.129412651062012\n",
            "Sample 41 :\n",
            "dist 10.550861358642578\n",
            "Sample 42 :\n",
            "dist 9.572949409484863\n",
            "Sample 43 :\n",
            "dist 9.089716911315918\n",
            "Sample 44 :\n",
            "dist 10.07865047454834\n",
            "Sample 45 :\n",
            "dist 11.207891464233398\n",
            "Sample 46 :\n",
            "dist 9.691022872924805\n",
            "Sample 47 :\n",
            "dist 4.894937038421631\n",
            "Sample 48 :\n",
            "dist 11.485407829284668\n",
            "Sample 49 :\n",
            "dist 9.054219245910645\n",
            "Sample 50 :\n",
            "dist 10.833091735839844\n",
            "Sample 51 :\n",
            "dist 15.017980575561523\n",
            "Sample 52 :\n",
            "dist 8.019840240478516\n",
            "Sample 53 :\n",
            "dist 10.395532608032227\n",
            "Sample 54 :\n",
            "dist 8.920267105102539\n",
            "Sample 55 :\n",
            "dist 10.09423542022705\n",
            "Sample 56 :\n",
            "dist 12.336888313293457\n",
            "Sample 57 :\n",
            "dist 9.864704132080078\n",
            "Sample 58 :\n",
            "dist 9.039032936096191\n",
            "Sample 59 :\n",
            "dist 9.089716911315918\n",
            "Sample 60 :\n",
            "dist 11.129412651062012\n",
            "Sample 61 :\n",
            "dist 9.10306453704834\n",
            "Sample 62 :\n",
            "dist 7.31467342376709\n",
            "Sample 63 :\n",
            "dist 10.186614990234375\n",
            "Sample 64 :\n",
            "dist 6.924028396606445\n",
            "Sample 65 :\n",
            "dist 9.054219245910645\n",
            "Sample 66 :\n",
            "dist 9.864704132080078\n",
            "Sample 67 :\n",
            "dist 8.019840240478516\n",
            "Sample 68 :\n",
            "dist 11.24622917175293\n",
            "Sample 69 :\n",
            "dist 7.15024995803833\n",
            "Sample 70 :\n",
            "dist 11.168105125427246\n",
            "Sample 71 :\n",
            "dist 6.714141368865967\n",
            "Sample 72 :\n",
            "dist 8.992192268371582\n",
            "Sample 73 :\n",
            "dist 6.086319923400879\n",
            "Sample 74 :\n",
            "dist 7.15024995803833\n",
            "Sample 75 :\n",
            "dist 10.27345085144043\n",
            "Sample 76 :\n",
            "dist 9.089716911315918\n",
            "Sample 77 :\n",
            "dist 9.691022872924805\n",
            "Sample 78 :\n",
            "dist 9.004803657531738\n",
            "Sample 79 :\n",
            "dist 8.019840240478516\n",
            "Sample 80 :\n",
            "dist 9.550772666931152\n",
            "Sample 81 :\n",
            "dist 11.94116497039795\n",
            "Sample 82 :\n",
            "dist 8.920267105102539\n",
            "Sample 83 :\n",
            "dist 10.75291919708252\n",
            "Sample 84 :\n",
            "dist 10.310395240783691\n",
            "Sample 85 :\n",
            "dist 11.129412651062012\n",
            "Sample 86 :\n",
            "dist 10.206989288330078\n",
            "Sample 87 :\n",
            "dist 6.924028396606445\n",
            "Sample 88 :\n",
            "dist 14.807355880737305\n",
            "Sample 89 :\n",
            "dist 13.467585563659668\n",
            "Sample 90 :\n",
            "dist 8.460783004760742\n",
            "Sample 91 :\n",
            "dist 11.485407829284668\n",
            "Sample 92 :\n",
            "dist 9.691022872924805\n",
            "Sample 93 :\n",
            "dist 10.00361156463623\n",
            "Sample 94 :\n",
            "dist 6.229179382324219\n",
            "Sample 95 :\n",
            "dist 9.213743209838867\n",
            "Sample 96 :\n",
            "dist 10.287613868713379\n",
            "Sample 97 :\n",
            "dist 6.714141368865967\n",
            "Sample 98 :\n",
            "dist 8.976008415222168\n",
            "Sample 99 :\n",
            "dist 9.089716911315918\n",
            "Sample 100 :\n",
            "dist 10.550861358642578\n",
            "Sample 101 :\n",
            "dist 11.594229698181152\n",
            "Sample 102 :\n",
            "dist 8.16422176361084\n",
            "Sample 103 :\n",
            "dist 3.7555344104766846\n",
            "Sample 104 :\n",
            "dist 6.714141368865967\n",
            "Sample 105 :\n",
            "dist 8.992192268371582\n",
            "Sample 106 :\n",
            "dist 9.691022872924805\n",
            "Sample 107 :\n",
            "dist 8.812785148620605\n",
            "Sample 108 :\n",
            "dist 11.737712860107422\n",
            "Sample 109 :\n",
            "dist 8.812785148620605\n",
            "Sample 110 :\n",
            "dist 9.214948654174805\n",
            "Sample 111 :\n",
            "dist 10.423500061035156\n",
            "Sample 112 :\n",
            "dist 9.100789070129395\n",
            "Sample 113 :\n",
            "dist 9.659614562988281\n",
            "Sample 114 :\n",
            "dist 9.213743209838867\n",
            "Sample 115 :\n",
            "dist 10.206989288330078\n",
            "Sample 116 :\n",
            "dist 8.64344596862793\n",
            "Sample 117 :\n",
            "dist 9.550772666931152\n",
            "Sample 118 :\n",
            "dist 10.473353385925293\n",
            "Sample 119 :\n",
            "dist 10.053415298461914\n",
            "Sample 120 :\n",
            "dist 12.336888313293457\n",
            "Sample 121 :\n",
            "dist 9.550772666931152\n",
            "Sample 122 :\n",
            "dist 10.07865047454834\n",
            "Sample 123 :\n",
            "dist 10.814924240112305\n",
            "Sample 124 :\n",
            "dist 6.08117151260376\n",
            "Sample 125 :\n",
            "dist 15.017980575561523\n",
            "Sample 126 :\n",
            "dist 9.856998443603516\n",
            "Sample 127 :\n",
            "dist 6.714141368865967\n",
            "Sample 128 :\n",
            "dist 9.550772666931152\n",
            "Sample 129 :\n",
            "dist 5.922364711761475\n",
            "Sample 130 :\n",
            "dist 9.089716911315918\n",
            "Sample 131 :\n",
            "dist 7.31467342376709\n",
            "Sample 132 :\n",
            "dist 14.941376686096191\n",
            "Sample 133 :\n",
            "dist 8.64344596862793\n",
            "Sample 134 :\n",
            "dist 11.129412651062012\n",
            "Sample 135 :\n",
            "dist 8.992192268371582\n",
            "Sample 136 :\n",
            "dist 11.67253303527832\n",
            "Sample 137 :\n",
            "dist 10.053415298461914\n",
            "Sample 138 :\n",
            "dist 9.249247550964355\n",
            "Sample 139 :\n",
            "dist 11.594229698181152\n",
            "Sample 140 :\n",
            "dist 10.053415298461914\n",
            "Sample 141 :\n",
            "dist 9.054219245910645\n",
            "Sample 142 :\n",
            "dist 6.924028396606445\n",
            "Sample 143 :\n",
            "dist 9.659614562988281\n",
            "Sample 144 :\n",
            "dist 10.27345085144043\n",
            "Sample 145 :\n",
            "dist 11.91559886932373\n",
            "Sample 146 :\n",
            "dist 9.039032936096191\n",
            "Sample 147 :\n",
            "dist 13.467585563659668\n",
            "Sample 148 :\n",
            "dist 10.00361156463623\n",
            "Sample 149 :\n",
            "dist 10.00361156463623\n",
            "Sample 150 :\n",
            "dist 10.00361156463623\n",
            "Sample 151 :\n",
            "dist 5.0141496658325195\n",
            "Sample 152 :\n",
            "dist 9.10306453704834\n",
            "Sample 153 :\n",
            "dist 12.618820190429688\n",
            "Sample 154 :\n",
            "dist 14.807355880737305\n",
            "Sample 155 :\n",
            "dist 8.976008415222168\n",
            "Sample 156 :\n",
            "dist 9.550772666931152\n",
            "Sample 157 :\n",
            "dist 6.714141368865967\n",
            "Sample 158 :\n",
            "dist 11.91559886932373\n",
            "Sample 159 :\n",
            "dist 12.167671203613281\n",
            "Sample 160 :\n",
            "dist 9.214948654174805\n",
            "Sample 161 :\n",
            "dist 13.467585563659668\n",
            "Sample 162 :\n",
            "dist 14.807355880737305\n",
            "Sample 163 :\n",
            "dist 10.09423542022705\n",
            "Sample 164 :\n",
            "dist 11.162457466125488\n",
            "Sample 165 :\n",
            "dist 12.167671203613281\n",
            "Sample 166 :\n",
            "dist 9.249247550964355\n",
            "Sample 167 :\n",
            "dist 10.1978759765625\n",
            "Sample 168 :\n",
            "dist 9.864704132080078\n",
            "Sample 169 :\n",
            "dist 5.922364711761475\n",
            "Sample 170 :\n",
            "dist 9.089716911315918\n",
            "Sample 171 :\n",
            "dist 6.714141368865967\n",
            "Sample 172 :\n",
            "dist 8.460783004760742\n",
            "Sample 173 :\n",
            "dist 13.467585563659668\n",
            "Sample 174 :\n",
            "dist 11.129412651062012\n",
            "Sample 175 :\n",
            "dist 9.214948654174805\n",
            "Sample 176 :\n",
            "dist 9.10306453704834\n",
            "Sample 177 :\n",
            "dist 11.168105125427246\n",
            "Sample 178 :\n",
            "dist 14.605659484863281\n",
            "Sample 179 :\n",
            "dist 8.812785148620605\n",
            "Sample 180 :\n",
            "dist 5.922364711761475\n",
            "Sample 181 :\n",
            "dist 11.594229698181152\n",
            "Sample 182 :\n",
            "dist 9.739049911499023\n",
            "Sample 183 :\n",
            "dist 9.005356788635254\n",
            "Sample 184 :\n",
            "dist 8.903407096862793\n",
            "Sample 185 :\n",
            "dist 10.287613868713379\n",
            "Sample 186 :\n",
            "dist 11.347253799438477\n",
            "Sample 187 :\n",
            "dist 14.197386741638184\n",
            "Sample 188 :\n",
            "dist 7.5439677238464355\n",
            "Sample 189 :\n",
            "dist 10.585667610168457\n",
            "Sample 190 :\n",
            "dist 6.924028396606445\n",
            "Sample 191 :\n",
            "dist 11.099358558654785\n",
            "Sample 192 :\n",
            "dist 10.423500061035156\n",
            "Sample 193 :\n",
            "dist 8.992192268371582\n",
            "Sample 194 :\n",
            "dist 10.186614990234375\n",
            "Sample 195 :\n",
            "dist 9.10306453704834\n",
            "Sample 196 :\n",
            "dist 12.372003555297852\n",
            "Sample 197 :\n",
            "dist 12.618820190429688\n",
            "Sample 198 :\n",
            "dist 13.546152114868164\n",
            "Sample 199 :\n",
            "dist 13.467585563659668\n",
            "Sample 200 :\n",
            "dist 7.31467342376709\n",
            "Sample 201 :\n",
            "dist 12.649848937988281\n",
            "Sample 202 :\n",
            "dist 6.714141368865967\n",
            "Sample 203 :\n",
            "dist 9.039032936096191\n",
            "Sample 204 :\n",
            "dist 4.514033794403076\n",
            "Sample 205 :\n",
            "dist 10.00361156463623\n",
            "Sample 206 :\n",
            "dist 9.550772666931152\n",
            "Sample 207 :\n",
            "dist 9.039032936096191\n",
            "Sample 208 :\n",
            "dist 9.572949409484863\n",
            "Sample 209 :\n",
            "dist 8.920267105102539\n",
            "Sample 210 :\n",
            "dist 11.207891464233398\n",
            "Sample 211 :\n",
            "dist 10.565771102905273\n",
            "Sample 212 :\n",
            "dist 8.756172180175781\n",
            "Sample 213 :\n",
            "dist 7.356421947479248\n",
            "Sample 214 :\n",
            "dist 10.1978759765625\n",
            "Sample 215 :\n",
            "dist 10.813817024230957\n",
            "Sample 216 :\n",
            "dist 7.31467342376709\n",
            "Sample 217 :\n",
            "dist 9.10306453704834\n",
            "Sample 218 :\n",
            "dist 15.70533275604248\n",
            "Sample 219 :\n",
            "dist 10.27345085144043\n",
            "Sample 220 :\n",
            "dist 10.852871894836426\n",
            "Sample 221 :\n",
            "dist 7.826977729797363\n",
            "Sample 222 :\n",
            "dist 9.822888374328613\n",
            "Sample 223 :\n",
            "dist 10.287613868713379\n",
            "Sample 224 :\n",
            "dist 6.08117151260376\n",
            "Sample 225 :\n",
            "dist 9.249247550964355\n",
            "Sample 226 :\n",
            "dist 14.048661231994629\n",
            "Sample 227 :\n",
            "dist 13.467585563659668\n",
            "Sample 228 :\n",
            "dist 11.099358558654785\n",
            "Sample 229 :\n",
            "dist 5.6047210693359375\n",
            "Sample 230 :\n",
            "dist 7.45473051071167\n",
            "Sample 231 :\n",
            "dist 8.16422176361084\n",
            "Sample 232 :\n",
            "dist 11.594229698181152\n",
            "Sample 233 :\n",
            "dist 8.047513008117676\n",
            "Sample 234 :\n",
            "dist 4.514033794403076\n",
            "Sample 235 :\n",
            "dist 10.395532608032227\n",
            "Sample 236 :\n",
            "dist 9.572949409484863\n",
            "Sample 237 :\n",
            "dist 10.07865047454834\n",
            "Sample 238 :\n",
            "dist 10.758524894714355\n",
            "Sample 239 :\n",
            "dist 11.129412651062012\n",
            "Sample 240 :\n",
            "dist 10.372018814086914\n",
            "Sample 241 :\n",
            "dist 11.942307472229004\n",
            "Sample 242 :\n",
            "dist 10.215923309326172\n",
            "Sample 243 :\n",
            "dist 9.856998443603516\n",
            "Sample 244 :\n",
            "dist 6.714141368865967\n",
            "Sample 245 :\n",
            "dist 10.395532608032227\n",
            "Sample 246 :\n",
            "dist 12.618820190429688\n",
            "Sample 247 :\n",
            "dist 8.728707313537598\n",
            "Sample 248 :\n",
            "dist 9.856998443603516\n",
            "Sample 249 :\n",
            "dist 9.328691482543945\n",
            "Sample 250 :\n",
            "dist 14.197386741638184\n",
            "Sample 251 :\n",
            "dist 4.514033794403076\n",
            "Sample 252 :\n",
            "dist 6.086319923400879\n",
            "Sample 253 :\n",
            "dist 10.564896583557129\n",
            "Sample 254 :\n",
            "dist 11.594229698181152\n",
            "Sample 255 :\n",
            "dist 6.086319923400879\n",
            "Sample 256 :\n",
            "dist 8.976008415222168\n",
            "Sample 257 :\n",
            "dist 7.356421947479248\n",
            "Sample 258 :\n",
            "dist 10.473353385925293\n",
            "Sample 259 :\n",
            "dist 12.167671203613281\n",
            "Sample 260 :\n",
            "dist 12.618820190429688\n",
            "Sample 261 :\n",
            "dist 6.086319923400879\n",
            "Sample 262 :\n",
            "dist 6.924028396606445\n",
            "Sample 263 :\n",
            "dist 8.019840240478516\n",
            "Sample 264 :\n",
            "dist 9.691022872924805\n",
            "Sample 265 :\n",
            "dist 6.905245780944824\n",
            "Sample 266 :\n",
            "dist 12.336888313293457\n",
            "Sample 267 :\n",
            "dist 10.437501907348633\n",
            "Sample 268 :\n",
            "dist 10.58375072479248\n",
            "Sample 269 :\n",
            "dist 3.7555344104766846\n",
            "Sample 270 :\n",
            "dist 8.992192268371582\n",
            "Sample 271 :\n",
            "dist 9.039032936096191\n",
            "Sample 272 :\n",
            "dist 6.722531318664551\n",
            "Sample 273 :\n",
            "dist 11.594229698181152\n",
            "Sample 274 :\n",
            "dist 9.10306453704834\n",
            "Sample 275 :\n",
            "dist 10.186614990234375\n",
            "Sample 276 :\n",
            "dist 9.089716911315918\n",
            "Sample 277 :\n",
            "dist 12.860376358032227\n",
            "Sample 278 :\n",
            "dist 9.005356788635254\n",
            "Sample 279 :\n",
            "dist 9.659614562988281\n",
            "Sample 280 :\n",
            "dist 7.356421947479248\n",
            "Sample 281 :\n",
            "dist 9.739049911499023\n",
            "Sample 282 :\n",
            "dist 9.856998443603516\n",
            "Sample 283 :\n",
            "dist 9.864704132080078\n",
            "Sample 284 :\n",
            "dist 7.356421947479248\n",
            "Sample 285 :\n",
            "dist 8.865540504455566\n",
            "Sample 286 :\n",
            "dist 10.27345085144043\n",
            "Sample 287 :\n",
            "dist 10.09423542022705\n",
            "Sample 288 :\n",
            "dist 9.039032936096191\n",
            "Sample 289 :\n",
            "dist 8.019840240478516\n",
            "Sample 290 :\n",
            "dist 11.594229698181152\n",
            "Sample 291 :\n",
            "dist 10.07865047454834\n",
            "Sample 292 :\n",
            "dist 10.423500061035156\n",
            "Sample 293 :\n",
            "dist 9.864704132080078\n",
            "Sample 294 :\n",
            "dist 8.019840240478516\n",
            "Sample 295 :\n",
            "dist 6.905245780944824\n",
            "Sample 296 :\n",
            "dist 9.089716911315918\n",
            "Sample 297 :\n",
            "dist 5.791200160980225\n",
            "Sample 298 :\n",
            "dist 10.565771102905273\n",
            "Sample 299 :\n",
            "dist 9.214948654174805\n",
            "Sample 300 :\n",
            "dist 8.756172180175781\n",
            "Sample 301 :\n",
            "dist 6.714141368865967\n",
            "Sample 302 :\n",
            "dist 11.162457466125488\n",
            "Sample 303 :\n",
            "dist 9.864704132080078\n",
            "Sample 304 :\n",
            "dist 10.437501907348633\n",
            "Sample 305 :\n",
            "dist 10.506524085998535\n",
            "Sample 306 :\n",
            "dist 12.167671203613281\n",
            "Sample 307 :\n",
            "dist 10.473353385925293\n",
            "Sample 308 :\n",
            "dist 8.812785148620605\n",
            "Sample 309 :\n",
            "dist 9.214948654174805\n",
            "Sample 310 :\n",
            "dist 13.546152114868164\n",
            "Sample 311 :\n",
            "dist 10.565771102905273\n",
            "Sample 312 :\n",
            "dist 9.213743209838867\n",
            "Sample 313 :\n",
            "dist 9.659614562988281\n",
            "Sample 314 :\n",
            "dist 5.826895713806152\n",
            "Sample 315 :\n",
            "dist 7.444174289703369\n",
            "Sample 316 :\n",
            "dist 9.10306453704834\n",
            "Sample 317 :\n",
            "dist 9.214948654174805\n",
            "Sample 318 :\n",
            "dist 11.099358558654785\n",
            "Sample 319 :\n",
            "dist 14.359901428222656\n",
            "Sample 320 :\n",
            "dist 11.207891464233398\n",
            "Sample 321 :\n",
            "dist 9.739049911499023\n",
            "Sample 322 :\n",
            "dist 10.816109657287598\n",
            "Sample 323 :\n",
            "dist 8.812785148620605\n",
            "Sample 324 :\n",
            "dist 9.054219245910645\n",
            "Sample 325 :\n",
            "dist 12.860376358032227\n",
            "Sample 326 :\n",
            "dist 8.16422176361084\n",
            "Sample 327 :\n",
            "dist 10.11634349822998\n",
            "Sample 328 :\n",
            "dist 7.45473051071167\n",
            "Sample 329 :\n",
            "dist 10.053415298461914\n",
            "Sample 330 :\n",
            "dist 10.27345085144043\n",
            "Sample 331 :\n",
            "dist 9.659614562988281\n",
            "Sample 332 :\n",
            "dist 8.16422176361084\n",
            "Sample 333 :\n",
            "dist 10.372018814086914\n",
            "Sample 334 :\n",
            "dist 10.814924240112305\n",
            "Sample 335 :\n",
            "dist 10.11634349822998\n",
            "Sample 336 :\n",
            "dist 12.860376358032227\n",
            "Sample 337 :\n",
            "dist 10.565771102905273\n",
            "Sample 338 :\n",
            "dist 10.585667610168457\n",
            "Sample 339 :\n",
            "dist 9.822664260864258\n",
            "Sample 340 :\n",
            "dist 10.09423542022705\n",
            "Sample 341 :\n",
            "dist 8.812785148620605\n",
            "Sample 342 :\n",
            "dist 15.017980575561523\n",
            "Sample 343 :\n",
            "dist 10.833091735839844\n",
            "Sample 344 :\n",
            "dist 11.207891464233398\n",
            "Sample 345 :\n",
            "dist 9.864704132080078\n",
            "Sample 346 :\n",
            "dist 7.801884174346924\n",
            "Sample 347 :\n",
            "dist 12.372003555297852\n",
            "Sample 348 :\n",
            "dist 9.054219245910645\n",
            "Sample 349 :\n",
            "dist 8.395001411437988\n",
            "Sample 350 :\n",
            "dist 10.206989288330078\n",
            "Sample 351 :\n",
            "dist 12.123115539550781\n",
            "Sample 352 :\n",
            "dist 10.816109657287598\n",
            "Sample 353 :\n",
            "dist 7.444174289703369\n",
            "Sample 354 :\n",
            "dist 11.207891464233398\n",
            "Sample 355 :\n",
            "dist 7.356421947479248\n",
            "Sample 356 :\n",
            "dist 12.860376358032227\n",
            "Sample 357 :\n",
            "dist 9.822888374328613\n",
            "Sample 358 :\n",
            "dist 10.852871894836426\n",
            "Sample 359 :\n",
            "dist 10.310395240783691\n",
            "Sample 360 :\n",
            "dist 11.942307472229004\n",
            "Sample 361 :\n",
            "dist 5.0141496658325195\n",
            "Sample 362 :\n",
            "dist 12.336888313293457\n",
            "Sample 363 :\n",
            "dist 10.437501907348633\n",
            "Sample 364 :\n",
            "dist 10.550861358642578\n",
            "Sample 365 :\n",
            "dist 10.053415298461914\n",
            "Sample 366 :\n",
            "dist 8.16422176361084\n",
            "Sample 367 :\n",
            "dist 12.366839408874512\n",
            "Sample 368 :\n",
            "dist 5.826895713806152\n",
            "Sample 369 :\n",
            "dist 8.920267105102539\n",
            "Sample 370 :\n",
            "dist 11.485407829284668\n",
            "Sample 371 :\n",
            "dist 7.45473051071167\n",
            "Sample 372 :\n",
            "dist 8.812785148620605\n",
            "Sample 373 :\n",
            "dist 12.372003555297852\n",
            "Sample 374 :\n",
            "dist 8.812785148620605\n",
            "Sample 375 :\n",
            "dist 7.15024995803833\n",
            "Sample 376 :\n",
            "dist 9.213743209838867\n",
            "Sample 377 :\n",
            "dist 10.506524085998535\n",
            "Sample 378 :\n",
            "dist 11.942307472229004\n",
            "Sample 379 :\n",
            "dist 9.054219245910645\n",
            "Sample 380 :\n",
            "dist 5.922364711761475\n",
            "Sample 381 :\n",
            "dist 13.467585563659668\n",
            "Sample 382 :\n",
            "dist 13.467585563659668\n",
            "Sample 383 :\n",
            "dist 4.894937038421631\n",
            "Sample 384 :\n",
            "dist 11.129412651062012\n",
            "Sample 385 :\n",
            "dist 10.852871894836426\n",
            "Sample 386 :\n",
            "dist 8.812785148620605\n",
            "Sample 387 :\n",
            "dist 6.647155284881592\n",
            "Sample 388 :\n",
            "dist 11.207891464233398\n",
            "Sample 389 :\n",
            "dist 11.099358558654785\n",
            "Sample 390 :\n",
            "dist 10.506524085998535\n",
            "Sample 391 :\n",
            "dist 8.64344596862793\n",
            "Sample 392 :\n",
            "dist 9.856998443603516\n",
            "Sample 393 :\n",
            "dist 5.791200160980225\n",
            "Sample 394 :\n",
            "dist 8.047513008117676\n",
            "Sample 395 :\n",
            "dist 11.594229698181152\n",
            "Sample 396 :\n",
            "dist 8.992192268371582\n",
            "Sample 397 :\n",
            "dist 9.100789070129395\n",
            "Sample 398 :\n",
            "dist 9.214948654174805\n",
            "Sample 399 :\n",
            "dist 7.801884174346924\n",
            "Sample 400 :\n",
            "dist 9.100789070129395\n",
            "Sample 401 :\n",
            "dist 7.15024995803833\n",
            "Sample 402 :\n",
            "dist 5.0141496658325195\n",
            "Sample 403 :\n",
            "dist 8.756172180175781\n",
            "Sample 404 :\n",
            "dist 10.852871894836426\n",
            "Sample 405 :\n",
            "dist 14.848955154418945\n",
            "Sample 406 :\n",
            "dist 5.060171127319336\n",
            "Sample 407 :\n",
            "dist 5.0141496658325195\n",
            "Sample 408 :\n",
            "dist 9.856998443603516\n",
            "Sample 409 :\n",
            "dist 9.039032936096191\n",
            "Sample 410 :\n",
            "dist 10.09423542022705\n",
            "Sample 411 :\n",
            "dist 10.550861358642578\n",
            "Sample 412 :\n",
            "dist 10.813817024230957\n",
            "Sample 413 :\n",
            "dist 12.366839408874512\n",
            "Sample 414 :\n",
            "dist 15.017980575561523\n",
            "Sample 415 :\n",
            "dist 10.186614990234375\n",
            "Sample 416 :\n",
            "dist 9.039032936096191\n",
            "Sample 417 :\n",
            "dist 10.00361156463623\n",
            "Sample 418 :\n",
            "dist 11.67253303527832\n",
            "Sample 419 :\n",
            "dist 10.547954559326172\n",
            "Sample 420 :\n",
            "dist 13.343537330627441\n",
            "Sample 421 :\n",
            "dist 9.100789070129395\n",
            "Sample 422 :\n",
            "dist 13.467585563659668\n",
            "Sample 423 :\n",
            "dist 9.039032936096191\n",
            "Sample 424 :\n",
            "dist 9.550772666931152\n",
            "Sample 425 :\n",
            "dist 10.585667610168457\n",
            "Sample 426 :\n",
            "dist 10.206989288330078\n",
            "Sample 427 :\n",
            "dist 11.162457466125488\n",
            "Sample 428 :\n",
            "dist 7.356421947479248\n",
            "Sample 429 :\n",
            "dist 9.856998443603516\n",
            "Sample 430 :\n",
            "dist 7.45473051071167\n",
            "Sample 431 :\n",
            "dist 10.75291919708252\n",
            "Sample 432 :\n",
            "dist 6.714141368865967\n",
            "Sample 433 :\n",
            "dist 5.060171127319336\n",
            "Sample 434 :\n",
            "dist 10.186614990234375\n",
            "Sample 435 :\n",
            "dist 9.054219245910645\n",
            "Sample 436 :\n",
            "dist 7.15024995803833\n",
            "Sample 437 :\n",
            "dist 10.215923309326172\n",
            "Sample 438 :\n",
            "dist 12.336888313293457\n",
            "Sample 439 :\n",
            "dist 10.06309700012207\n",
            "Sample 440 :\n",
            "dist 11.084732055664062\n",
            "Sample 441 :\n",
            "dist 7.801884174346924\n",
            "Sample 442 :\n",
            "dist 7.5439677238464355\n",
            "Sample 443 :\n",
            "dist 10.00361156463623\n",
            "Sample 444 :\n",
            "dist 7.801884174346924\n",
            "Sample 445 :\n",
            "dist 10.415663719177246\n",
            "Sample 446 :\n",
            "dist 14.197386741638184\n",
            "Sample 447 :\n",
            "dist 10.565771102905273\n",
            "Sample 448 :\n",
            "dist 11.91559886932373\n",
            "Sample 449 :\n",
            "dist 9.089716911315918\n",
            "Sample 450 :\n",
            "dist 11.942307472229004\n",
            "Sample 451 :\n",
            "dist 11.737712860107422\n",
            "Sample 452 :\n",
            "dist 5.922364711761475\n",
            "Sample 453 :\n",
            "dist 14.197386741638184\n",
            "Sample 454 :\n",
            "dist 11.91559886932373\n",
            "Sample 455 :\n",
            "dist 7.45473051071167\n",
            "Sample 456 :\n",
            "dist 10.1978759765625\n",
            "Sample 457 :\n",
            "dist 8.992192268371582\n",
            "Sample 458 :\n",
            "dist 9.864704132080078\n",
            "Sample 459 :\n",
            "dist 9.004803657531738\n",
            "Sample 460 :\n",
            "dist 11.129412651062012\n",
            "Sample 461 :\n",
            "dist 9.856998443603516\n",
            "Sample 462 :\n",
            "dist 10.287613868713379\n",
            "Sample 463 :\n",
            "dist 9.739049911499023\n",
            "Sample 464 :\n",
            "dist 6.924028396606445\n",
            "Sample 465 :\n",
            "dist 7.356421947479248\n",
            "Sample 466 :\n",
            "dist 11.099358558654785\n",
            "Sample 467 :\n",
            "dist 9.004803657531738\n",
            "Sample 468 :\n",
            "dist 7.356421947479248\n",
            "Sample 469 :\n",
            "dist 9.739049911499023\n",
            "Sample 470 :\n",
            "dist 9.10306453704834\n",
            "Sample 471 :\n",
            "dist 11.594229698181152\n",
            "Sample 472 :\n",
            "dist 5.0141496658325195\n",
            "Sample 473 :\n",
            "dist 11.162457466125488\n",
            "Sample 474 :\n",
            "dist 9.328691482543945\n",
            "Sample 475 :\n",
            "dist 10.186614990234375\n",
            "Sample 476 :\n",
            "dist 10.06309700012207\n",
            "Sample 477 :\n",
            "dist 12.167671203613281\n",
            "Sample 478 :\n",
            "dist 10.09423542022705\n",
            "Sample 479 :\n",
            "dist 10.423500061035156\n",
            "Sample 480 :\n",
            "dist 10.07865047454834\n",
            "Sample 481 :\n",
            "dist 5.826895713806152\n",
            "Sample 482 :\n",
            "dist 10.186614990234375\n",
            "Sample 483 :\n",
            "dist 6.647155284881592\n",
            "Sample 484 :\n",
            "dist 6.086319923400879\n",
            "Sample 485 :\n",
            "dist 8.865540504455566\n",
            "Sample 486 :\n",
            "dist 10.215923309326172\n",
            "Sample 487 :\n",
            "dist 11.24622917175293\n",
            "Sample 488 :\n",
            "dist 10.547954559326172\n",
            "Sample 489 :\n",
            "dist 10.473353385925293\n",
            "Sample 490 :\n",
            "dist 10.139031410217285\n",
            "Sample 491 :\n",
            "dist 9.054219245910645\n",
            "Sample 492 :\n",
            "dist 6.647155284881592\n",
            "Sample 493 :\n",
            "dist 9.215288162231445\n",
            "Sample 494 :\n",
            "dist 7.31467342376709\n",
            "Sample 495 :\n",
            "dist 12.123115539550781\n",
            "Sample 496 :\n",
            "dist 9.864704132080078\n",
            "Sample 497 :\n",
            "dist 11.594229698181152\n",
            "Sample 498 :\n",
            "dist 6.714141368865967\n",
            "Sample 499 :\n",
            "dist 10.415663719177246\n",
            "Sample 500 :\n",
            "dist 10.628581047058105\n",
            "Sample 501 :\n",
            "dist 14.29787540435791\n",
            "Sample 502 :\n",
            "dist 9.659614562988281\n",
            "Sample 503 :\n",
            "dist 10.816109657287598\n",
            "Sample 504 :\n",
            "dist 8.77670955657959\n",
            "Sample 505 :\n",
            "dist 9.039032936096191\n",
            "Sample 506 :\n",
            "dist 11.168105125427246\n",
            "Sample 507 :\n",
            "dist 9.004803657531738\n",
            "Sample 508 :\n",
            "dist 9.691022872924805\n",
            "Sample 509 :\n",
            "dist 12.123115539550781\n",
            "Sample 510 :\n",
            "dist 7.356421947479248\n",
            "Sample 511 :\n",
            "dist 8.16422176361084\n",
            "Sample 512 :\n",
            "dist 10.186614990234375\n",
            "Sample 513 :\n",
            "dist 9.215288162231445\n",
            "Sample 514 :\n",
            "dist 13.467585563659668\n",
            "Sample 515 :\n",
            "dist 9.856998443603516\n",
            "Sample 516 :\n",
            "dist 12.336888313293457\n",
            "Sample 517 :\n",
            "dist 7.489496231079102\n",
            "Sample 518 :\n",
            "dist 10.1978759765625\n",
            "Sample 519 :\n",
            "dist 10.813817024230957\n",
            "Sample 520 :\n",
            "dist 9.039032936096191\n",
            "Sample 521 :\n",
            "dist 14.197386741638184\n",
            "Sample 522 :\n",
            "dist 9.572949409484863\n",
            "Sample 523 :\n",
            "dist 8.976008415222168\n",
            "Sample 524 :\n",
            "dist 8.992192268371582\n",
            "Sample 525 :\n",
            "dist 9.822888374328613\n",
            "Sample 526 :\n",
            "dist 7.826977729797363\n",
            "Sample 527 :\n",
            "dist 10.36896800994873\n",
            "Sample 528 :\n",
            "dist 11.162457466125488\n",
            "Sample 529 :\n",
            "dist 8.019840240478516\n",
            "Sample 530 :\n",
            "dist 8.903407096862793\n",
            "Sample 531 :\n",
            "dist 12.167671203613281\n",
            "Sample 532 :\n",
            "dist 5.922364711761475\n",
            "Sample 533 :\n",
            "dist 12.123115539550781\n",
            "Sample 534 :\n",
            "dist 8.976008415222168\n",
            "Sample 535 :\n",
            "dist 9.039032936096191\n",
            "Sample 536 :\n",
            "dist 8.77670955657959\n",
            "Sample 537 :\n",
            "dist 9.659614562988281\n",
            "Sample 538 :\n",
            "dist 8.77670955657959\n",
            "Sample 539 :\n",
            "dist 3.7555344104766846\n",
            "Sample 540 :\n",
            "dist 10.423500061035156\n",
            "Sample 541 :\n",
            "dist 9.864704132080078\n",
            "Sample 542 :\n",
            "dist 10.186614990234375\n",
            "Sample 543 :\n",
            "dist 15.017980575561523\n",
            "Sample 544 :\n",
            "dist 6.722531318664551\n",
            "Sample 545 :\n",
            "dist 11.737712860107422\n",
            "Sample 546 :\n",
            "dist 9.659614562988281\n",
            "Sample 547 :\n",
            "dist 10.565771102905273\n",
            "Sample 548 :\n",
            "dist 9.214948654174805\n",
            "Sample 549 :\n",
            "dist 9.691022872924805\n",
            "Sample 550 :\n",
            "dist 10.814924240112305\n",
            "Sample 551 :\n",
            "dist 9.572949409484863\n",
            "Sample 552 :\n",
            "dist 11.108943939208984\n",
            "Sample 553 :\n",
            "dist 7.444174289703369\n",
            "Sample 554 :\n",
            "dist 10.565771102905273\n",
            "Sample 555 :\n",
            "dist 5.922364711761475\n",
            "Sample 556 :\n",
            "dist 10.592268943786621\n",
            "Sample 557 :\n",
            "dist 10.206989288330078\n",
            "Sample 558 :\n",
            "dist 11.594229698181152\n",
            "Sample 559 :\n",
            "dist 10.547954559326172\n",
            "Sample 560 :\n",
            "dist 10.852871894836426\n",
            "Sample 561 :\n",
            "dist 10.565771102905273\n",
            "Sample 562 :\n",
            "dist 9.328691482543945\n",
            "Sample 563 :\n",
            "dist 8.019840240478516\n",
            "Sample 564 :\n",
            "dist 8.64344596862793\n",
            "Sample 565 :\n",
            "dist 11.942307472229004\n",
            "Sample 566 :\n",
            "dist 9.249247550964355\n",
            "Sample 567 :\n",
            "dist 9.214948654174805\n",
            "Sample 568 :\n",
            "dist 10.053415298461914\n",
            "Sample 569 :\n",
            "dist 15.772144317626953\n",
            "Sample 570 :\n",
            "dist 10.11634349822998\n",
            "Sample 571 :\n",
            "dist 12.167671203613281\n",
            "Sample 572 :\n",
            "dist 12.167671203613281\n",
            "Sample 573 :\n",
            "dist 7.826977729797363\n",
            "Sample 574 :\n",
            "dist 7.801884174346924\n",
            "Sample 575 :\n",
            "dist 7.45473051071167\n",
            "Sample 576 :\n",
            "dist 9.659614562988281\n",
            "Sample 577 :\n",
            "dist 9.328691482543945\n",
            "Sample 578 :\n",
            "dist 12.618820190429688\n",
            "Sample 579 :\n",
            "dist 10.585667610168457\n",
            "Sample 580 :\n",
            "dist 9.659614562988281\n",
            "Sample 581 :\n",
            "dist 9.005356788635254\n",
            "Sample 582 :\n",
            "dist 6.714141368865967\n",
            "Sample 583 :\n",
            "dist 11.099358558654785\n",
            "Sample 584 :\n",
            "dist 8.019840240478516\n",
            "Sample 585 :\n",
            "dist 10.395532608032227\n",
            "Sample 586 :\n",
            "dist 7.444174289703369\n",
            "Sample 587 :\n",
            "dist 9.039032936096191\n",
            "Sample 588 :\n",
            "dist 8.992192268371582\n",
            "Sample 589 :\n",
            "dist 9.054219245910645\n",
            "Sample 590 :\n",
            "dist 7.444174289703369\n",
            "Sample 591 :\n",
            "dist 6.714141368865967\n",
            "Sample 592 :\n",
            "dist 9.328691482543945\n",
            "Sample 593 :\n",
            "dist 10.423500061035156\n",
            "Sample 594 :\n",
            "dist 10.550861358642578\n",
            "Sample 595 :\n",
            "dist 9.856998443603516\n",
            "Sample 596 :\n",
            "dist 10.852871894836426\n",
            "Sample 597 :\n",
            "dist 10.585667610168457\n",
            "Sample 598 :\n",
            "dist 10.215923309326172\n",
            "Sample 599 :\n",
            "dist 10.05289363861084\n",
            "Sample 600 :\n",
            "dist 8.16422176361084\n",
            "Sample 601 :\n",
            "dist 6.086319923400879\n",
            "Sample 602 :\n",
            "dist 7.295797348022461\n",
            "Sample 603 :\n",
            "dist 8.64344596862793\n",
            "Sample 604 :\n",
            "dist 9.572949409484863\n",
            "Sample 605 :\n",
            "dist 10.813817024230957\n",
            "Sample 606 :\n",
            "dist 8.812785148620605\n",
            "Sample 607 :\n",
            "dist 10.11634349822998\n",
            "Sample 608 :\n",
            "dist 9.249247550964355\n",
            "Sample 609 :\n",
            "dist 11.485407829284668\n",
            "Sample 610 :\n",
            "dist 9.822664260864258\n",
            "Sample 611 :\n",
            "dist 12.372003555297852\n",
            "Sample 612 :\n",
            "dist 7.356421947479248\n",
            "Sample 613 :\n",
            "dist 7.826977729797363\n",
            "Sample 614 :\n",
            "dist 8.756172180175781\n",
            "Sample 615 :\n",
            "dist 8.865540504455566\n",
            "Sample 616 :\n",
            "dist 10.814924240112305\n",
            "Sample 617 :\n",
            "dist 6.714141368865967\n",
            "Sample 618 :\n",
            "dist 12.860376358032227\n",
            "Sample 619 :\n",
            "dist 10.1978759765625\n",
            "Sample 620 :\n",
            "dist 8.16422176361084\n",
            "Sample 621 :\n",
            "dist 8.992192268371582\n",
            "Sample 622 :\n",
            "dist 11.24622917175293\n",
            "Sample 623 :\n",
            "dist 11.129412651062012\n",
            "Sample 624 :\n",
            "dist 12.226571083068848\n",
            "Sample 625 :\n",
            "dist 11.207891464233398\n",
            "Sample 626 :\n",
            "dist 7.45473051071167\n",
            "Sample 627 :\n",
            "dist 11.942307472229004\n",
            "Sample 628 :\n",
            "dist 8.16422176361084\n",
            "Sample 629 :\n",
            "dist 10.565771102905273\n",
            "Sample 630 :\n",
            "dist 15.22869873046875\n",
            "Sample 631 :\n",
            "dist 7.31467342376709\n",
            "Sample 632 :\n",
            "dist 10.1978759765625\n",
            "Sample 633 :\n",
            "dist 4.329094409942627\n",
            "Sample 634 :\n",
            "dist 11.737712860107422\n",
            "Sample 635 :\n",
            "dist 8.865540504455566\n",
            "Sample 636 :\n",
            "dist 8.920267105102539\n",
            "Sample 637 :\n",
            "dist 10.09423542022705\n",
            "Sample 638 :\n",
            "dist 9.039032936096191\n",
            "Sample 639 :\n",
            "dist 14.605659484863281\n",
            "Sample 640 :\n",
            "dist 9.822888374328613\n",
            "Sample 641 :\n",
            "dist 7.31467342376709\n",
            "Sample 642 :\n",
            "dist 8.16422176361084\n",
            "Sample 643 :\n",
            "dist 7.928226947784424\n",
            "Sample 644 :\n",
            "dist 10.1978759765625\n",
            "Sample 645 :\n",
            "dist 8.047513008117676\n",
            "Sample 646 :\n",
            "dist 9.691022872924805\n",
            "Sample 647 :\n",
            "dist 7.45473051071167\n",
            "Sample 648 :\n",
            "dist 9.039032936096191\n",
            "Sample 649 :\n",
            "dist 15.017980575561523\n",
            "Sample 650 :\n",
            "dist 10.628581047058105\n",
            "Sample 651 :\n",
            "dist 5.922364711761475\n",
            "Sample 652 :\n",
            "dist 8.16422176361084\n",
            "Sample 653 :\n",
            "dist 9.100789070129395\n",
            "Sample 654 :\n",
            "dist 10.423500061035156\n",
            "Sample 655 :\n",
            "dist 9.856998443603516\n",
            "Sample 656 :\n",
            "dist 10.585667610168457\n",
            "Sample 657 :\n",
            "dist 9.739049911499023\n",
            "Sample 658 :\n",
            "dist 10.36896800994873\n",
            "Sample 659 :\n",
            "dist 9.039032936096191\n",
            "Sample 660 :\n",
            "dist 5.826895713806152\n",
            "Sample 661 :\n",
            "dist 11.942307472229004\n",
            "Sample 662 :\n",
            "dist 11.24622917175293\n",
            "Sample 663 :\n",
            "dist 9.822664260864258\n",
            "Sample 664 :\n",
            "dist 5.0141496658325195\n",
            "Sample 665 :\n",
            "dist 10.547954559326172\n",
            "Sample 666 :\n",
            "dist 11.737712860107422\n",
            "Sample 667 :\n",
            "dist 7.45473051071167\n",
            "Sample 668 :\n",
            "dist 9.214948654174805\n",
            "Sample 669 :\n",
            "dist 9.739049911499023\n",
            "Sample 670 :\n",
            "dist 9.005356788635254\n",
            "Sample 671 :\n",
            "dist 9.822664260864258\n",
            "Sample 672 :\n",
            "dist 11.594229698181152\n",
            "Sample 673 :\n",
            "dist 10.423500061035156\n",
            "Sample 674 :\n",
            "dist 7.801884174346924\n",
            "Sample 675 :\n",
            "dist 7.356421947479248\n",
            "Sample 676 :\n",
            "dist 8.395001411437988\n",
            "Sample 677 :\n",
            "dist 9.005356788635254\n",
            "Sample 678 :\n",
            "dist 10.00361156463623\n",
            "Sample 679 :\n",
            "dist 10.585667610168457\n",
            "Sample 680 :\n",
            "dist 5.6047210693359375\n",
            "Sample 681 :\n",
            "dist 9.089716911315918\n",
            "Sample 682 :\n",
            "dist 11.942307472229004\n",
            "Sample 683 :\n",
            "dist 9.659614562988281\n",
            "Sample 684 :\n",
            "dist 6.714141368865967\n",
            "Sample 685 :\n",
            "dist 9.089716911315918\n",
            "Sample 686 :\n",
            "dist 4.894937038421631\n",
            "Sample 687 :\n",
            "dist 9.039032936096191\n",
            "Sample 688 :\n",
            "dist 9.100789070129395\n",
            "Sample 689 :\n",
            "dist 8.019840240478516\n",
            "Sample 690 :\n",
            "dist 10.506524085998535\n",
            "Sample 691 :\n",
            "dist 11.737712860107422\n",
            "Sample 692 :\n",
            "dist 9.659614562988281\n",
            "Sample 693 :\n",
            "dist 10.1978759765625\n",
            "Sample 694 :\n",
            "dist 11.67253303527832\n",
            "Sample 695 :\n",
            "dist 11.129412651062012\n",
            "Sample 696 :\n",
            "dist 9.739049911499023\n",
            "Sample 697 :\n",
            "dist 11.207891464233398\n",
            "Sample 698 :\n",
            "dist 9.572949409484863\n",
            "Sample 699 :\n",
            "dist 10.816109657287598\n",
            "Sample 700 :\n",
            "dist 12.123115539550781\n",
            "Sample 701 :\n",
            "dist 9.328691482543945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CULsI8xHWm9w",
        "colab_type": "code",
        "outputId": "daf8e8eb-6208-4154-8eee-15523714d0cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df_dist.sort_values(by=0, ascending=\"True\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3.755534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>3.755534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>3.755534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>3.755534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633</th>\n",
              "      <td>4.329094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>4.514034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>4.514034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>4.514034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>4.514034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>4.894937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>686</th>\n",
              "      <td>4.894937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>4.894937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4.894937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>407</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.014150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>5.060171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>5.060171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>680</th>\n",
              "      <td>5.604721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5.604721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>5.604721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>5.791200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>5.791200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>5.826896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>660</th>\n",
              "      <td>5.826896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>13.467586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>13.546152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>13.546152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>14.048661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>14.197387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>14.297875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>14.297875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>14.359901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639</th>\n",
              "      <td>14.605659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>14.605659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>14.807356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>14.807356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>14.807356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>405</th>\n",
              "      <td>14.848955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>14.941377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649</th>\n",
              "      <td>15.017981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630</th>\n",
              "      <td>15.228699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>15.705333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>15.705333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>15.772144</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>702 rows  1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0\n",
              "18    3.755534\n",
              "269   3.755534\n",
              "103   3.755534\n",
              "539   3.755534\n",
              "633   4.329094\n",
              "34    4.514034\n",
              "251   4.514034\n",
              "204   4.514034\n",
              "234   4.514034\n",
              "47    4.894937\n",
              "686   4.894937\n",
              "383   4.894937\n",
              "28    4.894937\n",
              "151   5.014150\n",
              "664   5.014150\n",
              "35    5.014150\n",
              "472   5.014150\n",
              "361   5.014150\n",
              "402   5.014150\n",
              "407   5.014150\n",
              "2     5.014150\n",
              "406   5.060171\n",
              "433   5.060171\n",
              "680   5.604721\n",
              "26    5.604721\n",
              "229   5.604721\n",
              "393   5.791200\n",
              "297   5.791200\n",
              "481   5.826896\n",
              "660   5.826896\n",
              "..         ...\n",
              "514  13.467586\n",
              "198  13.546152\n",
              "310  13.546152\n",
              "226  14.048661\n",
              "521  14.197387\n",
              "446  14.197387\n",
              "38   14.197387\n",
              "453  14.197387\n",
              "250  14.197387\n",
              "187  14.197387\n",
              "501  14.297875\n",
              "24   14.297875\n",
              "319  14.359901\n",
              "639  14.605659\n",
              "178  14.605659\n",
              "88   14.807356\n",
              "162  14.807356\n",
              "154  14.807356\n",
              "405  14.848955\n",
              "132  14.941377\n",
              "51   15.017981\n",
              "342  15.017981\n",
              "125  15.017981\n",
              "543  15.017981\n",
              "414  15.017981\n",
              "649  15.017981\n",
              "630  15.228699\n",
              "19   15.705333\n",
              "218  15.705333\n",
              "569  15.772144\n",
              "\n",
              "[702 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}