{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1219,
     "status": "ok",
     "timestamp": 1565371395728,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "zGxqA9-2WM_B",
    "outputId": "05a4de57-0ffa-43a9-d3ad-56f0da345231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OcpK6L0ZkcwZ"
   },
   "source": [
    "**This version uses 1 dim values for distance. Weights are not shared.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3111,
     "status": "ok",
     "timestamp": 1565302337592,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "dYyZZNm3aM0o",
    "outputId": "683882c2-6fef-41d1-c819-7741d9340493"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Reshape, Dropout, Add, merge, Subtract\n",
    "from keras.layers import Convolution1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import concatenate, Activation\n",
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "import h5py\n",
    "import time\n",
    "import keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 933,
     "status": "error",
     "timestamp": 1566243668556,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "j5QQAwEZV4E0",
    "outputId": "85cfce59-d8b4-425e-d7ef-b673d83d974a"
   },
   "outputs": [],
   "source": [
    "def mat_mul(A, B):\n",
    "    return tf.matmul(A, B)\n",
    "\n",
    "\n",
    "def load_h5(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    id = f['id'][:]\n",
    "    return (data, label, id)\n",
    "\n",
    "\n",
    "def rotate_point_cloud(batch_data):\n",
    "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
    "        rotation is per shape based along up direction\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        shape_pc = batch_data[k, ...]\n",
    "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "    return rotated_data\n",
    "\n",
    "\n",
    "def jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):\n",
    "    \"\"\" Randomly jitter points. jittering is per point.\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, jittered batch of point clouds\n",
    "    \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    assert(clip > 0)\n",
    "    jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1 * clip, clip)\n",
    "    jittered_data += batch_data\n",
    "    return jittered_data\n",
    "  \n",
    "\n",
    "# number of points in each sample\n",
    "num_points = 2048\n",
    "\n",
    "# define optimizer\n",
    "adam = optimizers.Adam(lr=0.001, decay=0.7)\n",
    "\n",
    "def base_model(input):\n",
    "  # input_Transformation_net\n",
    "  input_points = input\n",
    "  x = Convolution1D(64, 1, activation='relu', input_shape=(num_points, 3))(input_points)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Convolution1D(128, 1, activation='relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Convolution1D(1024, 1, activation='relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = MaxPooling1D(pool_size=num_points)(x)\n",
    "  x = Dense(512, activation='relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dense(256, activation='relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dense(9, weights=[np.zeros([256, 9]), np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32)])(x)\n",
    "  input_T = Reshape((3, 3))(x)\n",
    "\n",
    "  # forward net\n",
    "  g = Lambda(mat_mul, arguments={'B': input_T})(input_points)\n",
    "  g = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\n",
    "  g = BatchNormalization()(g)\n",
    "  g = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\n",
    "  g = BatchNormalization()(g)\n",
    "\n",
    "  # feature transform net\n",
    "  f = Convolution1D(64, 1, activation='relu')(g)\n",
    "  f = BatchNormalization()(f)\n",
    "  f = Convolution1D(128, 1, activation='relu')(f)\n",
    "  f = BatchNormalization()(f)\n",
    "  f = Convolution1D(1024, 1, activation='relu')(f)\n",
    "  f = BatchNormalization()(f)\n",
    "  f = MaxPooling1D(pool_size=num_points)(f)\n",
    "  f = Dense(512, activation='relu')(f)\n",
    "  f = BatchNormalization()(f)\n",
    "  f = Dense(256, activation='relu')(f)\n",
    "  f = BatchNormalization()(f)\n",
    "  f = Dense(64 * 64, weights=[np.zeros([256, 64 * 64]), np.eye(64).flatten().astype(np.float32)])(f)\n",
    "  feature_T = Reshape((64, 64))(f)\n",
    "\n",
    "  # forward net\n",
    "  g = Lambda(mat_mul, arguments={'B': feature_T})(g)\n",
    "  g = Convolution1D(64, 1, activation='relu')(g)\n",
    "  g = BatchNormalization()(g)\n",
    "  g = Convolution1D(128, 1, activation='relu')(g)\n",
    "  g = BatchNormalization()(g)\n",
    "  g = Convolution1D(20, 1, activation='relu')(g)\n",
    "  g = BatchNormalization()(g)\n",
    "\n",
    "  # global_feature\n",
    "  global_feature = MaxPooling1D(pool_size=num_points)(g)\n",
    "  out = Flatten()(global_feature)\n",
    "  \n",
    "  return out\n",
    "\n",
    "input_points1=Input(shape=(num_points, 3))\n",
    "input_points2=Input(shape=(num_points, 3))\n",
    "input_points3=Input(shape=(num_points, 3))\n",
    "\n",
    "out1 = base_model(input=input_points1)\n",
    "out2 = base_model(input=input_points2)\n",
    "out3 = base_model(input=input_points3)\n",
    "\n",
    "\n",
    "# ------------------------------------- Concatenate ---\n",
    "\n",
    "# Getting the L1 Distance between the 2 encodings\n",
    "L1_layer = Lambda(lambda tensor:tf.reduce_sum(K.square(tensor[0] - tensor[1]), axis=1, keepdims=True))\n",
    "# L1_layer = Lambda(lambda tensor:K.sum(K.square(tensor[0] - tensor[1]), axis=1, keepdims=True))\n",
    "\n",
    "# Add the distance function to the network\n",
    "distance1 = L1_layer([out1, out2])\n",
    "distance2 = L1_layer([out2, out3])\n",
    "print(\"distance1\", distance1)\n",
    "\n",
    "# distance1 - distance2\n",
    "subtracted = keras.layers.Subtract()([distance1, distance2])\n",
    "print(\"subrtacted\", subtracted)\n",
    "\n",
    "\n",
    "prediction = Dense(1, activation='sigmoid')(subtracted)\n",
    "# prediction = Activation(K.sigmoid)(subtracted)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# print the model summary\n",
    "model = Model(inputs=[input_points1, input_points2, input_points3], outputs=prediction)\n",
    "print(model.summary())\n",
    "\n",
    "## Visualize network\n",
    "try:\n",
    "  from keras.utils.vis_utils import plot_model as plot\n",
    "  plot(model, to_file = 'drive/My Drive/Triplet_Network_o.png')\n",
    "except ImportError:\n",
    "  print('It seems like the dependencies for drawing the model (pydot, graphviz) are not installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1563564097524,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "4Kq5SnmCHJ5l",
    "outputId": "7d5ab35e-f9cb-40ec-fd8e-7a7c867aa712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    5209\n",
      "dtype: int64\n",
      "False    264\n",
      "dtype: int64\n",
      "False    5473\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if there is duplicates\n",
    "print(df_results.duplicated().value_counts())\n",
    "print(df_results_test.duplicated().value_counts())\n",
    "df_concat = pd.concat([df_results_test, df_results])\n",
    "print(df_concat.duplicated().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fi0bMaerZHMe"
   },
   "outputs": [],
   "source": [
    "# # Load preprocessed tada\n",
    "\n",
    "# # Training data\n",
    "# data_name = \"_dining_train_3000\"\n",
    "\n",
    "# name1 = \"drive/My Drive/Style_input_data/input_triplet_data1\" + data_name + \".npy\"\n",
    "# input_data1_train = np.load(name1)\n",
    "# name2 = \"drive/My Drive/Style_input_data/input_triplet_data2\" + data_name + \".npy\"\n",
    "# input_data2_train = np.load(name2)\n",
    "# name3 = \"drive/My Drive/Style_input_data/input_triplet_data3\" + data_name + \".npy\"\n",
    "# input_data3_train = np.load(name3)\n",
    "# name_y = \"drive/My Drive/Style_input_data/y_data\" + data_name\n",
    "# y_data_train = pd.read_pickle(name_y)\n",
    "\n",
    "\n",
    "# # # # Testing data\n",
    "# data_name = \"_dining_test\"\n",
    "\n",
    "# name1 = \"drive/My Drive/Style_input_data_1/input_triplet_data1\" + data_name + \".npy\"\n",
    "# input_data1_test = np.load(name1)\n",
    "# name2 = \"drive/My Drive/Style_input_data_1/input_triplet_data2\" + data_name + \".npy\"\n",
    "# input_data2_test = np.load(name2)\n",
    "# name3 = \"drive/My Drive/Style_input_data_1/input_triplet_data2\" + data_name + \".npy\"\n",
    "# input_data3_test = np.load(name3)\n",
    "# name_y = \"drive/My Drive/Style_input_data_1/y_data\" + data_name \n",
    "# y_data_test = pd.read_pickle(name_y)\n",
    "\n",
    "\n",
    "## Style Data 2 ##\n",
    "### Training data\n",
    "data_name = \"_furniture_train_fold0\"\n",
    "\n",
    "name1 = \"drive/My Drive/Style_input_data_2/input_triplet_data1\" + data_name + \".npy\"\n",
    "input_data1_train = np.load(name1)\n",
    "name2 = \"drive/My Drive/Style_input_data_2/input_triplet_data2\" + data_name + \".npy\"\n",
    "input_data2_train = np.load(name2)\n",
    "name3 = \"drive/My Drive/Style_input_data_2/input_triplet_data3\" + data_name + \".npy\"\n",
    "input_data3_train = np.load(name3)\n",
    "name_y = \"drive/My Drive/Style_input_data_2/y_data\" + data_name\n",
    "y_data_train = pd.read_pickle(name_y)\n",
    "\n",
    "### Testing data\n",
    "data_name = \"_furniture_test_fold0\"\n",
    "\n",
    "name1 = \"drive/My Drive/Style_input_data_2/input_triplet_data1\" + data_name + \".npy\"\n",
    "input_data1_test = np.load(name1)\n",
    "name2 = \"drive/My Drive/Style_input_data_2/input_triplet_data2\" + data_name + \".npy\"\n",
    "input_data2_test = np.load(name2)\n",
    "name3 = \"drive/My Drive/Style_input_data_2/input_triplet_data3\" + data_name + \".npy\"\n",
    "input_data3_test = np.load(name3)\n",
    "name_y = \"drive/My Drive/Style_input_data_2/y_data\" + data_name\n",
    "y_data_test = pd.read_pickle(name_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1140,
     "status": "ok",
     "timestamp": 1564239116158,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "YgMBqQO8w2A-",
    "outputId": "4f935ee2-87ef-4247-85c8-05179cfabe1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16274, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AiksHuhBcIux"
   },
   "source": [
    "## Train on Style Data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88936,
     "status": "error",
     "timestamp": 1565087813851,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "HLIFNROVbm8M",
    "outputId": "d0d505cd-42d8-4685-81c1-ec63e7810872"
   },
   "outputs": [],
   "source": [
    "import keras.callbacks\n",
    "\n",
    "# compile classification model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "####### Use for style data 1 #######\n",
    "# Training and testing data\n",
    "X_train = [input_data1_train,input_data2_train, input_data3_train]\n",
    "Y_train = y_data_train[\"target\"]\n",
    "\n",
    "X_test = [input_data1_test,input_data2_test, input_data3_test]\n",
    "Y_test = y_data_test[\"target\"]\n",
    "\n",
    "\n",
    "\n",
    "# Time\n",
    "start = time.time()\n",
    "\n",
    "# TensorBoard\n",
    "tb_cb = keras.callbacks.TensorBoard(log_dir=\"drive/My Drive/tflog/\")\n",
    "cbks = [tb_cb]\n",
    "\n",
    "num_epoch = 5\n",
    "# Fit model on training data\n",
    "for i in range(1, num_epoch):\n",
    "    # rotate and jitter the points\n",
    "#     train_points_rotate = rotate_point_cloud(train_points_r)\n",
    "#     train_points_jitter = jitter_point_cloud(train_points_rotate)\n",
    "    print(\"Total epoch:\", i, \"/\", num_epoch)\n",
    "    model.fit(X_train, Y_train, batch_size=16, epochs=1, shuffle=True, verbose=1, callbacks=cbks)\n",
    "    elapsed_time = time.time() - start\n",
    "    print(\"elapsed_time:{0}\".format(elapsed_time/60) + \"[minutes]\")\n",
    "   \n",
    "    model.save_weights(\"drive/My Drive/Weights/style_model_weights_living.h5\")\n",
    "    \n",
    "    if i % 1 == 0:\n",
    "      scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "      print('Test loss: ', scores[0])\n",
    "      print('Test accuracy: ', scores[1])\n",
    "\n",
    "\n",
    "## Save model and weights\n",
    "model.save_weights(\"drive/My Drive/Weights/style_model_weights_living.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 927,
     "status": "ok",
     "timestamp": 1564242243532,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "TEHVqlsTGUUP",
    "outputId": "9246c517-8245-4b80-c07b-c65158859282"
   },
   "outputs": [],
   "source": [
    "y_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4B_2lChcPgr"
   },
   "source": [
    "## Train on Style Data 2 (cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63460,
     "status": "error",
     "timestamp": 1565088015145,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "2lbYDtjKcCS6",
    "outputId": "86959e3a-8655-4f7a-fc75-c7f4288baa7a"
   },
   "outputs": [],
   "source": [
    "import keras.callbacks\n",
    "\n",
    "# compile classification model\n",
    "model.reset_states()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "####### Use for style data 2 #######\n",
    "# Training and testing data\n",
    "# The triplet is in the order [\"a\", \"query\", \"b\"]\n",
    "X_train = [input_data1_train,input_data2_train, input_data3_train]\n",
    "Y_train = y_data_train[\"target\"]\n",
    "\n",
    "\n",
    "X_test = [input_data1_test,input_data2_test, input_data3_test]\n",
    "Y_test = y_data_test[\"target\"]\n",
    "\n",
    "\n",
    "# Time\n",
    "start = time.time()\n",
    "\n",
    "# TensorBoard\n",
    "tb_cb = keras.callbacks.TensorBoard(log_dir=\"drive/My Drive/tflog/\")\n",
    "cbks = [tb_cb]\n",
    "\n",
    "num_epoch = 10\n",
    "# Fit model on training data\n",
    "for i in range(1, num_epoch):\n",
    "    # rotate and jitter the points\n",
    "#     train_points_rotate = rotate_point_cloud(train_points_r)\n",
    "#     train_points_jitter = jitter_point_cloud(train_points_rotate)\n",
    "    print(\"Total epoch:\", i, \"/\", num_epoch)\n",
    "    model.fit(X_train, Y_train, batch_size=16, epochs=1, shuffle=True, verbose=1, callbacks=cbks)\n",
    "    elapsed_time = time.time() - start\n",
    "    print(\"elapsed_time:{0}\".format(elapsed_time/60) + \"[minutes]\")\n",
    "   \n",
    "    model.save_weights(\"drive/My Drive/Weights/style_model_weights.h5\")\n",
    "    \n",
    "    if i % 1 == 0:\n",
    "      scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "      print('Test loss: ', scores[0])\n",
    "      print('Test accuracy: ', scores[1])\n",
    "\n",
    "\n",
    "## Save model and weights\n",
    "model.save_weights(\"drive/My Drive/Weights/style_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1269,
     "status": "ok",
     "timestamp": 1563880961880,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "XM45J8Pugxp_",
    "outputId": "33ee20df-817b-4770-fa84-24f7131c225a"
   },
   "outputs": [],
   "source": [
    "print(len(input_data3))\n",
    "print(len(input_data3_train))\n",
    "print(len(input_data3_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1122,
     "status": "ok",
     "timestamp": 1563880974395,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "tq1cvD7Tf66P",
    "outputId": "2ff6815e-f246-43fd-ed3f-3f91f84e1250"
   },
   "outputs": [],
   "source": [
    "print(len(Y))\n",
    "print(len(Y_train))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4311,
     "status": "ok",
     "timestamp": 1564241175524,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "n_yAKooBzhN9",
    "outputId": "90d181e6-c966-4934-c476-d800d3260cfc"
   },
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "Y_test = np_utils.to_categorical(y_data_test[\"target\"], k)\n",
    "X_test = [input_data1_test,input_data2_test, input_data3_test]\n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1472,
     "status": "ok",
     "timestamp": 1564241300100,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "nf8hKjwwC8k4",
    "outputId": "72f8399d-9c09-428a-e867-5be40f235f79"
   },
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11444,
     "status": "ok",
     "timestamp": 1564241264264,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "oj4PTFqeC0Wt",
    "outputId": "58ff0b54-3da4-4562-a69a-c973df26f0d5"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test,1,verbose=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2610,
     "status": "ok",
     "timestamp": 1557568468078,
     "user": {
      "displayName": "Yuu Sakaguchi",
      "photoUrl": "",
      "userId": "03751953654818345659"
     },
     "user_tz": -120
    },
    "id": "AJskm5z6NpGa",
    "outputId": "0c01f373-12b4-4244-b91b-152ac8fdddc8"
   },
   "outputs": [],
   "source": [
    "!cat /proc/uptime | awk '{print $1 /60 /60 \" hours\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJcxmFFAWI81"
   },
   "outputs": [],
   "source": [
    "# Scaling\n",
    "from sklearn import preprocessing\n",
    "mm = preprocessing.MinMaxScaler()\n",
    "for i in range(len(test_points)):\n",
    "    test_points[i] = mm.fit_transform(test_points[i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "style_model_triplet_train_v6.ipynb",
   "provenance": [
    {
     "file_id": "1y9ZKtNPR36czyo0GM7OoxoXsGtW7T3S_",
     "timestamp": 1564684096170
    },
    {
     "file_id": "1B3WsfzlrHyfZrxvyDxByW0tEGo4XQCor",
     "timestamp": 1564238841925
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
